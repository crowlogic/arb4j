\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}

\begin{document}

\section{Proof of Non-redundant Parameter Space Construction}

\begin{theorem}
Given a likelihood function $L(\theta)$ where $\theta \in \mathbb{R}^n$, the transformation $\theta_{\text{reduced}} = V_k^T\theta$ constructed from the Fisher Information Matrix (FIM) eigenvectors corresponding to non-zero eigenvalues yields a non-redundant parameter space where $k < n$ if and only if there were redundancies in the original parameterization.
\end{theorem}

\begin{proof}
Let $F$ be the Fisher Information Matrix with elements:
$$F_{ij} = -\mathbb{E}\left[\frac{\partial^2}{\partial \theta_i \partial \theta_j} \log L(\theta)\right]$$

Since $F$ is symmetric and positive semidefinite, it has eigendecomposition $F = V\Lambda V^T$ where:
\begin{itemize}
    \item $\Lambda = \text{diag}(\lambda_1, \lambda_2, \lambda_3, \lambda_4, \lambda_5, \ldots, \lambda_n)$ where $\lambda_i \geq 0$
    \item $V = [v_1 \; v_2 \; v_3 \; v_4 \; v_5 \; \cdots \; v_n]$ with orthonormal eigenvectors
\end{itemize}

The rank $k$ of $F$ equals the number of non-zero eigenvalues[1]. When $k < n$, the model is parameter redundant with deficiency $d = n - k$[5]. 

Let $V_k = [v_1 \; v_2 \; v_3 \; \cdots \; v_k]$ contain only the eigenvectors corresponding to non-zero eigenvalues. The transformation $\theta_{\text{reduced}} = V_k^T\theta$ then:
\begin{itemize}
    \item Projects onto a space of dimension $k < n$ if and only if redundancies existed
    \item Contains only independent parameters since each basis vector corresponds to a unique non-zero eigenvalue
    \item Preserves all information in the likelihood since zero eigenvalues indicate directions of flat likelihood[2]
\end{itemize}

Therefore, $\theta_{\text{reduced}}$ provides a minimal, non-redundant parameterization.
\end{proof}

\end{document}