\documentclass{article}
\usepackage{amsmath, amssymb, amsthm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\title{Eigenfunctions of Non-Stationary Operators via Orthogonal Polynomials}
\author{}
\date{}

\begin{document}

\maketitle

\section{Introduction}

We present a rigorous mathematical framework for constructing eigenfunctions of non-stationary operators using orthogonal polynomials. This approach is applicable to a wide class of processes, including those with infinite variance.

\section{Construction of Eigenfunctions}

\subsection{Orthogonal Polynomials}

Let $\mu(\omega)$ be a measure (not necessarily absolutely continuous). We define polynomials $p_n(\omega)$ orthogonal with respect to $\mu(\omega)$:

\begin{equation}
\int_{-\infty}^{\infty} p_n(\omega) p_m(\omega) d\mu(\omega) = \delta_{nm}
\end{equation}

where $\delta_{nm}$ is the Kronecker delta.

\subsection{Time-Domain Functions}

We transform the orthogonal polynomials to the time domain:

\begin{equation}
r_n(t) = \int_{-\infty}^{\infty} p_n(\omega) e^{i\omega t} d\mu(\omega)
\end{equation}

\subsection{Orthogonalization}

We orthogonalize the functions $r_n(t)$ using the standard $L^2$ inner product:

\begin{equation}
\psi_n(t) = \sum_{k=0}^n a_{nk} r_k(t)
\end{equation}

where the coefficients $a_{nk}$ are determined by the Gram-Schmidt process:

\begin{equation}
a_{nk} = \begin{cases}
1 & \text{if } k = n \\
-\sum_{j=k}^{n-1} a_{nj} \langle r_n, \psi_j \rangle & \text{if } k < n \\
0 & \text{if } k > n
\end{cases}
\end{equation}

Here, $\langle f, g \rangle$ denotes the standard $L^2$ inner product:

\begin{equation}
\langle f, g \rangle = \int_{-\infty}^{\infty} f(t) g(t) dt
\end{equation}

\section{Process Representation}

The process $X(t)$ can be represented as:

\begin{equation}
X(t) = \sum_{n=0}^{\infty} c_n \psi_n(t)
\end{equation}

where $c_n$ are random coefficients (not necessarily with finite variance).

\section{Convergence and Continuity}

\begin{theorem}[Uniform Convergence]
The series $\sum_{n=0}^{\infty} c_n \psi_n(t)$ converges uniformly almost surely.
\end{theorem}

\begin{proof}[Sketch]
We use maximal inequalities that don't rely on moment conditions. Let $S_N(t) = \sum_{n=0}^N c_n \psi_n(t)$. We can show that for any $\epsilon > 0$:

\[ P(\sup_{t,N,M} |S_N(t) - S_M(t)| > \epsilon) \to 0 \text{ as } N,M \to \infty \]

This implies uniform convergence almost surely.
\end{proof}

\begin{theorem}[Sample Path Continuity]
Under suitable conditions on $\psi_n(t)$, the process $X(t)$ has continuous sample paths almost surely.
\end{theorem}

\begin{proof}[Sketch]
We use the modulus of continuity of $\psi_n(t)$. Let $\omega_n(\delta)$ be the modulus of continuity of $\psi_n(t)$. If $\sum_{n=0}^{\infty} |c_n| \omega_n(\delta)$ converges almost surely for all $\delta > 0$, then $X(t)$ has continuous sample paths almost surely.
\end{proof}

\section{Conclusion}

This framework provides a rigorous approach to constructing eigenfunctions for non-stationary operators using regular functions, without assumptions about finite variance or moments. The key steps are:

\begin{enumerate}
\item Construct orthogonal polynomials with respect to a measure $\mu(\omega)$.
\item Transform these to time-domain functions.
\item Orthogonalize using the standard $L^2$ inner product.
\item Represent the process as a series expansion using these functions.
\item Prove convergence and continuity properties without moment assumptions.
\end{enumerate}

This approach is applicable to a wide class of processes, including those with infinite variance, providing a powerful tool for the analysis and representation of non-stationary processes.

\end{document}