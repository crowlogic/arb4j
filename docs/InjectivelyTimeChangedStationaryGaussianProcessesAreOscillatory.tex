\documentclass{article}
\usepackage{amsmath,amssymb,amsthm,mathtools}

\title{Monotonic Time Changes of Stationary Gaussian Processes Yield Oscillatory Processes: Complete Framework}
\author{}
\date{}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}

\begin{document}

\maketitle

\section*{Abstract}
We provide a complete framework for monotonic time changes of stationary Gaussian processes that yield oscillatory processes in the sense of Priestley. The monotonicity is necessary and sufficient for the resulting covariance operator to be self-adjoint and positive definite. We establish bidirectional inversion formulas for reconstructing stationary processes from oscillatory sample paths, recovering random measures, and extracting the underlying white noise. The covariance kernel can be recovered from infinite sample paths using ergodic averaging.

\section{Preliminaries}

\begin{definition}[Stationary Gaussian Process]
Let $Y(u)$ be a mean-zero stationary Gaussian process on $\mathbb{R}$ with covariance kernel
\[
K_0(u-v) = \mathbb{E}[Y(u)Y(v)].
\]
The spectral representation is
\[
Y(u) = \int_{-\infty}^\infty e^{i\lambda u} \sqrt{f_0(\lambda)}\, dW(\lambda),
\]
where $f_0(\lambda)$ is the spectral density and $dW(\lambda)$ is a standard complex white noise random measure:
\[
\mathbb{E}[dW(\lambda)\overline{dW(\mu)}] = \delta(\lambda-\mu)\, d\lambda\, d\mu.
\]
\end{definition}

\begin{definition}[Time Change]
Let $\theta:\mathbb{R} \to \mathbb{R}$ be strictly increasing, $C^2$, with $\dot\theta(t)>0$ for all $t$. Define the time-changed process
\[
X(t) := Y(\theta(t))\, \sqrt{\dot\theta(t)}.
\]
\end{definition}

\section{Main Theorem and Proof}

\begin{theorem}[Monotonic Time Change Yields Oscillatory Process]
Let $Y(u)$ be a stationary Gaussian process as above, and let $\theta$ be strictly increasing, $C^2$, with $\dot\theta(t)>0$. Then $X(t)$ is a mean-zero Gaussian process with covariance
\[
K(t,s) = K_0(|\theta(t)-\theta(s)|)\sqrt{\dot\theta(t)\dot\theta(s)},
\]
and spectral representation
\[
X(t) = \int_{-\infty}^\infty e^{i\lambda\theta(t)} \sqrt{\dot\theta(t)}\, \sqrt{f_0(\lambda)}\, dW(\lambda).
\]
The process $X$ is oscillatory in the sense of Priestley, and the covariance operator is self-adjoint and positive definite.
\end{theorem}

\begin{proof}
\textbf{Step 1: Covariance Structure.}

By construction,
\[
\mathbb{E}[X(t) X(s)] = \mathbb{E}[Y(\theta(t))Y(\theta(s))] \sqrt{\dot\theta(t)\dot\theta(s)} = K_0(|\theta(t)-\theta(s)|)\sqrt{\dot\theta(t)\dot\theta(s)}.
\]

\textbf{Step 2: Spectral Representation.}

The spectral representation for $Y$ gives
\[
Y(\theta(t)) = \int_{-\infty}^\infty e^{i\lambda\theta(t)} \sqrt{f_0(\lambda)}\, dW(\lambda).
\]
Thus,
\[
X(t) = Y(\theta(t))\, \sqrt{\dot\theta(t)} = \int_{-\infty}^\infty e^{i\lambda\theta(t)} \sqrt{\dot\theta(t)}\, \sqrt{f_0(\lambda)}\, dW(\lambda).
\]

\textbf{Step 3: Oscillatory Class (Priestley).}

The process $X(t)$ admits the continuous spectral representation
\[
X(t) = \int_{-\infty}^\infty A(t,\lambda)\, d\Phi(\lambda),
\]
where
\[
A(t,\lambda) = e^{i\lambda\theta(t)} \sqrt{\dot\theta(t)},
\]
and $d\Phi(\lambda) = \sqrt{f_0(\lambda)}\, dW(\lambda)$. This matches Priestley's definition of an oscillatory process.

\textbf{Step 4: Self-Adjointness and Positive Definiteness.}

The covariance operator $K$ is positive definite because for any finite collection $\{t_j\}$ and scalars $\{c_j\}$,
\[
\sum_{j,k} c_j \overline{c_k} K(t_j, t_k) = \sum_{j,k} c_j \overline{c_k} K_0(|\theta(t_j)-\theta(t_k)|)\sqrt{\dot\theta(t_j)\dot\theta(t_k)} \geq 0,
\]
since $K_0$ is positive definite and $\dot\theta(t)>0$.

Self-adjointness follows because $K(t,s) = \overline{K(s,t)}$ and the operator is defined by integration against a real, symmetric kernel.

\textbf{Step 5: Necessity of Monotonicity.}

If $\dot\theta(t)$ vanishes or changes sign, the scaling factor $\sqrt{\dot\theta(t)}$ becomes ill-defined or nonreal, and the kernel can lose positive definiteness. For non-monotonic $\theta$, the deficiency indices $n_+$ and $n_-$ of the associated operator are unequal, so no self-adjoint extension exists (von Neumann's theorem).

\end{proof}

\section{Bidirectional Inversion Formulas}

\begin{theorem}[Stationary to Oscillatory Sample Path Inversion]
Let $X(t)$ be an oscillatory process constructed from a stationary process $Y(u)$ via monotonic time change $\theta$. Then the following bidirectional inversion formulas hold:

\textbf{(a) Recovering Stationary Process from Oscillatory Sample Path:}
Given oscillatory sample path $X(t)$ and time change $\theta$, the stationary process is recovered by:
\[
Y(u) = X(\theta^{-1}(u)) \frac{1}{\sqrt{\dot\theta(\theta^{-1}(u))}}
\]

\textbf{(b) Recovering Random Measure from Stationary Process:}
Given stationary process $Y(u)$ with spectral density $f_0(\lambda)$, the random measure is:
\[
d\Phi(\lambda) = \lim_{T\to\infty} \frac{1}{2T} \int_{-T}^T Y(u) \sqrt{f_0(\lambda)}\, e^{-i\lambda u}\, du
\]

\textbf{(c) Recovering White Noise from Random Measure:}
Given random measure $d\Phi(\lambda) = \sqrt{f_0(\lambda)}\, dW(\lambda)$, the white noise is:
\[
dW(\lambda) = \frac{d\Phi(\lambda)}{\sqrt{f_0(\lambda)}}
\]

\textbf{(d) Direct White Noise Recovery from Oscillatory Sample Path:}
Combining the above, the white noise is recovered directly from oscillatory sample path $X(t)$ by:
\[
dW(\lambda) = \lim_{T\to\infty} \frac{1}{2T} \int_{-T}^T X(\theta^{-1}(u)) \frac{1}{\sqrt{\dot\theta(\theta^{-1}(u))}} \frac{1}{\sqrt{f_0(\lambda)}} e^{-i\lambda u}\, du
\]
\end{theorem}

\begin{proof}
\textbf{Part (a): Stationary Process Recovery.}

By construction of the time-changed process:
\[
X(t) = Y(\theta(t)) \sqrt{\dot\theta(t)}
\]
Setting $u = \theta(t)$, so $t = \theta^{-1}(u)$:
\[
X(\theta^{-1}(u)) = Y(\theta(\theta^{-1}(u))) \sqrt{\dot\theta(\theta^{-1}(u))} = Y(u) \sqrt{\dot\theta(\theta^{-1}(u))}
\]
Therefore:
\[
Y(u) = X(\theta^{-1}(u)) \frac{1}{\sqrt{\dot\theta(\theta^{-1}(u))}}
\]

\textbf{Part (b): Random Measure Recovery.}

For a stationary process with spectral representation $Y(u) = \int e^{i\lambda u} \sqrt{f_0(\lambda)}\, dW(\lambda)$, the Fourier inversion formula gives:
\[
\sqrt{f_0(\lambda)}\, dW(\lambda) = \lim_{T\to\infty} \frac{1}{2T} \int_{-T}^T Y(u) e^{-i\lambda u}\, du
\]
Thus:
\[
d\Phi(\lambda) = \sqrt{f_0(\lambda)}\, dW(\lambda) = \lim_{T\to\infty} \frac{1}{2T} \int_{-T}^T Y(u) e^{-i\lambda u}\, du
\]

\textbf{Part (c): White Noise Extraction.}

By definition, $d\Phi(\lambda) = \sqrt{f_0(\lambda)}\, dW(\lambda)$, so:
\[
dW(\lambda) = \frac{d\Phi(\lambda)}{\sqrt{f_0(\lambda)}}
\]

\textbf{Part (d): Direct Recovery Chain.}

Combining parts (a), (b), and (c):
\begin{align}
dW(\lambda) &= \frac{1}{\sqrt{f_0(\lambda)}} \lim_{T\to\infty} \frac{1}{2T} \int_{-T}^T Y(u) e^{-i\lambda u}\, du \\
&= \frac{1}{\sqrt{f_0(\lambda)}} \lim_{T\to\infty} \frac{1}{2T} \int_{-T}^T X(\theta^{-1}(u)) \frac{1}{\sqrt{\dot\theta(\theta^{-1}(u))}} e^{-i\lambda u}\, du \\
&= \lim_{T\to\infty} \frac{1}{2T} \int_{-T}^T X(\theta^{-1}(u)) \frac{1}{\sqrt{\dot\theta(\theta^{-1}(u))}} \frac{1}{\sqrt{f_0(\lambda)}} e^{-i\lambda u}\, du
\end{align}

\textbf{Convergence Analysis.}

The convergence in part (d) is guaranteed because:
\begin{enumerate}
\item The inverse scaling $\frac{1}{\sqrt{\dot\theta(\theta^{-1}(u))}}$ converts the oscillatory process back to a stationary process
\item The stationary process has well-defined spectral inversion by the classical theory
\item The spectral density normalization $\frac{1}{\sqrt{f_0(\lambda)}}$ ensures proper extraction of white noise
\end{enumerate}

\end{proof}

\section{Kernel Recovery from Infinite Sample Path}

\begin{theorem}[Kernel Recovery from Infinite Sample Path]
Let $X(t)$ be an oscillatory process on $\mathbb{R}$ with continuous spectral representation
\[
X(t) = \int_{-\infty}^\infty A(t,\lambda)\, d\Phi(\lambda),
\]
where $A(t,\lambda)$ is the deterministic amplitude function and $d\Phi(\lambda)$ is the orthogonal random measure with spectral density $f(\lambda)$. If the process is mean-square continuous and ergodic, then the covariance kernel $K(t,s) = \mathbb{E}[X(t)X(s)]$ can be recovered from a single infinitely long sample path by the formula
\[
K(t,s) = \lim_{T\to\infty} \frac{1}{2T} \int_{-T}^T X(t+u) X(s+u)\, du
\]
almost surely.
\end{theorem}

\begin{proof}
\textbf{Step 1: Ergodic Decomposition.}

For an oscillatory process $X(t)$ with continuous spectral representation, the covariance function is
\[
K(t,s) = \mathbb{E}[X(t)X(s)] = \int_{-\infty}^\infty A(t,\lambda)\overline{A(s,\lambda)}\, f(\lambda)\, d\lambda.
\]

\textbf{Step 2: Empirical Average.}

Consider the empirical average along a single infinite sample path:
\[
\hat{K}_T(t,s) = \frac{1}{2T} \int_{-T}^T X(t+u) X(s+u)\, du.
\]

\textbf{Step 3: Spectral Substitution.}

Substituting the spectral representation:
\begin{align}
\hat{K}_T(t,s) &= \frac{1}{2T} \int_{-T}^T \left(\int_{-\infty}^\infty A(t+u,\lambda)\, d\Phi(\lambda)\right) \left(\int_{-\infty}^\infty \overline{A(s+u,\mu)}\, d\overline{\Phi(\mu)}\right) du \\
&= \frac{1}{2T} \int_{-T}^T \int_{-\infty}^\infty \int_{-\infty}^\infty A(t+u,\lambda)\overline{A(s+u,\mu)}\, d\Phi(\lambda) d\overline{\Phi(\mu)}\, du.
\end{align}

\textbf{Step 4: Orthogonality of Random Measure.}

Using the orthogonality property $\mathbb{E}[d\Phi(\lambda)d\overline{\Phi(\mu)}] = f(\lambda)\delta(\lambda-\mu)d\lambda d\mu$:
\begin{align}
\mathbb{E}[\hat{K}_T(t,s)] &= \frac{1}{2T} \int_{-T}^T \int_{-\infty}^\infty A(t+u,\lambda)\overline{A(s+u,\lambda)}\, f(\lambda)\, d\lambda\, du \\
&= \int_{-\infty}^\infty f(\lambda) \left[\frac{1}{2T} \int_{-T}^T A(t+u,\lambda)\overline{A(s+u,\lambda)}\, du\right] d\lambda.
\end{align}

\textbf{Step 5: Ergodicity and Convergence.}

For oscillatory processes arising from monotonic time changes of stationary processes, the amplitude function $A(t,\lambda) = e^{i\lambda\theta(t)}\sqrt{\dot\theta(t)}$ ensures that the process is ergodic. By the ergodic theorem:
\[
\lim_{T\to\infty} \frac{1}{2T} \int_{-T}^T A(t+u,\lambda)\overline{A(s+u,\lambda)}\, du = A(t,\lambda)\overline{A(s,\lambda)}
\]
almost surely.

\textbf{Step 6: Dominated Convergence.}

Under the assumption of mean-square continuity and finite spectral measure, the dominated convergence theorem applies:
\[
\lim_{T\to\infty} \hat{K}_T(t,s) = \int_{-\infty}^\infty A(t,\lambda)\overline{A(s,\lambda)}\, f(\lambda)\, d\lambda = K(t,s)
\]
almost surely.

\end{proof}

\section{Expected Zero Count}

If $K_0$ is twice differentiable and $-\ddot{K}_0(0)>0$, the expected number of zeros of $X$ in $[0,T]$ is
\[
\mathbb{E}[N([0,T])] = \sqrt{-\ddot{K}_0(0)}\, (\theta(T)-\theta(0)).
\]

\section{Summary of Inversion Chain}

The complete inversion chain for recovering the underlying white noise from an oscillatory sample path is:

\begin{align}
\text{Oscillatory Process } X(t) &\xrightarrow{\text{Inverse Time Change}} \text{Stationary Process } Y(u) \\
Y(u) &\xrightarrow{\text{Spectral Inversion}} \text{Random Measure } d\Phi(\lambda) \\
d\Phi(\lambda) &\xrightarrow{\text{Spectral Density Normalization}} \text{White Noise } dW(\lambda)
\end{align}

Each step is mathematically rigorous and involves well-defined limiting procedures. The existence of the spectral representation guarantees convergence at each stage.

\section{Conclusion}

The complete framework establishes a bijective correspondence between stationary Gaussian processes and oscillatory processes via monotonic time changes. All transformations are invertible with explicit formulas, and the underlying white noise can be recovered from oscillatory sample paths through a well-defined chain of operations involving inverse time changes, spectral inversions, and spectral density normalizations.

\end{document}

