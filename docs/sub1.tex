\documentclass{mc}

%%%%% journal info -- DO NOT CHANGE %%%%%%%%%
\setcounter{page}{1}
\renewcommand\thisnumber{x}
\renewcommand\thisyear{202x}
\renewcommand\thismonth{xxx}
\renewcommand\thisvolume{xx}
\renewcommand\datereceived{}
\renewcommand\dateaccepted{??}
%%%%%%%% end journal info %%%%%%%%%%%%%
\newcommand{\mathd}{\mathrm{d}}
\usepackage[numbers]{natbib}
\bibliographystyle{plainnat}

\usepackage{mathrsfs}

\begin{document}

\markboth{Stephen Crowley}{Gaussian Processes Generated By Monotonically Modulated Stationary Kernels}

\title[Modulated Gaussian Processes]{Gaussian Processes Generated By Monotonically Modulated Stationary Kernels}

\author[Stephen Crowley]{Stephen Crowley\corrauth}
\email{{\tt stephencrowley214@gmail.com} (Stephen Crowley)}

\begin{abstract}
This article examines Gaussian processes generated by monotonically modulating stationary kernels. An explicit isometry 
between the original and the modulated reproducing kernel Hilbert spaces is established, preserving eigenvalues and normalization. 
The expected number of zeros over the interval $[0,T]$ is shown to be exactly $\sqrt{-\ddot{K}(0)}(\theta(T)-\theta(0))$, 
where $\ddot{K}(0)$ is the second derivative of the kernel at zero and $\theta$ is the modulating function.
\end{abstract}

\keywords{Gaussian processes, stationary kernels, monotonic modulation, eigenfunction analysis, zero-crossing function}

\ams{60G15, 60G10, 47A35, 47B34, 47B07}

\maketitle

\section{Introduction}

This article explores the properties of Gaussian
processes\cite{correlationTheoryOfStationaryRandomProcesses}\cite{stationaryAndRelatedStochasticProcesses}
generated by
monotonically modulating the kernels of stationary Gaussian processes. The
investigation centers on three key aspects: (1) the relationship between
eigenfunctions of the covariance operators defined by the original and the modulated kernels, 
(2) the preservation of normalization and eigenvalues under modulation, and (3) 
the expected number of zeros of the resulting processes. Beginning with a 
precise definition of the class of modulating functions $\mathcal{F}$, the 
article proceeds to establish theorems on eigenfunction transformation, normalization
preservation, and a formula for the expected value of the zero-counting function over $[0,T]$. 
These results provide a foundatio for understanding how stationary Gaussian processes
transform when modulated by monotonically increasing functions.
\section{Main Results}

\begin{definition}
  \label{scalingFunctions}Let $\mathcal{F}$ denote the class of functions
  $\theta : \mathbb{R} \to \mathbb{R}$ which are:
  \begin{enumerate}
    \item piecewise continuous with piecewise continuous first derivative,
    
    \item strictly monotonically increasing
    \begin{equation}
      \theta (t) < \theta (s) \forall - \infty \leqslant t < s \leqslant
      \infty
    \end{equation}
    \item and have a finite limiting derivative at infinity
    \begin{equation}
      \lim_{t \to \infty}  \dot{\theta} (t) < \infty
    \end{equation}
  \end{enumerate}
\end{definition}

\begin{remark}
  The conditions in Definition \ref{scalingFunctions} are somewhat redundant
  since a strictly monotonically increasing function must necessarily have a
  positive derivative.
\end{remark}

\

\begin{theorem}[Eigenfunctions]
  For any stationary kernel $K (t, s) = K (|t - s|)$, the eigenfunctions of
  the integral covariance operator
  \begin{equation}
    T_{K_{\theta}}  [f] (t) = \int_0^{\infty} K_{\theta} (| t - s |) f (s)
    \mathd s
  \end{equation}
  defined by the $\theta$-modulated kernel
  \begin{equation}
    K_{\theta} (t, s) = K (| \theta (t) - \theta (s) |)
  \end{equation}
  are given $\forall \theta \in \mathcal{F}$ by
  \begin{equation}
    \begin{array}{llllll}
      \phi_n (t) = \psi_n (\theta (t)) \sqrt{\dot{\theta} (t)} &  &  &  &  & 
    \end{array}
  \end{equation}
  which satisfies the eigenfunction equation
  \begin{equation}
    \begin{array}{ll}
      T_{K_{\theta}} [\phi_n] (t) & = \lambda_n \int_0^{\infty} K_{\theta} (|
      t - s |) \phi_n (s) \mathd s\\
      & = \lambda_n \int_0^{\infty} K_{\theta} (| t - s |) \psi_n (\theta
      (s)) \sqrt{\dot{\theta} (s)} \mathd s\\
      & = \lambda_n \int_0^{\infty} K (| \theta (t) - \theta (s) |) \psi_n
      (\theta (s)) \sqrt{\dot{\theta} (s)} \mathd s\\
      & = \lambda_n \phi_n (t)
    \end{array}
  \end{equation}
  where $\psi_n$ are the normalized eigenfunctions of the covariance operator defined by the original unmodulated
  kernel $K (|t - s|)$ which satisfy
  \begin{equation}
    \begin{array}{ll}
      T_K [\psi_n] (t) & = \lambda_n \int_0^{\infty} K  (| t - s |) \psi_n (s)
      \mathd s\\
      & = \lambda_n \psi_n (t)
    \end{array}
  \end{equation}
\end{theorem}

\begin{proof}
  The eigenfunction equation for the modulated kernel's covariance operator is:
  \begin{equation}
    \int_{- \infty}^{\infty} K (| \theta (t) - \theta (s) |) \phi_n (s) ds =
    \lambda_n \phi_n (t)
  \end{equation}
  The variables can be changed by substituting $u = \theta (s)$, $v = \theta
  (t)$:
  \begin{equation}
    \int_{- \infty}^{\infty} K (|v - u|) \frac{\phi_n (\theta^{- 1}
    (u))}{\dot{\theta} (\theta^{- 1} (u))} du = \lambda_n \phi_n (\theta^{- 1}
    (v))
  \end{equation}
  which is valid due to the strict monotonicity of $\theta$ which assures its
  invertability. Let
  \begin{equation}
    \psi_n (u) = \frac{\phi_n (\theta^{- 1} (u))}{\sqrt{\dot{\theta}
    (\theta^{- 1} (u))}}
  \end{equation}
  Then:
  \begin{equation}
    \int_{- \infty}^{\infty} K (|v - u|) \psi_n (u) du = \lambda_n \psi_n (v)
  \end{equation}
  This is precisely the eigenfunction equation for the original kernel $K$'s covariance operator. Therefore,
  \begin{equation}
    \phi_n (t) = \psi_n (\theta (t)) \sqrt{\dot{\theta} (t)}
  \end{equation}
  are the eigenfunctions of the modulated kernel's covariance operator 
  \begin{equation}
    \begin{array}{ll}
      T_{K_{\theta}} [\phi_n] (t) & = \lambda_n \int_0^{\infty} K_{\theta} (|
      t - s |) \phi_n (s) \mathd s
    \end{array}
  \end{equation}
  and $\psi_n$ are the eigenfunctions of the original kernel's covariance operator which satisfy
  \begin{equation}
    \begin{array}{ll}
      T_K [\psi_n] (t) & = \lambda_n \int_0^{\infty} K  (| t - s |) \psi_n (s)
      \mathd s
    \end{array}
  \end{equation}
  
\end{proof}

\begin{corollary}[Eigenvalue Invariance]
  The eigenvalues $\{\lambda_n \}$ of the modulated kernel $K_{\theta}$'s covariance operator are identical to those of the original kernel $K$'s covariance operator.
\end{corollary}

\begin{proof}
  For normalized $\psi_n$:
  \begin{equation}
    \int_{- \infty}^{\infty} | \phi_n (t) |^2 dt = \int_{- \infty}^{\infty} |
    \psi_n (\theta (t)) |^2 \dot{\theta} (t) dt
  \end{equation}
  Under the change of variables $u = \theta (t)$:
  \begin{equation}
    \int_{- \infty}^{\infty} | \psi_n (u) |^2 du = 1
  \end{equation}
  Therefore the $\phi_n$ are already normalized without additional constants.
\end{proof}

\begin{theorem}[Operator Conjugation]
  The transformation operator
  \begin{equation}
    M_{\theta} [\phi] (t) = \sqrt{\dot{\theta} (t)} \phi (\theta (t))
  \end{equation}
  conjugates the integral covariance operator
  \begin{equation}
    T_K [\phi] (t) = \int_0^{\infty} K (|t - s|) \phi (s) \mathd s
  \end{equation}
  where the resulting conjugated operator is
  \begin{equation}
    \begin{array}{ll}
      T_{K_{\theta}}  [\phi] (t) & {= M_{\theta}}  [T_K [M_{\theta}^{- 1}
      [\phi]]] (t)\\
      & = M \left[ \int_0^{\infty} K (|t - s|) \frac{\phi (\theta^{- 1}
      (s))}{\sqrt{\dot{\theta} (\theta^{- 1} (s))}} \mathd s \right] (t)\\
      & = \sqrt{\dot{\theta} (t)}  \int_0^{\infty} K (| \theta (t) - \theta (s)|)
      \frac{\phi (\theta^{- 1} (s))}{\sqrt{\dot{\theta} (\theta^{- 1} (s))}}
      \mathd s\\
      & = \int_0^{\infty} K (| \theta (t) - \theta (s) |) \phi (s) \mathd s\\
      & = \int_0^{\infty} K_{\theta} (| t - s |) \phi (s) \mathd s
    \end{array} \label{a}
  \end{equation}
  providing an explicit isometry between the original and modulated kernel
  Hilbert spaces.
\end{theorem}

\begin{proof}
  Observe that $M$ has inverse operator
  \begin{equation}
    M^{- 1} [\phi] (t) = \frac{\phi (\theta^{- 1} (t))}{\sqrt{\dot{\theta}
    (\theta^{- 1} (t))}}
  \end{equation}
  which follows from the invertibility of $\theta$ due to strict monotonicity
  and note that the last equality in Equation (\ref{a}) follows from the
  change of variables $s \mapsto \theta (s)$ with Jacobian $\dot{\theta} (s)$,
  demonstrating that the conjugated operator is precisely the integral
  operator with modulated kernel $K (| \theta (t) - \theta (s) |)$.
\end{proof}

\begin{theorem}[Expected Zero-Counting Function]
  Let $\theta \in \mathcal{F}$ and let $K (\cdot)$ be any positive-definite,
  stationary covariance function, twice differentiable at $0$. Consider the
  centered Gaussian process with covariance
  \begin{equation}
    K_{\theta} (t, s) = K (| \theta (t) - \theta (s) |)
  \end{equation}
  Then the expected number of zeros in $[0, T]$ is
  \begin{equation}
    \mathbb{E} [N ([0, T])] = \sqrt{- \ddot{K} (0)}  \hspace{0.17em} (\theta
    (T) - \theta (0))
  \end{equation}
\end{theorem}

\begin{proof}
  By the Kac-Rice
  formula{\cite[10.3.1]{stationaryAndRelatedStochasticProcesses}}:
  \begin{equation}
    \mathbb{E} [N ([0, T])] = \int_0^T \sqrt{- \lim_{s \to t}  \hspace{0.17em}
    \frac{\partial^2}{\partial t \partial s}  \hspace{0.17em} K_{\theta} (s,
    t)}  \hspace{0.27em} dt
  \end{equation}
  Computing the mixed partial derivative and taking the limit as $s \to t$:
  \begin{equation}
    \lim_{s \to t}  \hspace{0.17em} \frac{\partial^2}{\partial t \partial s} 
    \hspace{0.17em} K_{\theta} (s, t) = - \ddot{K} (0) \hspace{0.17em}
    \dot{\theta} (t)^2
  \end{equation}
  Therefore
  \begin{equation}
    \mathbb{E} [N ([0, T])] = \sqrt{- \ddot{K} (0)}  \int_0^T \dot{\theta} (t)
    \hspace{0.17em} dt = \sqrt{- \ddot{K} (0)}  \hspace{0.17em} (\theta (T) -
    \theta (0))
  \end{equation}
  so that
  \begin{equation}
    \begin{array}{ll}
      \sqrt{- \ddot{K} (0)}  \hspace{0.17em} (\theta (T) - \theta (0)) & =
      \sqrt{- \ddot{K} (0)}  \int_0^T \dot{\theta} (t)  \hspace{0.17em} dt\\
      & = \int_0^T \sqrt{- \ddot{K} (0) \dot{\theta} (t)^2}  \hspace{0.27em}
      dt\\
      & = \int_0^T \sqrt{- \lim_{s \to t}  \hspace{0.17em}
      \frac{\partial^2}{\partial t \partial s}  \hspace{0.17em} K (| \theta
      (t) - \theta (s) |)}  \hspace{0.27em} dt
    \end{array}
  \end{equation}
  which is precisely the Kac-Rice formula for the expected zero-counting
  function.
\end{proof}

\begin{theorem}[Sample Path Transformation]
Let $X(t)$ be a centered Gaussian process with stationary covariance kernel $K(|t-s|)$ and almost surely continuous sample paths. Let $\theta \in \mathcal{F}$ be a strictly increasing modulating function. Then there exists a Gaussian process $Y(t)$ with:

1. Covariance kernel $K_\theta(t,s) = K(|\theta(t)-\theta(s)|)$
2. Sample paths related to $X$ through $Y(t) = X(\theta(t))$ almost surely
3. Expected zero crossings satisfying $\mathbb{E}[N_Y([0,T])] = \sqrt{-\ddot{K}(0)}(\theta(T)-\theta(0))$
\end{theorem}

\begin{proof}
\textbf{Step 1: Process Construction} \\
Define $Y(t) := X(\theta(t))$. Since $\theta$ is strictly increasing and measurable, $Y$ remains adapted to the original filtration. By almost sure continuity of $X$ and continuity of $\theta$, $Y$ inherits almost sure path continuity.

\textbf{Step 2: Covariance Structure} \\
For any $t,s \geq 0$:
\begin{equation}
\begin{aligned}
\mathbb{E}[Y(t)Y(s)] &= \mathbb{E}[X(\theta(t))X(\theta(s))] \\
&= K(|\theta(t)-\theta(s)|) \\
&= K_\theta(t,s)
\end{aligned}
\end{equation}
Thus $Y$ has the modulated covariance kernel.

\textbf{Step 3: Gaussian Preservation} \\
For any finite collection $\{t_1,...,t_n\}$, the vector $(Y(t_1),...,Y(t_n)) = (X(\theta(t_1)),...,X(\theta(t_n)))$ is Gaussian by stationarity of $X$. Therefore $Y$ is a Gaussian process.

\textbf{Step 4: Zero Crossing Relationship} \\
From Theorem 3 in the original paper, for any Gaussian process with kernel $K_\theta$:
\begin{equation}
\mathbb{E}[N_Y([0,T])] = \sqrt{-\ddot{K}(0)}\int_0^T \dot{\theta}(t)dt = \sqrt{-\ddot{K}(0)}(\theta(T)-\theta(0))
\end{equation}

\textbf{Step 5: Pathwise Isomorphism} \\
The mapping $X \mapsto Y$ preserves:
\begin{itemize}
\item Gaussian measure structure (via covariance isometry)
\item Path continuity (by continuity of $\theta$)
\item Measurability (as $\theta$ is measurable)
\end{itemize}

This bijective mapping establishes an isomorphism between the original and modulated process path spaces. The inverse transformation is given by $Y \mapsto X(t) = Y(\theta^{-1}(t))$.
\end{proof}

\begin{theorem}[Sample Path Transformation]
Let $X(t)$ be a Gaussian process with stationary covariance kernel $K(|t - s|)$, and let $\theta(t)$ be a strictly increasing, differentiable function with $\dot{\theta}(t) > 0$. Define the modulated Gaussian process $Y(t)$ by
\[
Y(t) = X(\theta(t)) \sqrt{\dot{\theta}(t)}.
\]
The sample paths of $Y(t)$ are related explicitly to those of $X(t)$ by evaluating the paths of $X$ at the transformed argument $\theta(t)$ and multiplying by a factor of $\sqrt{\dot{\theta}(t)}$. In particular, the zero-crossings of $Y(t)$ occur at precisely those points $t$ for which $X(\theta(t)) = 0$. Consequently, the number of zero-crossings of $Y(t)$ on any interval $[a,b]$ equals the number of zero-crossings of $X(t)$ on the interval $[\theta(a),\theta(b)]$.
\end{theorem}

\begin{proof}
The sample paths of $Y(t)$ are given directly by definition as transformations of the sample paths of $X(t)$:
\[
Y(t) = X(\theta(t)) \sqrt{\dot{\theta}(t)},
\]
thus explicitly relating each realization of $Y(t)$ to a corresponding realization of $X(t)$ via evaluation at $\theta(t)$ and scaling by $\sqrt{\dot{\theta}(t)}$.

For the zero-crossings, we observe that
\[
Y(t) = 0 \iff X(\theta(t)) \sqrt{\dot{\theta}(t)} = 0.
\]
Since $\dot{\theta}(t) > 0$, this equation holds if and only if
\[
X(\theta(t)) = 0.
\]
Because $\theta(t)$ is strictly increasing and continuous, it defines a one-to-one correspondence between the intervals $[a, b]$ and $[\theta(a), \theta(b)]$. Thus, zero-crossings of $Y(t)$ directly correspond to zero-crossings of $X(t)$ under the mapping induced by $\theta(t)$, and the number of zero-crossings is preserved exactly.
\end{proof}


\section{Conclusion}

The analysis presented in this article establishes several fundamental
properties of Gaussian processes generated by monotonically modulated
stationary kernels. Key results include: (1) a theorem demonstrating that the
eigenfunctions of the covariance operator defined by the modulated kernel are 
compositions of the stationary kernel's covariance operator eigenfunctions with 
the modulating function, times the square root of the modulating function's 
derivative, (2) proof of normalization and eigenvalue preservation under this 
transformation, establishing an isometry between the original and the modulated 
reproducing kernel Hilbert spaces, and (3) a concise formula for the expected value 
of the zero-counting function of the monotonically transformed process, expressed 
in terms of the original kernel's second derivative at zero times the modulating function's
values at the boundaries of the interval to which the expectation corresponds.

% \section*{Acknowledgement}
%The author would like to thank the referees for their helpful suggestions.

\bibliography{refs}


\end{document}
