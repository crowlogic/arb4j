\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

\title{Foundations of Hamilton-Jacobi-Bellman Equations and Stochastic Optimal Control}
\author{}
\date{}

\begin{document}
\maketitle

\section{Foundations of Hamilton-Jacobi-Bellman Equations}

The Hamilton-Jacobi-Bellman (HJB) equation provides necessary and sufficient conditions for optimality in stochastic control problems. Let's develop the mathematical framework systematically.

\section{Problem Formulation}

Consider a stochastic dynamical system with state $x(t)$ evolving according to:

\[ dx(t) = f(x(t),u(t),t)dt + \sigma(x(t),u(t),t)dW(t) \]

where:
\begin{itemize}
\item $u(t)$ is the control input
\item $W(t)$ is a Wiener process
\item $f$ represents the drift term
\item $\sigma$ represents the diffusion term
\end{itemize}

The objective is to minimize a cost functional:

\[ J(x,t,u) = E\left[\int_t^T L(x(s),u(s),s)ds + \phi(x(T))\right] \]

where:
\begin{itemize}
\item $L$ is the running cost
\item $\phi$ is the terminal cost
\item $T$ is the terminal time
\end{itemize}

\section{Dynamic Programming Principle}

The fundamental principle states that the value function $V(x,t)$ satisfies:

\[ V(x,t) = \min_{u(\cdot)} E\left[V(x(t+dt),t+dt) + L(x(t),u(t),t)dt\right] \]

This leads to the HJB equation through infinitesimal analysis.

\section{The HJB Equation}

For a smooth value function, the HJB equation is:

\[ -V_t = \min_{u}\left\{L(x,u,t) + V_x f(x,u,t) + \frac{1}{2}tr(\sigma\sigma^T V_{xx})\right\} \]

with terminal condition:

\[ V(x,T) = \phi(x) \]

where:
\begin{itemize}
\item $V_t$ is the partial derivative with respect to time
\item $V_x$ is the gradient with respect to state
\item $V_{xx}$ is the Hessian matrix
\end{itemize}

\section{Verification Theorem}

For a smooth solution $V$ of the HJB equation, if:

\begin{enumerate}
\item $V$ satisfies the terminal condition
\item $V$ is twice continuously differentiable
\item The minimizer $u^*(x,t)$ exists
\end{enumerate}

Then $V$ is the optimal value function and $u^*$ is the optimal control.

\section{Stochastic Maximum Principle}

The optimal control can be characterized through:

\[ u^*(x,t) = \arg\min_{u} \{L(x,u,t) + V_x f(x,u,t) + \frac{1}{2}tr(\sigma\sigma^T V_{xx})\} \]

This provides a feedback law when the value function is known.

\section{Viscosity Solutions}

When classical solutions don't exist, the concept of viscosity solutions becomes crucial:

\begin{enumerate}
\item A continuous function $V$ is a viscosity subsolution if for any smooth test function $\phi$:
\[ -\phi_t(x_0,t_0) - H(x_0,D\phi(x_0,t_0),D^2\phi(x_0,t_0)) \leq 0 \]

\item A continuous function $V$ is a viscosity supersolution if for any smooth test function $\phi$:
\[ -\phi_t(x_0,t_0) - H(x_0,D\phi(x_0,t_0),D^2\phi(x_0,t_0)) \geq 0 \]
\end{enumerate}

where $H$ is the Hamiltonian.

\section{Linear-Quadratic Case}

For linear systems with quadratic costs:

\[ dx = (Ax + Bu)dt + \sigma dW \]
\[ L(x,u) = x^TQx + u^TRu \]

The value function takes the form:

\[ V(x,t) = x^TP(t)x \]

where $P(t)$ satisfies the Riccati equation.

\section{Numerical Methods}

Common approaches include:
\begin{itemize}
\item Finite difference methods for the HJB PDE
\item Policy iteration
\item Value iteration
\item Approximate dynamic programming for high-dimensional problems
\end{itemize}

This framework provides the theoretical foundation for solving stochastic optimal control problems, though practical implementation often requires problem-specific approximations or numerical methods.

\end{document}
