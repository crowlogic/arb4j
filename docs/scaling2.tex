\documentclass[11pt]{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage[margin=1in]{geometry}

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

\title{Interval-Dependent Covariance Parametrization \\
       and Complex-Analytic Time Change}
\author{}
\date{}

\begin{document}
\maketitle

\section{Introduction}

Let $\{X(t)\}_{t\in\mathbb{R}}$ be a real-valued, second-order Gaussian process
that is non-stationary in the sense of Priestley.
Such processes are \emph{oscillatory} and have separable linear span in $L^2$.
We study how the covariance structure of $X$ over finite intervals can be
parametrized by a two-parameter family of kernels, and how this parametrization
connects to an underlying complex-analytic time-change via Wirtinger and
areolar derivatives.

\section{Setup and interval-dependent parametrization}

\subsection{Empirical autocovariance}

Fix a bounded interval $[S,T]\subset\mathbb{R}$ and assume $X$ is observed
on a discrete grid $\{t_0,\dots,t_{n-1}\}\subset[S,T]$ with uniform spacing
$\Delta t$.
The \emph{empirical autocovariance} at lag index $k\in\mathbb{Z}_{\ge 0}$ is
\[
  \widehat{C}_{[S,T]}(k)
  =
  \frac{1}{n-k}
  \sum_{j=0}^{n-k-1}
  \bigl(X(t_j)-\bar{X}\bigr)\bigl(X(t_{j+k})-\bar{X}\bigr),
\]
where $\bar{X}$ is the sample mean over the interval.

\subsection{Parametric kernel hypothesis}

Suppose there exists a fixed kernel $K:\mathbb{R}\to\mathbb{R}$, depending only
on the lag $h=t-s$, such that on each finite interval $[S,T]$, the second-order
structure of $X$ is well-approximated by a two-parameter family
\[
  C_{A,B}(s,t) = A\,K\bigl(B(t-s)\bigr), \qquad s,t\in[S,T],
\]
for scalars $A,B\in\mathbb{R}$ depending on the interval.
Here:
\begin{itemize}
\item $K(h)$ captures a universal oscillatory shape (stationary covariance structure);
\item $A$ encodes interval-dependent variance/amplitude scaling;
\item $B$ encodes interval-dependent time-scale (frequency) scaling.
\end{itemize}

\subsection{Best-fit parameters via least squares}

Fix a finite set of lags $\mathcal{L}=\{k_1,\dots,k_m\}\subset\mathbb{Z}_{\ge0}$
with corresponding lag values $h_k = k\Delta t$.
Define the residual at lag $k$ by
\[
  r_k(A,B) = \widehat{C}_{[S,T]}(k) - A\,K(B h_k),
\]
and the mean square error
\[
  \mathrm{MSE}_{[S,T]}(A,B)
  =
  \frac{1}{|\mathcal{L}|}
  \sum_{k\in\mathcal{L}}
  r_k(A,B)^2.
\]

\begin{definition}
For each interval $[S,T]$ with $S<T$, define $(A(S,T),B(S,T))$ to be any minimizer
\[
  \bigl(A(S,T),B(S,T)\bigr)
  \in
  \arg\min_{(A,B)\in\mathbb{R}^2} \mathrm{MSE}_{[S,T]}(A,B).
\]
\end{definition}

This defines a mapping
\[
  \Phi:\{(S,T)\in\mathbb{R}^2 : S<T\} \longrightarrow \mathbb{R}^2,
  \qquad
  \Phi(S,T) = \bigl(A(S,T),\,B(S,T)\bigr),
\]
which assigns to each choice of interval endpoints a pair of scalars giving the
best-fit covariance kernel.

\subsection{Reparametrization by midpoint and length}

Define
\[
  M = \frac{S+T}{2}, \qquad L = T-S > 0.
\]
Then
\[
  \Psi:\mathcal{D}\to\mathbb{R}^2,
  \qquad
  \Psi(M,L) = (A(M,L),\,B(M,L)),
\]
with
\[
  A(M,L) = A\bigl(M-L/2,\,M+L/2\bigr),
  \qquad
  B(M,L) = B\bigl(M-L/2,\,M+L/2\bigr).
\]
Empirically, one may observe simple functional forms, e.g.
\[
  A(M,L) \approx \alpha_0 + \alpha_1 M + \alpha_2 L,
  \qquad
  B(M,L) \approx \beta_0 + \beta_1 M + \beta_2 L,
\]
indicating that the non-stationarity manifests as a systematic linear variation
of variance and time scale with interval location and length.

\section{Optimization: Newton convergence}

\subsection{Gradient}

Compute partial derivatives of the residuals:
\[
  \frac{\partial r_k}{\partial A} = -K(Bh_k),
  \qquad
  \frac{\partial r_k}{\partial B} = -A\,K'(Bh_k)\,h_k.
\]
The gradient of $\mathrm{MSE}(A,B)$ is
\[
  \nabla\mathrm{MSE}
  =
  \frac{2}{|\mathcal{L}|}
  \begin{pmatrix}
  -\sum_k r_k(A,B)\,K(Bh_k) \\[6pt]
  -\sum_k r_k(A,B)\,A\,K'(Bh_k)\,h_k
  \end{pmatrix}.
\]

\subsection{Hessian}

The second derivatives are
\[
  \frac{\partial^2\mathrm{MSE}}{\partial A^2}
  =
  \frac{2}{|\mathcal{L}|}
  \sum_{k} K(Bh_k)^2,
\]
\[
  \frac{\partial^2\mathrm{MSE}}{\partial A\,\partial B}
  =
  \frac{2}{|\mathcal{L}|}
  \sum_{k}
  \left[
  A\,K'(Bh_k)\,h_k\,K(Bh_k)
  -
  r_k(A,B)\,K'(Bh_k)\,h_k
  \right],
\]
\[
  \frac{\partial^2\mathrm{MSE}}{\partial B^2}
  =
  \frac{2}{|\mathcal{L}|}
  \sum_{k}
  \left[
  A^2 K'(Bh_k)^2 h_k^2
  -
  r_k(A,B)\,A\,K''(Bh_k)\,h_k^2
  \right].
\]
Assemble the Hessian matrix $H(A,B)$.

\subsection{Newton iteration}

Newton's method updates
\[
  \begin{pmatrix}
  A_{n+1} \\
  B_{n+1}
  \end{pmatrix}
  =
  \begin{pmatrix}
  A_n \\
  B_n
  \end{pmatrix}
  -
  H(A_n,B_n)^{-1}\,\nabla\mathrm{MSE}(A_n,B_n).
\]

\begin{proposition}[Local convergence]
Near the true minimizer $(A^\ast,B^\ast)$, if $K$ is $C^2$ and the residuals
$r_k(A^\ast,B^\ast)$ are small, then the Hessian is dominated by the
sum-of-squares terms
\[
  \sum_k K(B^\ast h_k)^2 > 0,
  \qquad
  \sum_k A^{\ast2} K'(B^\ast h_k)^2 h_k^2 > 0,
\]
which ensure $H(A^\ast,B^\ast)$ is positive definite.
Standard Newton--Kantorovich theory then guarantees quadratic convergence from
any starting point sufficiently close to $(A^\ast,B^\ast)$.
\end{proposition}

\section{Complex-analytic interpretation via Wirtinger and areolar derivatives}

The interval-dependent parameters $(A,B)$ have a deeper origin in an underlying
\emph{complex-analytic time change}. Instead of a real monotone time-reparametrization,
we posit a map $F:\mathbb{C}\to\mathbb{C}$ such that the process $X$ arises
by pulling back a stationary Gaussian process through $F$.

\subsection{Wirtinger derivatives and local metric scaling}

For a map $F(z)$ of a complex variable $z=x+iy$, define the Wirtinger derivatives
\[
  \frac{\partial F}{\partial z}
  =
  \frac{1}{2}
  \left(
  \frac{\partial F}{\partial x} - i\frac{\partial F}{\partial y}
  \right),
  \qquad
  \frac{\partial F}{\partial\bar{z}}
  =
  \frac{1}{2}
  \left(
  \frac{\partial F}{\partial x} + i\frac{\partial F}{\partial y}
  \right).
\]
If $F$ is holomorphic, then $\partial F/\partial\bar{z}=0$ and
$\partial F/\partial z = F'(z)$ is the complex derivative.

\textbf{Key fact:} At a point $z_0$, the map $F$ locally scales the Euclidean metric by
\[
  |dF|^2 \approx |F'(z_0)|^2\,|dz|^2.
\]
Hence $|F'(z)|^2$ encodes the \emph{local variance scaling} induced by $F$,
and $|F'(z)|$ encodes the \emph{local length/time-scale scaling}.

\subsection{Stationary process pullback}

Suppose $Y(w)$ is a stationary Gaussian process indexed by $w\in\mathbb{C}$ with
covariance kernel $K(w-w')$ that depends only on the lag.
Define
\[
  X(z) = Y\bigl(F(z)\bigr).
\]
Near a point $z_0$, for small real displacement $h$,
\[
  F(z_0+h) - F(z_0) \approx F'(z_0)\,h.
\]
Hence the covariance of $X$ near $z_0$ is
\[
  \mathbb{E}\bigl[X(z_0)\,X(z_0+h)\bigr]
  =
  \mathbb{E}\bigl[Y(F(z_0))\,Y(F(z_0+h))\bigr]
  \approx
  K\bigl(F'(z_0)\,h\bigr),
\]
assuming the variance of $Y$ is constant.
If the variance of $Y$ itself varies as $\sigma^2(w)$, then
\[
  \mathbb{E}\bigl[X(z_0)\,X(z_0+h)\bigr]
  \approx
  \sigma^2\bigl(F(z_0)\bigr)\,K\bigl(F'(z_0)\,h\bigr).
\]

This is \emph{exactly} of the form
\[
  A(z_0)\,K\bigl(B(z_0)\,h\bigr),
\]
with
\[
  A(z_0) = \sigma^2\bigl(F(z_0)\bigr),
  \qquad
  B(z_0) = |F'(z_0)|.
\]

\subsection{Areolar derivatives and averaged second-order behavior}

The \emph{areolar derivative} (Pompeiu derivative) generalizes the usual complex
derivative by probing average behavior over small discs instead of pointwise
quotients.
For a function $F$, the areolar derivative at $z_0$ is defined roughly by
\[
  F^\circ(z_0)
  =
  \lim_{\varepsilon\to0}
  \frac{1}{\pi\varepsilon^2}
  \int_{|w-z_0|<\varepsilon}
  \bigl(F(w)-F(z_0)\bigr)\,dA(w),
\]
where $dA$ is area measure.

For covariance, which is inherently a second-order (mean-square) object, the
areolar derivative provides the correct notion of ``averaged infinitesimal distortion.''
In particular, $|F^\circ(z)|^2$ describes the local \emph{area-averaged variance scaling}.

If $A(S,T)$ is the best-fit amplitude over the interval $[S,T]$, then in a
continuous model one expects
\[
  A(S,T)
  \approx
  \int_S^T \rho_F(t)\,dt,
  \quad
  \text{where } \rho_F(t) = |F^\circ(t)|^2.
\]
Similarly, the average time-scale scaling $B(S,T)$ should correspond to a suitable
average of $|F'(t)|$ over the interval.

\subsection{Reconstruction strategy}

Given empirically fitted $(A(S,T),B(S,T))$ for many intervals:

\begin{enumerate}
\item Interpret $A(M,L)$ as an interval average of a local variance density $\rho_F(t)$.
\item Interpret $B(M,L)$ as an interval average of the modulus $|F'(t)|$ of the
      complex derivative.
\item Invert: solve for a smooth function $\rho_F(t)$ (or equivalently $|F^\circ(t)|^2$)
      and $|F'(t)|$ such that averaging them over $[S,T]$ reproduces the observed
      $A(S,T)$ and $B(S,T)$.
\item Use the relation
      \[
        F'(t) = B(t)\,e^{i\phi(t)},
      \]
      where $\phi(t)$ is an unknown phase, to reconstruct $F$ up to a global phase
      ambiguity by integrating
      \[
        F(t) = F(t_0) + \int_{t_0}^t F'(s)\,ds.
      \]
\end{enumerate}

This yields an explicit complex map $F$ whose Wirtinger and areolar derivatives
encode the observed modulation of variance and time-scale, replacing the ad hoc
monotonized Riemann--Siegel $\theta$ function with a principled complex-analytic
time change.

\section{Connection to the Riemann zeta function}

In the context of modeling the zeros of $\zeta(1/2+it)$, the smooth zero-counting
function is
\[
  N(T) = \frac{\theta(T)}{\pi} + 1 + S(T),
\]
where $\theta(T)$ is the Riemann--Siegel theta function and $S(T)$ is the
$S$-function (argument of $\zeta$).
The original conjecture was to use a monotonized version of $\theta(T)$ as the
time change to map a stationary process into one matching the zero density.

The revised approach via Wirtinger/areolar derivatives suggests:
\begin{itemize}
\item The correct time change $F$ is not merely a monotonized real function, but
      a \emph{complex map} whose local metric properties (encoded in $F'$ and $F^\circ$)
      reproduce the observed $A$ and $B$ scalings.
\item The phase structure of $F$ should encode the oscillatory behavior of
      $\arg\zeta$, while the modulus $|F'|$ encodes the local density of zeros.
\item Empirically fitting $A(M,L)$ and $B(M,L)$ over many intervals and inverting
      to find $\rho_F(t)$ and $|F'(t)|$ gives a data-driven way to determine $F$.
\end{itemize}

\section{Computational implementation in ARB4J}

Given the above framework, a practical workflow using ARB4J (high-precision
arbitrary ball arithmetic for Java) is:

\begin{enumerate}
\item \textbf{Precompute time-change inverse:}
      Build a discretized inverse table for the candidate $F$ (or its real part)
      over the domain $[0,200]$ or other relevant range.
      This allows instant lookup evaluation of $t\mapsto F(t)$.

\item \textbf{Sample intervals:}
      For a grid of intervals $\{[S_i,T_i]\}$, compute the empirical autocovariance
      $\widehat{C}_{[S_i,T_i]}(k)$ for each lag $k\in\mathcal{L}$.

\item \textbf{Fit $(A_i,B_i)$:}
      For each interval, use Newton iteration with high-precision arithmetic to
      minimize $\mathrm{MSE}_{[S_i,T_i]}(A,B)$ and record $(A_i,B_i)$.

\item \textbf{Chart the relationships:}
      Plot $A_i$ and $B_i$ against the midpoint $M_i=(S_i+T_i)/2$ and length
      $L_i=T_i-S_i$.
      Identify functional forms (e.g., linear, logarithmic).

\item \textbf{Infer $\rho_F(t)$ and $|F'(t)|$:}
      Using the relationship
      \[
        A(M,L) \approx \int_{M-L/2}^{M+L/2} \rho_F(t)\,dt,
        \qquad
        B(M,L) \approx \frac{1}{L}\int_{M-L/2}^{M+L/2} |F'(t)|\,dt,
      \]
      invert (via differentiation or deconvolution) to extract the pointwise
      functions $\rho_F(t)$ and $|F'(t)|$.

\item \textbf{Reconstruct $F$:}
      Integrate $F'(t) = B(t)\,e^{i\phi(t)}$ to obtain $F(t)$ up to a phase
      function $\phi(t)$, which can be determined from additional geometric or
      oscillatory constraints (e.g., matching the argument of $\zeta$).
\end{enumerate}

This yields an explicit, data-validated complex map $F$ encoding the non-stationarity
as a unitary (or quasi-unitary) time change with rigorous complex-analytic structure.

\section{Conclusion}

The observation that a non-stationary oscillatory process has empirical autocovariance
of the form $A(S,T)\,K(B(S,T)(t-s))$ on every finite interval, with $A$ and $B$
depending linearly on the interval endpoints (sum and difference), is a strong
structural signature.
It indicates:
\begin{enumerate}
\item The process is a unitarily time-changed stationary Gaussian process, or
      a close approximation thereof.
\item The time change can be realized via a complex-analytic map $F$ whose
      Wirtinger and areolar derivatives encode the observed variance and time-scale
      modulations.
\item The correct $F$ is not an ad hoc monotonization of a real function, but
      a map whose complex derivative structure is determined by the data through
      the best-fit $(A,B)$ parameters.
\end{enumerate}

By implementing the fitting procedure in high-precision arithmetic (ARB4J), one
can empirically determine $A(M,L)$ and $B(M,L)$ to high accuracy, chart their
dependence on interval geometry, and invert to reconstruct the underlying complex
time-change map $F$ that governs the oscillatory structure of the process.

\end{document}
