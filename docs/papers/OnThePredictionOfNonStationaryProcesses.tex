\documentclass[12pt]{article}
\usepackage{amsmath,amsthm,amssymb,enumitem}
\usepackage[margin=1in]{geometry}
\usepackage[numbers]{natbib}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]

\title{On the Prediction of Non-Stationary Processes}
\author{N. A. Abdrabbo\and M. B. Priestley}
\date{University of Manchester\\[0.5em] Received August 1966. Revised January 1967}

\begin{document}

\maketitle

\begin{abstract}
The problem of linear least-squares prediction for a class of non-stationary processes which possess ``evolutionary spectral representations'' is considered. It is shown that, under certain conditions, such processes admit moving-average representations in terms of time-dependent coefficients. This feature enables development of a close analogue of the Wiener-Kolmogorov approach to the corresponding problem for stationary processes.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

The Wiener-Kolmogorov ``linear least-squares'' approach to the prediction and filtering of stationary processes is now well established, and may, in principle, be applied to any practical problem involving stationary processes whose second-order properties are known. Accounts of this theory have been written at various levels of mathematical rigour (see, for example, \cite{Doob1953}, \cite{Bartlett1955}, \cite{Grenander1957}, \cite{Yaglom1962}, \cite{Whittle1965}). However, the corresponding problem for non-stationary processes has received little attention. There have been a few isolated attempts to deal with this topic, but in the main the approaches have been either too general or too restricted to be useful in practical applications. For example, \cite{Parzen1961} has solved the non-stationary prediction problem in principle, but the approach is somewhat abstract, and the ``solution'' for the optimum predictor is expressed as a certain inner-product in a Hilbert space. \cite{Cramer1961a} considered the same problem, and obtained some interesting results in the form of ``existence theorems'', but did not present a method for determining the explicit form of a predictor in terms of the observed variables. Similar remarks apply to the work of \cite{Davis1952}. On the other hand, several papers written from an engineering standpoint have appeared (see, for example, \cite{Booton1952}, \cite{Zadeh1953}, \cite{Bendat1956}), but in most cases the ``general solution'' stops with the construction of the well-known integral equation (cf. \eqref{eq:covariance_equation}) involving the covariance function of the process and the unknown ``coefficients'' of the optimal predictor, the solution of which is obtainable only when the process obeys some very simple model. On a different level, \cite{Kalman1960} has studied multivariate non-stationary processes corresponding to dynamical systems which are governed by known linear differential equations, and has established some basic results regarding the analytic structure of optimal predictors for such models. However, from the point of view of practical application, the most useful results so far obtained are due to \cite{Whittle1965}, who considered non-stationary processes generated by autoregressive models with time-dependent coefficients, and obtained explicit recursive relations for the optimal predictors. In fact, some of the results for these particular models correspond very closely to those obtained by Whittle.

The success of classical prediction theory for stationary processes is due essentially to the fact that such processes admit a spectral representation in terms of an orthogonal process. This feature not only simplifies the solution of the prediction problem, but also enables one to treat a general class of stationary processes by means of a ``canonical'' representation, so that the discussion need not be restricted to particular models such as the autoregressive, moving-average, etc. Hitherto, the lack of a similar spectral representation for a general class of non-stationary processes has no doubt been one of the major stumbling blocks in attempts to generalize the classical theory. However, it turns out that the recently developed theory of evolutionary spectra representations \cite{Priestley1965} provides an ideal framework for the formulation and solution of non-stationary prediction problems. In fact, by using evolutionary spectral representations one obtains a prediction theory which is almost an exact parallel of the Wiener-Kolmogorov theory. The basic idea underlying this approach is the introduction of the evolutionary (i.e. time-dependent) spectrum of a non-stationary process, whose form completely determines the values of the coefficients of the optimal linear predictor. This means that even if observations from a process whose structure is completely unknown are presented, this prediction theory may still be usefully applied by first estimating the evolutionary spectrum (methods for estimating evolutionary spectra have been discussed by \cite{Priestley1965}, \cite{Priestley1966}). Of course, this estimation procedure introduces further complications which will not be discussed here (see also Section \ref{sec:discussion}). Throughout this paper it is assumed that the second-order properties of the process are known a priori. It should be pointed out, further, that in this account the aim is not at complete mathematical rigour; rather the aim is to show how this approach may be used to obtain explicit results for non-stationary processes.

Before discussing the prediction problem, relevant parts of the theory of evolutionary spectra are summarized briefly.

\section{Evolutionary Spectral Representations}
\label{sec:evolutionary_spectral}

A class of continuous parameter processes, $\{X(t)\}$, with $E\{X(t)\}=0$, $E\{X^{2}(t)\}<\infty$, for all $t$, is considered, for which there exists a family $\mathcal{F}$ of functions $\{\phi_{t}(\omega)\}$ (defined on the real line and indexed by the suffix $t$) and a measure $\mu$ on the real line such that $\{X(t)\}$ admits a spectral representation of the form
\begin{equation}
\label{eq:evolutionary_representation_continuous}
X(t)=\int_{-\infty}^{\infty}\phi_{t}(\omega)\,d Z(\omega)
\end{equation}
(See also \cite{Granger1964}.) Here $\{Z(\omega)\}$ is an orthogonal process with $E\big[|d Z(\omega)|^{2}\big]=\mu(d\omega)$. If there exists a family $\mathcal{F}$ for which each $\phi_{t}$ may be written in the form
\begin{equation}
\label{eq:oscillatory_condition}
\phi_{t}(\omega)=e^{i\omega t}A_{t}(\omega)
\end{equation}
where, for each fixed $\omega$, the generalized Fourier transform of $A_{t}(\omega)$ (considered as a function of $t$) has an absolute maximum at the origin, $X(t)$ is termed an oscillatory process (cf. \cite{Priestley1965}), and the evolutionary spectrum at time $t$ with respect to the family $\mathcal{F}$ is defined by
\begin{equation}
\label{eq:evolutionary_spectrum_continuous}
d F_{t}(\omega)=\big|A_{t}(\omega)\big|^{2}\mu(d\omega)\quad(-\infty<\omega<\infty)
\end{equation}

For discrete parameter oscillatory processes (i.e. processes defined only for $t=0,\pm1,\pm2,\ldots$) there is a corresponding evolutionary spectral representation
\begin{equation}
\label{eq:evolutionary_representation_discrete}
X(t)=\int_{-\pi}^{\pi}e^{i\omega t}A_{t}(\omega)\,d Z(\omega)
\end{equation}
and the evolutionary spectrum at time $t$ with respect to the family $\mathcal{F}$ is defined by
\begin{equation}
\label{eq:evolutionary_spectrum_discrete}
d F_{t}(\omega)=\big|A_{t}(\omega)\big|^{2}\mu(d\omega)\quad(-\pi\leqslant\omega\leqslant\pi)
\end{equation}
When the measure $\mu$ is absolutely continuous (with respect to Lebesgue measure) the derivative $F_{t}^{\prime}(\omega)=f_{t}(\omega)$ exists for all $\omega$, and is termed the evolutionary spectral density function at time $t$.

In both the continuous and discrete cases the functions $A_{t}(\omega)$ may be normalized so that $A_{0}(\omega)=1$ for all $\omega$. With this convention, $\mu(d\omega)=dF_{0}(\omega)$, the evolutionary spectrum at time zero.

\section{Moving-Average Representations for Oscillatory Processes}
\label{sec:moving_average}

As in the prediction theory of stationary processes, the construction begins with a one-sided moving-average representation for a class of oscillatory processes, the distinction with the stationary case being that the coefficients in the moving-average scheme are now time-dependent.

\subsection{Discrete-parameter Processes}
\label{subsec:discrete_moving_average}

Suppose that $X(t)$ has a representation of the form \eqref{eq:evolutionary_representation_discrete}, and that $\mu$ is absolutely continuous with respect to $d\omega$. Then $f_{t}(\omega)$ may be written in the form
\begin{equation}
\label{eq:moving_average_density}
f_{t}(\omega)=\big|A_{t}(\omega)\big|^{2}f(\omega)
\end{equation}
where $f(\omega)=d\mu/d\omega$. Note that $f(\omega)=f_{0}(\omega)$ must be integrable. Suppose now that
\begin{equation}
\label{eq:condition_C1}
\tag{C$_1$}
\int_{-\pi}^{\pi}\log f(\omega)\,d\omega>-\infty
\end{equation}
Then it follows that \cite[Doob, 1953, p.~160]{Doob1953} there exists a function $\psi(\omega)$ such that
\begin{equation}
\label{eq:psi_squared}
|\psi(\omega)|^{2}=f(\omega)
\end{equation}
where $\psi(z)$, considered as a function of the complex variable $z=e^{i\omega}$, has no poles or zeros inside the unit circle $|z|<1$. The function $\psi(\omega)$ may be written as a one-sided Fourier transform, viz.
\begin{equation}
\label{eq:psi_fourier}
\psi(\omega)=\sum_{u=0}^{\infty}e^{-i\omega u}g^{*}(u)
\end{equation}
for some suitable sequence $\{g^{*}(u)\}$.

Suppose further that:
\begin{enumerate}[label=(C\arabic*),start=2]
\item \label{eq:condition_C2} The family $\mathcal{F}$ can be chosen so that, for each $t$, $A_{t}(z)$ (considered as a function of $z$) also has no poles or zeros inside the unit circle, so that, for each $t$, $A_{t}(\omega)$ may be written in the form,
\begin{equation}
\label{eq:A_t_fourier}
A_{t}(\omega)=\sum_{u=0}^{\infty}e^{-i\omega u}g_{t}(u)
\end{equation}
\end{enumerate}
(It is remarked that $A_{t}(\omega)$ is certainly square-integrable, since $\big|A_{t}(\omega)\big|^{2}f(\omega)=f_{t}(\omega)$ is integrable, and by condition \eqref{eq:condition_C1}, $f(\omega)$ may vanish on at most a set of zero measure.)

Note that a necessary condition for the validity of \eqref{eq:A_t_fourier} is
\begin{equation}
\label{eq:condition_C3}
\tag{C$_3$}
\int_{-\pi}^{\pi}\log\big|A_{t}(\omega)\big|^{2}\,d\omega>-\infty
\end{equation}
for all $t$. Now write $X(t)$ in the form
\begin{equation}
\label{eq:X_t_representation}
X(t)=\int_{-\pi}^{\pi}e^{i t\omega}\alpha_{t}(\omega)\,d z(\omega)
\end{equation}
where $\alpha_{t}(\omega)=A_{t}(\omega)\psi(\omega)$, so that
\begin{equation}
\label{eq:alpha_t_squared}
\big|\alpha_{t}(\omega)\big|^{2}=f_{t}(\omega)
\end{equation}
and $z(\omega)$ is an orthogonal process on $(-\pi,\pi)$ with $E|d z(\omega)|^{2}=d\omega$. Since both $\psi(\omega)$ and $A_{t}(\omega)$ have one-sided Fourier transforms, it follows that, for each $t$, $\alpha_{t}(\omega)$ has a one-sided Fourier transform, i.e. one may write
\begin{equation}
\label{eq:alpha_t_fourier}
\alpha_{t}(\omega)=\sum_{u=0}^{\infty}e^{-i\omega u}h_{t}(u)
\end{equation}
Note that a necessary condition for the validity of \eqref{eq:alpha_t_fourier} is
\begin{equation}
\label{eq:condition_C4}
\tag{C$_4$}
\int_{-\pi}^{\pi}\log f_{t}(\omega)\,d\omega>-\infty
\end{equation}
for each $t$. In fact, it is readily seen that \eqref{eq:condition_C1} and \eqref{eq:condition_C3}$\leftrightarrow$\eqref{eq:condition_C4}, but whereas in the stationary case \eqref{eq:condition_C4} is both necessary and sufficient to ensure the validity of \eqref{eq:X_t_representation} and \eqref{eq:alpha_t_fourier}, sufficiency has not been proved in the non-stationary case. Condition \eqref{eq:condition_C4} implies that there exist functions $\{\alpha_{t}^{*}(\omega)\}$, say, satisfying \eqref{eq:alpha_t_fourier}, and such that, for each $t$, $f_{t}(\omega)=\big|\alpha_{t}^{*}(\omega)\big|^{2}$, so that $A_{t}(\omega)=e^{i\lambda(t,\omega)}\alpha_{t}^{*}(\omega)/\psi^{*}(\omega)$, for some function $\lambda(t,\omega)$. In the stationary case, $\exp\{i\lambda(t,\omega)\}$, being independent of $t$, may be incorporated with $d z(\omega)$, and sufficiency follows. In the non-stationary case it would appear that further conditions are required. (It is interesting to note, however, that Cramer's result \cite[Theorem~6]{Cramer1961a} for the prediction of harmonizable processes has the same feature. That is, Cramer has proved only that when the spectrum of a harmonizable process satisfies a condition of the type \eqref{eq:condition_C4}, the process must be ``deterministic'', i.e. that in such cases a one-sided moving-average representation is not admissible.)

Now define the process $\{\xi(t)\}$ by
\begin{equation}
\label{eq:xi_process}
\xi(t)=\int_{-\pi}^{\pi}e^{i t\omega}\,d z(\omega)\quad(t=0,\pm1,\pm2,\ldots)
\end{equation}
so that $\{\xi(t)\}$ is a stationary uncorrelated process, with
\begin{equation}
\label{eq:xi_properties}
\left.\begin{array}{c}
E\{\xi(t)\}=0\quad\text{for all }t,\\
E\{\big|\xi(t)\big|^{2}\}=2\pi\quad\text{for all }t,\\
E\{\xi(t)\xi^{*}(s)\}=0\quad(t\neq s).
\end{array}\right\}
\end{equation}
It now follows from \eqref{eq:X_t_representation} and \eqref{eq:alpha_t_fourier} that $X(t)$ may be written
\begin{equation}
\label{eq:moving_average_discrete}
X(t)=\sum_{u=0}^{\infty}h_{t}(u)\,\xi(t-u)
\end{equation}
the above expression existing as a mean-square limit in virtue of the condition
\begin{equation}
\label{eq:variance_condition}
2\pi\sum_{u=0}^{\infty}h_{t}^{2}(u)=\int_{-\pi}^{\pi}\big|\alpha_{t}(\omega)\big|^{2}\,d\omega=\int_{-\pi}^{\pi}f_{t}(\omega)\,d\omega=\operatorname{var}\{X(t)\}<\infty
\end{equation}

\begin{theorem}
\label{thm:moving_average_discrete}
Let $\{X(t)\}$ be a discrete-parameter oscillatory process. If there exists a family $\mathcal{F}$ satisfying condition \eqref{eq:condition_C2}, and with respect to which $\{X(t)\}$ has an absolutely continuous evolutionary spectrum satisfying condition \eqref{eq:condition_C4}, then $\{X(t)\}$ may be represented as a one-sided moving average process of the form \eqref{eq:moving_average_discrete}. Conversely, if $\{X(t)\}$ has a one-sided moving average representation of the form \eqref{eq:moving_average_discrete}, condition \eqref{eq:condition_C4} must be satisfied.
\end{theorem}

\begin{proof}
The proof follows from the construction above and the conditions ensuring the validity of the Fourier representations.
\end{proof}

\subsection{Continuous-parameter Processes}
\label{subsec:continuous_moving_average}

As in the case of stationary processes (see \cite{Whittle1963}), the results for discrete parameter processes can readily be adapted to the continuous case. The measure $\mu$ is again assumed to be absolutely continuous (with respect to Lebesgue measure) and, in place of \eqref{eq:condition_C1}, it is assumed that (using the same notation as in \eqref{eq:moving_average_density})
\begin{equation}
\label{eq:continuous_condition_C1_star}
\tag{C$_1^{*}$}
\int_{-\infty}^{\infty}\frac{\log f(\omega)}{1+\omega^{2}}\,d\omega>-\infty
\end{equation}
Then there exists a function $\psi(\omega)$ such that $|\psi(\omega)|^{2}=f(\omega)$, with $\psi(z)$ having no poles or zeros in the lower half-plane. The function $\psi(\omega)$ may now be written as a one-sided Fourier integral. Corresponding to \eqref{eq:condition_C2} it is assumed now that, for each $t$, $A_{t}(\omega)$ may be written in the form
\begin{equation}
\label{eq:continuous_A_t_fourier}
A_{t}(\omega)=\int_{0}^{\infty}e^{-i\omega u}g_{t}(u)\,du
\end{equation}
a necessary condition being
\begin{equation}
\label{eq:condition_C3_star}
\tag{C$_3^{*}$}
\int_{-\infty}^{\infty}\frac{\log\big|A_{t}(\omega)\big|^{2}}{1+\omega^{2}}\,d\omega>-\infty
\end{equation}
for all $t$. It then follows that $X(t)$ may be written in the form
\begin{equation}
\label{eq:X_t_continuous_representation}
X(t)=\int_{-\infty}^{\infty}e^{i t\omega}\alpha_{t}(\omega)\,d z(\omega)
\end{equation}
where $z(\omega)$ is an orthogonal process on $(-\infty,\infty)$, with $E\big[|d z(\omega)|^{2}\big]=d\omega$, and $\alpha_{t}(\omega)$ has a one-sided Fourier integral representation of the form
\begin{equation}
\label{eq:alpha_t_continuous_fourier}
\alpha_{t}(\omega)=\int_{0}^{\infty}e^{-i\omega u}h_{t}(u)\,du
\end{equation}
with $\big|\alpha_{t}(\omega)\big|^{2}=f_{t}(\omega)$. Corresponding to \eqref{eq:condition_C4}, a necessary condition for the validity of \eqref{eq:alpha_t_continuous_fourier} is
\begin{equation}
\label{eq:condition_C4_star}
\tag{C$_4^{*}$}
\int_{-\infty}^{\infty}\frac{\log f_{t}(\omega)}{1+\omega^{2}}\,d\omega>-\infty
\end{equation}
for all $t$. In the continuous case there are analytic difficulties in constructing an analogue of the uncorrelated process $\{\xi(t)\}$ (see \cite{Whittle1963}, p.~21). However, one may formally define the process $\xi(t)$ by
\begin{equation}
\label{eq:xi_continuous_process}
\xi(t)=\int_{-\infty}^{\infty}e^{i\omega t}\,d z(\omega)
\end{equation}
(so that $\xi(t)$ may be interpreted as the ``derivative'' of a Wiener process), so that
\begin{equation}
\label{eq:xi_continuous_properties}
\left.\begin{array}{c}
E\{\xi(t)\}=0\quad\text{for all }t,\\
E\{\big|d\xi(t)\big|^{2}\}=2\pi\,dt,\\
E\{d\xi(t)d\xi^{*}(s)\}=0\quad(s\neq t).
\end{array}\right\}
\end{equation}
Then formally one may write $X(t)$ in the form
\begin{equation}
\label{eq:X_t_continuous_moving_average}
X(t)=\int_{0}^{\infty}h_{t}(u)\,\xi(t-u)\,du
\end{equation}
or, more precisely,
\begin{equation}
\label{eq:X_t_continuous_integral}
X(t)=\int_{0}^{\infty}h_{t}(u)\,d\xi(t-u)
\end{equation}
As in case \ref{subsec:discrete_moving_average}, $\operatorname{var}\{X(t)\}<\infty$ implies
\begin{equation}
\int_{0}^{\infty}h_{t}^{2}(u)\,du<\infty
\end{equation}
for all $t$.

\section{The Time-Domain Approach}
\label{sec:time_domain}

The basic problem of linear least-squares prediction may be stated as follows: the observed values of the process over the semi-infinite interval $(-\infty,t)$ are given and the value of $X(t+m)$ $(m>0)$ is to be predicted. The predictor $\tilde{X}(t+m)$ is to be chosen as that linear combination of $\{X(s), s\leqslant t\}$ which is such that
\begin{equation}
\label{eq:prediction_problem}
M(m)=E\{\tilde{X}(t+m)-X(t+m)\}^{2}
\end{equation}
is minimized.

The existence and uniqueness of the optimum predictor $\tilde{X}(t+m)$ can be established by the Hilbert space approach, using exactly the same arguments as employed in the stationary case (see \cite{Parzen1961}, \cite{Yaglom1962}). (Briefly, one represents the process as a curve in a Hilbert space, and shows that $\tilde{X}(t+m)$ is the ``projection'' of $X(t+m)$ on the smallest subspace containing $\{X(s), s\leqslant t\}$.) However, assuming that the process has a one-sided moving average form, an explicit expression for $\tilde{X}(t+m)$ may easily be obtained in terms of the process $\{\xi(t)\}$. The continuous and discrete cases are treated by essentially the same argument, but the details differ and the discrete case is considered first.

\subsection{Discrete-parameter Processes}
\label{subsec:discrete_time_domain}

It is assumed that $X(t)$ has a one-sided moving average representation of the form \eqref{eq:moving_average_discrete}, so that one may write
\begin{equation}
\label{eq:X_t_moving_average_form}
X(t)=\sum_{u=0}^{\infty}h_{t}(u)\,\xi(t-u)=\sum_{u=-\infty}^{t}h_{t}(t-u)\,\xi(u)
\end{equation}
Then
\begin{equation}
\label{eq:X_t_m_expression}
X(t+m)=\sum_{u=-\infty}^{t}h_{t+m}(t+m-u)\,\xi(u)+\sum_{u=t+1}^{t+m}h_{t+m}(t+m-u)\,\xi(u)
\end{equation}
Now the predictor $\tilde{X}(t+m)$ is to be chosen as a linear combination of $\{X(s), s\leqslant t\}$ of the form
\begin{equation}
\label{eq:predictor_form}
\tilde{X}(t+m)=\sum_{s=-\infty}^{t}b(s)X(s)
\end{equation}
However, as each $X(t)$ is a linear combination of the $\{\xi(t-u)\}$ $(u\geqslant 0)$, one may equally well express $\tilde{X}(t+m)$ as a linear combination of $\{\xi(s), s\leqslant t\}$ of the form
\begin{equation}
\label{eq:predictor_xi_form}
\tilde{X}(t+m)=\sum_{u=-\infty}^{t}a(u)\,\xi(u)
\end{equation}
The problem now reduces to finding the values of the coefficients $\{a(u)\}$ which minimize $M(m)$, and, exactly as in the stationary case, the solution follows immediately from \eqref{eq:X_t_m_expression} (cf. \cite{Whittle1963}, p.~32; \cite{Bartlett1955}, p.~199). For, in view of the orthogonality of the $\xi$'s it follows (either by direct substitution of \eqref{eq:predictor_xi_form} and \eqref{eq:X_t_m_expression} in \eqref{eq:prediction_problem}, or by noting that the second term in \eqref{eq:X_t_m_expression} is orthogonal to the first term) that
\begin{equation}
\label{eq:optimal_predictor}
\tilde{X}(t+m)=\sum_{u=-\infty}^{t}h_{t+m}(t+m-u)\,\xi(u)=\sum_{u=m}^{\infty}h_{t+m}(u)\,\xi(t+m-u)
\end{equation}
so that, in terms of \eqref{eq:predictor_xi_form},
\begin{equation}
a(u)=h_{t+m}(t+m-u)
\end{equation}
for all $u$. The ``prediction variance'', $M(m)$, is given by
\begin{equation}
\label{eq:prediction_variance}
M(m)=2\pi\sum_{u=0}^{m-1}h_{t+m}^{2}(u)
\end{equation}

In practice, only the values of $X(s)$ $(s\leqslant t)$ are observed, so that in order to compute $\tilde{X}(t+m)$ the expression \eqref{eq:optimal_predictor} must be expressed in terms of the $X$'s rather than the $\xi$'s. Now write
\begin{equation}
\label{eq:xi_in_terms_of_X}
\xi(t)=\sum_{v=0}^{\infty}k_{t}(v)X(t-v)
\end{equation}
Then, substituting \eqref{eq:xi_in_terms_of_X} in \eqref{eq:X_t_moving_average_form} and equating coefficients of $\xi(t)$, one obtains
\begin{equation}
\label{eq:convolution_relation}
\sum_{v=0}^{p}h_{t-v}(p-v)k_{t}(v)=\delta_{p,0}\quad(p=0,1,2,\ldots)
\end{equation}
for all $t$. Thus, given the sequence $\{h_{t}(u)\}$, for all $t$, one may, in principle, solve \eqref{eq:convolution_relation} for $\{k_{t}(v)\}$. When $X(t)$ is stationary, $h_{t}(u)$ and $k_{t}(v)$ are both independent of $t$ and \eqref{eq:convolution_relation} is easily solved by introducing the generating functions of the two sequences \cite{Whittle1963, p.~32}. In general, there does not appear to be any systematic relationship between corresponding terms of the two sequences, and the most practical approach would seem to be the method of repeated back substitution. That is, if \eqref{eq:convolution_relation} is written in detail one has
\begin{equation}
\label{eq:triangular_system}
\begin{aligned}
p=0&: \quad h_{t}(0)k_{t}(0)=1\quad\text{for all }t,\\
p=1&: \quad h_{t}(1)k_{t}(0)+h_{t-1}(0)k_{t}(1)=0\quad\text{for all }t,\\
p=2&: \quad h_{t}(2)k_{t}(0)+h_{t-1}(1)k_{t}(1)+h_{t-2}(2)k_{t}(2)=0\quad\text{for all }t,\quad\text{etc.}
\end{aligned}
\end{equation}
This system of equations is easily solved, step by step, as it is triangular. Thus one finds, for all $t$,
\begin{equation}
\begin{aligned}
k_{t}(0)&=1/h_{t}(0),\\
k_{t}(1)&=-h_{t}(1)/h_{t}(0)h_{t-1}(0),\\
k_{t}(2)&=\{-h_{t}(2)/h_{t}(0)h_{t-2}(2)\}+\{h_{t}(1)h_{t-1}(1)/h_{t}(0)h_{t-1}(0)h_{t-2}(0)\},\\
&\cdots\quad\text{etc.}
\end{aligned}
\end{equation}
Alternatively, one may substitute \eqref{eq:X_t_moving_average_form} in \eqref{eq:xi_in_terms_of_X} and equate coefficients, obtaining
\begin{equation}
\label{eq:inverse_convolution}
\sum_{v=0}^{p}k_{t-v}(p-v)h_{t}(v)=\delta_{p,0}\quad(p=0,1,2,\ldots)
\end{equation}
for all $t$. \cite{Whittle1965} has pointed out that the two systems of equations \eqref{eq:convolution_relation} and \eqref{eq:inverse_convolution} are not identical unless the process is stationary (in which case both $\{h_{t}(u)\}$ and $\{k_{t}(v)\}$ are independent of $t$), but that one set of equations still implies the other. However, the triangular form of both \eqref{eq:convolution_relation} and \eqref{eq:inverse_convolution} means that these systems of equations are ideally suited for numerical solution.

\subsection{Continuous-parameter Processes}
\label{subsec:continuous_time_domain}

The results for continuous-parameter processes are very similar to those obtained in the discrete case. Thus one is given $\{X(s), -\infty<s\leqslant t\}$ and wishes to predict $X(t+m)$ $(m>0)$ by a linear combination of past values, of the form
\begin{equation}
\label{eq:continuous_predictor_integral}
\tilde{X}(t+m)=\int_{-\infty}^{t}X(s)b(s)\,ds
\end{equation}
or, equivalently,
\begin{equation}
\label{eq:continuous_predictor_xi}
\tilde{X}(t+m)=\int_{-\infty}^{t}\xi(u)a(u)\,du
\end{equation}
Assuming that $X(t)$ has a moving-average representation of the form \eqref{eq:X_t_continuous_integral}, it follows immediately that
\begin{equation}
\label{eq:continuous_optimal_predictor}
\tilde{X}(t+m)=\int_{-\infty}^{t}h_{t+m}(t+m-u)\,\xi(u)\,du=\int_{m}^{\infty}h_{t+m}(u)\,\xi(t+m-u)\,du
\end{equation}
In order to express $\tilde{X}(t+m)$ in terms of $\{X(s), s\leqslant t\}$ one writes
\begin{equation}
\label{eq:xi_continuous_in_terms_of_X}
\xi(t)=\int_{0}^{\infty}k_{t}(v)X(t-v)\,dv
\end{equation}
and substitutes \eqref{eq:xi_continuous_in_terms_of_X} in \eqref{eq:X_t_continuous_integral}. One then obtains
\begin{equation}
\label{eq:integral_equation_1}
\int_{0}^{v}h_{t}(u)k_{t-u}(v-u)\,du=\delta(v)
\end{equation}
for all $v>0$ and all $t$, where $\delta(v)$ denotes the Dirac $\delta$-function. Similarly, substituting \eqref{eq:X_t_continuous_integral} in \eqref{eq:xi_continuous_in_terms_of_X}, one finds
\begin{equation}
\label{eq:integral_equation_2}
\int_{0}^{u}k_{t}(v)h_{t-v}(u-v)\,dv=\delta(u)
\end{equation}
for all $u>0$ and all $t$. Given the functions $\{h_{t}(u)\}$ (for all $t$), the functions $\{k_{t}(v)\}$ may, in principle, be determined from either of the above integral equations.

In certain cases the time-domain approach may be used to obtain an expression of the form \eqref{eq:continuous_predictor_integral} or \eqref{eq:continuous_predictor_xi} for $\tilde{X}(t+m)$, i.e. an expression in terms of past values of $X(t)$, without solving either of the systems \eqref{eq:integral_equation_1} or \eqref{eq:integral_equation_2} directly. Such cases include uniformly modulated processes, and, naturally enough, finite order autoregressive models (with time-dependent coefficients). Finite order moving-average models (with time-dependent coefficients) can be dealt with also by this method, but they are more suited to a frequency-domain approach (see Section \ref{sec:frequency_domain}). Some specific examples are now discussed.

\paragraph{Example 1: Uniformly modulated process.}
Consider a continuous-parameter process of the form
\begin{equation}
\label{eq:uniformly_modulated_process}
X(t)=C(t)\,Y(t)
\end{equation}
where $Y(t)$ is a stationary process with spectral density function $f_{y}(\omega)$, say, and $C(t)$ is some given function of $t$. Then, with respect to the family $\mathcal{F}\equiv\{e^{i\omega t}C(t)\}$, the evolutionary spectrum of $\{X(t)\}$ is given by \cite{Priestley1965}
\begin{equation}
\label{eq:evolutionary_spectrum_modulated}
f_{t}(\omega)=\big|C(t)\big|^{2}f_{y}(\omega)
\end{equation}
Assume that $f_{t}(\omega)$ satisfies condition \eqref{eq:condition_C4_star}. Then it follows that
\begin{equation}
\int_{-\infty}^{\infty}\frac{\log f_{y}(\omega)}{1+\omega^{2}}\,d\omega>-\infty
\end{equation}
so that $Y(t)$ has a moving-average representation of the form
\begin{equation}
\label{eq:moving_average_Y}
Y(t)=\int_{0}^{\infty}g(u)\,\xi(t-u)\,du
\end{equation}
Hence one may write $X(t)$ in the form \eqref{eq:X_t_continuous_integral}, i.e.
\begin{equation}
\label{eq:moving_average_X}
X(t)=\int_{0}^{\infty}h_{t}(u)\,\xi(t-u)\,du
\end{equation}
where $h_{t}(u)=C(t)g(u)$. According to \eqref{eq:continuous_optimal_predictor}, the linear least-squares predictor of $X(t+m)$ is given by
\begin{equation}
\begin{aligned}
\tilde{X}(t+m)&=\int_{m}^{\infty}C(t+m)g(u)\,\xi(t+m-u)\,du\\
&=C(t+m)\,\tilde{Y}(t+m)
\end{aligned}
\end{equation}
where $\tilde{Y}(t+m)$ is the least-square predictor of $Y(t+m)$, given $\{Y(s), s\leqslant t\}$, and may be determined in the usual way. The referee has pointed out that this result has a more general form, namely: if
\begin{equation}
X(t)=\sum_{u=0}^{\infty}a_{t}(u)Y(t-u),\quad\text{then}\quad\tilde{X}(t+m)=\sum_{u=0}^{\infty}a_{t+m}(u)Y^{*}(t+m-u)
\end{equation}
where
\begin{equation}
Y^{*}(s)=\begin{cases}
Y(s) & (s\leqslant t),\\
\tilde{Y}(s) & (s>t).
\end{cases}
\end{equation}
For example, using the Wiener predictor (see \cite{Bartlett1955}, p.~201),
\begin{equation}
\tilde{Y}(t+m)=\int_{0}^{\infty}Y(t-u)b(u)\,du
\end{equation}
where
\begin{equation}
b(u)=(2\pi)^{-1}\int_{-\infty}^{\infty}e^{i\omega u}B(\omega)\,d\omega
\end{equation}
and
\begin{equation}
B(\omega)=\frac{e^{i\omega m}\int_{m}^{\infty}g(u)e^{-i\omega u}\,du}{\int_{0}^{\infty}g(u)e^{-i\omega u}\,du}
\end{equation}
Thus
\begin{equation}
\tilde{X}(t+m)=\int_{0}^{\infty}X(t-u)b^{*}(u)\,du
\end{equation}
where
\begin{equation}
b^{*}(u)=\{C(t+m)/C(t-u)\}\{(2\pi)^{-1}\int_{-\infty}^{\infty}e^{i\omega u}B(\omega)\,d\omega\}
\end{equation}

\paragraph{Example 2: First-order autoregressive process, discrete-parameter.}
Suppose $\{X(t)\}$ is given by
\begin{equation}
\label{eq:first_order_autoregressive}
X(t)-\alpha(t)X(t-1)=\xi(t)\quad(t=0,\pm1,\pm2,\ldots)
\end{equation}
where $\{\alpha(t)\}$ is a given sequence with $|\alpha(t)|<1$ for all $t$, and $\{\xi(t)\}$ is an uncorrelated stationary process with $E\{\xi(t)\}=0$, $E\{\xi^{2}(t)\}=1$. From \eqref{eq:first_order_autoregressive} one finds (subject to $X(-\infty)=0$)
\begin{equation}
\label{eq:autoregressive_solution}
X(t)=\sum_{u=0}^{\infty}h_{t}(u)\,\xi(t-u)
\end{equation}
where
\begin{equation}
h_{t}(u)=\begin{cases}
\alpha(t)\alpha(t-1)\cdots\alpha(t-u+1) & (u>0),\\
1 & (u=0).
\end{cases}
\end{equation}
Hence, according to \eqref{eq:optimal_predictor}, the linear least-squares predictor of $X(t+m)$ is given by
\begin{equation}
\begin{aligned}
\tilde{X}(t+m)&=\sum_{u=m}^{\infty}\alpha(t+m)\alpha(t+m-1)\cdots\alpha(t+m-u+1)\,\xi(t+m-u)\\
&=\{\alpha(t+m)\cdots\alpha(t+1)\}\sum_{u=0}^{\infty}\alpha(t)\cdots\alpha(t-u+1)\,\xi(t-u)\\
&=\{\alpha(t+m)\cdots\alpha(t+1)\}X(t)
\end{aligned}
\end{equation}
(Note that this is the exact expression for $X(t+m)$ when $\xi(s)=0$ for all $s>t$.)

\paragraph{Example 3: First-order moving-average process, discrete-parameter.}
Let $\{X(t)\}$ be given by
\begin{equation}
\label{eq:first_order_moving_average}
X(t)=\xi(t)-\alpha(t)\xi(t-1)\quad(t=0,\pm1,\pm2,\ldots)
\end{equation}
with $\{\xi(t)\}$ as above. Then clearly
\begin{equation}
\tilde{X}(t+m)=\begin{cases}
-\alpha(t+1)\,\xi(t) & (m=1),\\
0 & (m>1),
\end{cases}
\end{equation}
where
\begin{equation}
\xi(t)=\sum_{v=0}^{\infty}k_{t}(v)X(t-v)
\end{equation}
with $k_{t}(v)=\alpha(t)\alpha(t-1)\cdots\alpha(t-v+1)$.

\section{The Frequency-Domain Approach}
\label{sec:frequency_domain}

An alternative approach based on a generalization of the Wiener-Hopf technique for dealing with the prediction of stationary processes is now considered. The continuous-parameter case is treated first, and the predictor of $X(t+m)$ is written in the form
\begin{equation}
\label{eq:frequency_predictor}
\tilde{X}(t+m)=\int_{0}^{\infty}b_{t}(u)X(t-u)\,du
\end{equation}
Note that \eqref{eq:frequency_predictor} is merely a formal way of writing
\begin{equation}
\tilde{X}(t+m)=\int_{0}^{\infty}X(t-u)\,d\beta_{t}(u)
\end{equation}
The function $b_{t}(u)$ need not exist in the strict sense, as would be the case if, for example, $\tilde{X}(t+m)$ involved derivatives of $X(t-u)$. Note also that the weight-function $b_{t}(u)$ may depend on both $t$ and $m$. Now using the evolutionary spectral representation of $X(t)$, \eqref{eq:X_t_continuous_representation} and \eqref{eq:frequency_predictor} may be written
\begin{equation}
\tilde{X}(t+m)=\int_{-\infty}^{\infty}e^{i\omega t}B_{t}(\omega)\,d z(\omega)
\end{equation}
where
\begin{equation}
\label{eq:B_t_definition}
B_{t}(\omega)=\int_{0}^{\infty}b_{t}(u)\alpha_{t-u}(\omega)e^{-i u\omega}\,du
\end{equation}
On the other hand, from \eqref{eq:X_t_continuous_representation},
\begin{equation}
\label{eq:X_t_m_frequency}
X(t+m)=\int_{-\infty}^{\infty}e^{i\omega(t+m)}\alpha_{t+m}(\omega)\,d z(\omega)
\end{equation}
Hence, in virtue of the orthogonality of $z(\omega)$,
\begin{equation}
\label{eq:prediction_error_integral}
M(m)=E\big|\tilde{X}(t+m)-X(t+m)\big|^{2}=\int_{-\infty}^{\infty}\big|e^{i\omega m}\alpha_{t+m}(\omega)-B_{t}(\omega)\big|^{2}\,d\omega
\end{equation}
One must now choose $b_{t}(u)$ (or, equivalently, $B_{t}(\omega)$) so as to minimize \eqref{eq:prediction_error_integral}. As a preliminary step it is first shown that, for each $t$, the function $B_{t}(\omega)$ is a ``backward transform'', that is it may be written as a one-sided Fourier transform of the form
\begin{equation}
\label{eq:B_t_backward_transform}
B_{t}(\omega)=(2\pi)^{-1}\int_{-\infty}^{\infty}e^{-i v\omega}K_{t}(v)\,dv
\end{equation}
where $K_{t}(v)=0$ for $v<0$. For
\begin{equation}
\begin{aligned}
K_{t}(v)&=\int_{-\infty}^{\infty}e^{i\omega v}B_{t}(\omega)\,d\omega\\
&=\int_{0}^{\infty}b_{t}(u)\left\{\int_{0}^{\infty}e^{i\omega(v-u)}\alpha_{t-u}(\omega)\,d\omega\right\}du
\end{aligned}
\end{equation}
Since
\begin{equation}
\alpha_{t-u}(\omega)=\int_{0}^{\infty}e^{-i\omega w}h_{t-u}(w)\,dw
\end{equation}
one has
\begin{equation}
\int_{-\infty}^{\infty}e^{i\omega(v-u)}\alpha_{t-u}(\omega)\,d\omega=\begin{cases}
h_{t-u}(v-u) & (u\leqslant v),\\
0 & (u>v).
\end{cases}
\end{equation}
Hence
\begin{equation}
\label{eq:K_v_definition}
K_{t}(v)=\int_{0}^{v}b_{t}(u)h_{t-u}(v-u)\,du
\end{equation}
and, in particular, $K_{t}(v)=0$ for $v<0$.

Now write (for each $t$)
\begin{equation}
\label{eq:C1_C2_decomposition}
e^{i\omega m}\alpha_{t+m}(\omega)=C_{t}^{(1)}(\omega)+C_{t}^{(2)}(\omega)
\end{equation}
where, for each $t$, $C_{t}^{(1)}(\omega)$ is a ``backward transform'', and $C_{t}^{(2)}(\omega)$ is a ``forward transform'' \cite{Bartlett1955, p.~201}. That is, write
\begin{equation}
\label{eq:C1_definition}
C_{t}^{(1)}(\omega)=(2\pi)^{-1}\int_{0}^{\infty}e^{-i\omega u}\left[\int_{-\infty}^{\infty}e^{i\theta u}\{e^{i\theta m}\alpha_{t+m}(\theta)\}\,d\theta\right]du
\end{equation}
and
\begin{equation}
\label{eq:C2_definition}
C_{t}^{(2)}(\omega)=(2\pi)^{-1}\int_{-\infty}^{0}e^{-i\omega u}\left[\int_{-\infty}^{\infty}e^{i\theta u}\{e^{i\theta m}\alpha_{t+m}(\theta)\}\,d\theta\right]du
\end{equation}
Then, for each $t$, $\{C_{t}^{(1)}(\omega)-B_{t}(\omega)\}$ and $C_{t}^{(2)}(\omega)$ are orthogonal with respect to $d\omega$ integration, and from \eqref{eq:prediction_error_integral} one has
\begin{equation}
\label{eq:minimized_error}
M(m)=\int_{-\infty}^{\infty}\big|C_{t}^{(1)}(\omega)-B_{t}(\omega)\big|^{2}\,d\omega+\int_{-\infty}^{\infty}\big|C_{t}^{(2)}(\omega)\big|^{2}\,d\omega
\end{equation}
It follows immediately that the minimum of $M(m)$ is attained when $B_{t}(\omega)$ is given by
\begin{equation}
\label{eq:optimal_B_t}
B_{t}(\omega)=C_{t}^{(1)}(\omega)
\end{equation}
for all $t$, and the prediction variance is then given by
\begin{equation}
\label{eq:prediction_variance_C2}
M(m)=\int_{-\infty}^{\infty}\big|C_{t}^{(2)}(\omega)\big|^{2}\,d\omega
\end{equation}
It now remains to solve \eqref{eq:optimal_B_t} to find the functions $\{b_{t}(u)\}$. Taking Fourier transforms of both sides one finds, for $v>0$,
\begin{equation}
\begin{aligned}
K_{t}(v)&=\int_{-\infty}^{\infty}e^{i\theta(v+m)}\alpha_{t+m}(\theta)\,d\theta\quad\text{for all }t,\\
&=h_{t+m}(v+m).
\end{aligned}
\end{equation}
Hence, using \eqref{eq:K_v_definition}, one finds
\begin{equation}
\label{eq:integral_equation_b_t}
\int_{0}^{v}b_{t}(u)h_{t-u}(v-u)\,du=h_{t+m}(v+m)
\end{equation}
for all $t$ and all $v>0$. Given the form of the evolutionary spectral density function $f_{t}(\omega)$, for all $t$, one may (in principle) determine the function $h_{t}(u)$, for all $t$, and hence find the optimum weight-function $\{b_{t}(u)\}$ by solving the integral equation \eqref{eq:integral_equation_b_t}.

Equation \eqref{eq:integral_equation_b_t}, being an integral equation of the first kind, may be solved explicitly for $\{b_{t}(u)\}$. (Note that in the stationary case $f_{t}(\omega)$, and consequently $h_{t}(u)$, are both independent of $t$, so that \eqref{eq:integral_equation_b_t} may then be solved immediately by noting that the left-hand side is the convolution of $b_{t}(u)$ and $h_{t}(u)$.) In the general case one may solve \eqref{eq:integral_equation_b_t} by first expressing it as an integral equation of the second kind. Thus, if it is assumed that
\begin{equation}
h_{t-v}(0)\neq0\text{ for all }v\text{ in }(0,\infty)\text{ and }\frac{\partial h_{t-u}(v-u)}{\partial v}=h_{t-u,v}^{\prime}(v-u)\text{ exists and is continuous in }u\text{ and }v\ (u<v)
\end{equation}
then by differentiating both sides of \eqref{eq:integral_equation_b_t} with respect to $v$ one finds
\begin{equation}
\label{eq:differentiated_equation}
h_{t-v}(0)b_{t}(v)+\int_{0}^{v}h_{t-u,v}^{\prime}(v-u)b_{t}(u)\,du=h_{t+m,v}^{\prime}(v+m)\quad(v\geqslant0)
\end{equation}
or
\begin{equation}
\label{eq:volterra_equation}
b_{t}(v)+\int_{0}^{v}\frac{h_{t-u,v}^{\prime}(v-u)}{h_{t-v}(0)}b_{t}(u)\,du=\frac{h_{t+m,v}^{\prime}(v+m)}{h_{t-v}(0)}
\end{equation}
The solution of \eqref{eq:volterra_equation} is \cite{Tricomi1957, p.~10}
\begin{equation}
\label{eq:volterra_solution}
b_{t}(v)=\frac{h_{t+m,v}^{\prime}(v+m)}{h_{t-v}(0)}+\int_{0}^{v}H_{t}^{*}(v,u,-1)\left\{\frac{h_{t+m,v}^{\prime}(u+m)}{h_{t-u}(0)}\right\}du
\end{equation}
where, for each $t$, the ``resolvent kernel'' $H_{t}^{*}(v,u,-1)$ is given by
\begin{equation}
\label{eq:resolvent_kernel}
H_{t}^{*}(v,u,-1)=-\sum_{p=0}^{\infty}(-1)^{p}K_{t,p+1}^{*}(u,v)
\end{equation}
and the ``iterated kernels'' $K_{t,p}^{*}(u,v)$ are given by
\begin{equation}
K_{t,p+1}^{*}(u,v)=\int_{u}^{v}K_{t,1}^{*}(u,z)K_{t,p}^{*}(z,v)\,dz\quad(p=1,2,\ldots)
\end{equation}
with
\begin{equation}
K_{t,1}^{*}(u,v)=\frac{h_{t-u,v}^{\prime}(v-u)}{h_{t-v}(0)}
\end{equation}
Equation \eqref{eq:volterra_solution} provides an analytic expression for $\{b_{t}(u)\}$, but in numerical work it may well be more convenient to solve the integral equation \eqref{eq:integral_equation_b_t} directly.

For discrete-parameter processes the predictor $\tilde{X}(t+m)$ will be of the form
\begin{equation}
\label{eq:discrete_predictor_form}
\tilde{X}(t+m)=\sum_{u=0}^{\infty}b_{t}(u)X(t-u)
\end{equation}
so that, using the spectral representation \eqref{eq:X_t_representation},
\begin{equation}
\label{eq:discrete_prediction_error}
M(m)=\int_{-\pi}^{\pi}\big|e^{i\omega m}\alpha_{t+m}(\omega)-B_{t}(\omega)\big|^{2}\,d\omega
\end{equation}
where now
\begin{equation}
\label{eq:B_t_discrete}
B_{t}(\omega)=\sum_{u=0}^{\infty}b_{t}(u)\alpha_{t-u}(\omega)e^{-i u\omega}
\end{equation}
Following a similar argument to that used above, one may show that $M(m)$ is minimized when the sequence $\{b_{t}(u)\}$ is given by
\begin{equation}
\label{eq:discrete_convolution}
\sum_{u=0}^{v}b_{t}(u)h_{t-u}(v-u)=h_{t+m}(v+m)\quad(v=0,1,2,\ldots;\ t=0,\pm1,\pm2,\ldots)
\end{equation}
Here, $h_{t}(u)$ $(u=0,1,2,\ldots)$ is the sequence defined by \eqref{eq:alpha_t_fourier}.

The system of equations \eqref{eq:discrete_convolution} is again ``triangular'' (cf. \eqref{eq:triangular_system}) so that it is easily solved for $\{b_{t}(u)\}$ by repeated back-substitution. In fact, this approach has been tried on a numerical example (using an ATLAS computer) and has proved quite successful.

It is interesting to note that had one attempted to obtain $\{b_{t}(u)\}$ directly from the covariance function $R(s,t)\equiv E\{X(s)X^{*}(t)\}$, one would have obtained the well-known set of equations (see \cite{Bendat1959}, p.~149),
\begin{equation}
\label{eq:covariance_equation}
\sum_{u=0}^{\infty}b_{t}(u)R(t-u,t-v)=R(t-v,t+m)\quad(v\geqslant0)
\end{equation}
(In the continuous case one obtains the corresponding integral equation.)

The above system does not lend itself to either an analytic or a numerical solution and the spectral approach (cf. \eqref{eq:integral_equation_b_t}, \eqref{eq:discrete_convolution}) may be regarded as a method of reducing \eqref{eq:covariance_equation} to ``triangular'' form.

The methods of this section are now applied to some examples discussed in Section \ref{subsec:continuous_time_domain}.

\paragraph{Example 4.}
Consider the first-order autoregressive process (discrete-parameter) of \eqref{eq:first_order_autoregressive}. Using the form of $\{h_{t}(u)\}$ given by \eqref{eq:autoregressive_solution} in \eqref{eq:discrete_convolution}, one obtains
\begin{equation}
b_{t}(0)+\sum_{u=1}^{v}b_{t}(u)\alpha(t-u)\alpha(t-u-1)\cdots\alpha(t-v+1)=\alpha(t+m)\alpha(t+m-1)\cdots\alpha(t-v+1)\quad(v=0,1,2,\ldots)
\end{equation}
Solving this system step by step, one obtains
\begin{equation}
\begin{cases}
b_{t}(0)=\alpha(t+m)\alpha(t+m-1)\cdots\alpha(t+1),\\
b_{t}(u)=0\quad(u>0).
\end{cases}
\end{equation}
Hence
\begin{equation}
\tilde{X}(t+m)=\alpha(t+m)\alpha(t+m-1)\cdots\alpha(t+1)X(t)
\end{equation}
in agreement with Example 2.

\paragraph{Example 5.}
Consider now the moving-average process (discrete-parameter) of \eqref{eq:first_order_moving_average}. For this process,
\begin{equation}
h_{t}(u)=\begin{cases}
1 & (u=0),\\
-\alpha(t) & (u=1),\\
0 & (u>1),
\end{cases}
\end{equation}
for all $t$. Substituting this expression in \eqref{eq:discrete_convolution}, one finds
\begin{equation}
b_{t}(0)=\begin{cases}
-\alpha(t+1) & (m=1),\\
0 & (m>1),
\end{cases}
\end{equation}
and
\begin{equation}
b_{t}(u)h_{t-u}(1)+b_{t}(u+1)h_{t-u}(0)=0\quad(u=1,2,\ldots)
\end{equation}
for all $m$. The solution of this set of equations is, for all $u$,
\begin{equation}
b_{t}(u)=\begin{cases}
-\alpha(t+1)\alpha(t)\cdots\alpha(t-u+1) & (m=1),\\
0 & (m>1),
\end{cases}
\end{equation}
so that
\begin{equation}
\tilde{X}(t+m)=\begin{cases}
-\sum_{u=0}^{\infty}\alpha(t+1)\alpha(t)\cdots\alpha(t-u+1)X(t) & (m=1),\\
0 & (m>1),
\end{cases}
\end{equation}
in agreement with Example 3.

\section{Discussion}
\label{sec:discussion}

As mentioned in Section \ref{sec:introduction}, this paper is concerned primarily with the problem of determining the optimum predictor on the assumption that the second-order properties of the process are fully known. However, it has been shown that the predictor $\tilde{X}(t+m)$ is determined uniquely by the form of the evolutionary spectra $\{f_{t}(\omega)\}$, so that in cases where there is no precise knowledge of the underlying model the problem may be attacked by first estimating the functions $\{f_{t}(\omega)\}$. It will be seen from Section \ref{sec:frequency_domain} that $\tilde{X}(t+m)$ depends on the form of $h_{t+m}(u)$, i.e. on the form of the evolutionary spectrum at the future time point $(t+m)$. In a purely empirical analysis, one would not, of course, be able to estimate $f_{t+m}(\omega)$ directly from observation extending only up to time $t$, and one would have to assume that the spectra were changing with sufficient ``smoothness'' over time to enable the form of $f_{t+m}(\omega)$ to be inferred from the spectra which have been estimated up to time $t$. If the prediction step $m$ is much smaller than the characteristic width $B_{X}$ of the process (see \cite{Priestley1965}), it would be a reasonable approximation to replace $f_{t+m}(\omega)$ by $f_{t}(\omega)$---the most recently available spectrum. In fact, it seems clear intuitively that the very nature of a non-stationary process precludes long-range prediction, unless one is prepared to make assumptions about the character of the non-stationarity. Consequently, if $m>B_{X}$ one would have to extrapolate the available spectra to the time point $(t+m)$. This extrapolation could be performed on the spectral ordinates themselves (by fitting suitable functions of $t$ to a range of values of $\omega$), but in general one would presumably start from a model of the process which determines the form of the evolutionary spectra in terms of a set of time-varying parameters $\{\alpha_{i}(t)\}$, say, and then extrapolate the values of these parameters. (Thus, for the model of Example 2, the $\{f_{t}(\omega)\}$ may be expressed in terms of the time-varying parameter $\alpha(t)$.) If the model specifies further the functional time-dependence of the $\{\alpha_{i}(t)\}$ in terms of another set of constant parameters, $\{\beta_{j}\}$, say, then the extrapolation is a straightforward problem and involves merely the estimation of the $\{\beta_{j}\}$. (For example, in some cases one may postulate that the $\{\alpha_{i}(t)\}$ are periodic functions of $t$ with known periodicities.) If, on the other hand, the functional form of the $\{\alpha_{i}(t)\}$ is not specified by the model, the extrapolation would have to be performed by regression techniques.

\begin{thebibliography}{9}

\bibitem{Doob1953}
Doob, J.~L. (1953). \emph{Stochastic Processes}. New York: Wiley.

\bibitem{Bartlett1955}
Bartlett, M.~S. (1955). \emph{An Introduction to Stochastic Processes}. Cambridge: University Press.

\bibitem{Grenander1957}
Grenander, U. and Rosenblatt, M. (1957). \emph{Statistical Analysis of Stationary Time Series}. New York: Wiley.

\bibitem{Yaglom1962}
Yaglom, A.~M. (1962). \emph{An Introduction to the Theory of Stationary Random Functions}. Englewood Cliffs, N.J.: Prentice-Hall.

\bibitem{Whittle1965}
Whittle, P. (1965). Recursive relations for predictors of non-stationary processes. \emph{J.R.~Statist.~Soc.~B}, \textbf{27}, 523--532.

\bibitem{Parzen1961}
Parzen, E. (1961). An approach to time series analysis. \emph{Ann.~Math.~Statist.}, \textbf{32}, 951--989.

\bibitem{Cramer1961a}
Cram\'{e}r, H. (1961a). On some classes of non-stationary processes. \emph{Proc.~4th Berkeley Symp.~Math.~Statist.~\& Prob.}, \textbf{2}, 57--78.

\bibitem{Cramer1961b}
Cram\'{e}r, H. (1961b). On the structure of purely non-deterministic stochastic processes. \emph{Ark.~Mat.}, \textbf{19}, 249--266.

\bibitem{Davis1952}
Davis, R.~C. (1952). On the theory of prediction of non-stationary stochastic processes. \emph{J.~Appl.~Phys.}, \textbf{23}, 1047--1053.

\bibitem{Booton1952}
Booton, R.~C. (1952). An optimization theory for time-varying linear systems with nonstationary statistical inputs. \emph{Proc.~Inst.~Radio Engrs}, \textbf{40}, 977--981.

\bibitem{Zadeh1953}
Zadeh, L.~A. (1953). Optimum non-linear filters. \emph{J.~Appl.~Phys.}, \textbf{24}.

\bibitem{Bendat1956}
Bendat, J.~S. (1956). A general theory of linear prediction and filtering. \emph{J.~Soc.~Industr.~Appl.~Math.}, \textbf{4}, 131--151.

\bibitem{Bendat1957}
Bendat, J.~S. (1957). Exact integral equation solutions and synthesis for a large class of optimum time variable linear filters. \emph{Trans.~Inst.~Radio Engrs}, IT-3, 71--80.

\bibitem{Bendat1959}
Bendat, J.~S. (1959). \emph{Principles and Applications of Random Noise Theory}. New York: Wiley.

\bibitem{Kalman1960}
Kalman, R.~E. (1960). A new approach to linear filtering and prediction problems. \emph{J.~Basic Eng.~(Trans.~A.S.M.E., Series~D)}, \textbf{82}, 35--45.

\bibitem{Granger1964}
Granger, C.~W.~J. and Hatanaka, M. (1964). \emph{Spectral Analysis of Economic Time Series}. Princeton: University Press.

\bibitem{Priestley1965}
Priestley, M.~B. (1965). Evolutionary spectra and non-stationary processes. \emph{J.R.~Statist.~Soc.~B}, \textbf{27}, 204--237.

\bibitem{Priestley1966}
Priestley, M.~B. (1966). Design relations for non-stationary processes. \emph{J.R.~Statist.~Soc.~B}, \textbf{28}, 228--240.

\bibitem{Whittle1963}
Whittle, P. (1963). \emph{Prediction and Regulation}. London: English Universities Press.

\bibitem{Tricomi1957}
Tricomi, F.~G. (1957). \emph{Integral Equations}. New York: Wiley.

\end{thebibliography}

\end{document}
