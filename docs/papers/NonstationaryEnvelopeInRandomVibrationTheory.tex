\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,latexsym}

%%%%%%%%%% Start TeXmacs macros
\newcommand{\cdummy}{\cdot}
\newcommand{\tmem}[1]{{\em #1\/}}
\newcommand{\tmmathbf}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\tmnote}[1]{\thanks{\textit{Note:} #1}}
\newcommand{\tmtextbf}[1]{\text{{\bfseries{#1}}}}
\newcommand{\tmtextit}[1]{\text{{\itshape{#1}}}}
\newenvironment{proof}{\noindent\textbf{Proof\ }}{\hspace*{\fill}$\Box$\medskip}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
%%%%%%%%%% End TeXmacs macros

\begin{document}

\title{Nonstationary Envelope in Random Vibration Theory: A Theorem-Proof
Reformulation}

\author{
  Giuseppe Muscolino(Original Author)
  \tmnote{Researcher, Dipartimento di Ingegneria Strutturale e Geotecnica,
  Universit{\`a} degli Studi di Palermo, Viale delle Scienze, I-90128 Palermo,
  Italy.}
}

\date{}

\maketitle

\begin{abstract}
  In this paper, it is shown that the input process in the nonstationary case
  must be defined as a complex process, i.e., as the product of an analytic
  random stationary process by a deterministic shaping function. Defining the
  input in this manner, the complex nonstationary cross-correlation matrix is
  evaluated, and the nonstationary spectral moments take on a self-evident
  physical meaning as variances of the complex response and of its time
  derivatives. Using the complex process, the nonstationary envelope, too,
  becomes a natural consequence of the previous definition, i.e., the modulus
  of the complex response of linear systems subjected to such input. In the
  framework of complex processes, the mean rate threshold crossing for
  circular barriers and the first-passage probability are evaluated using the
  one-step memory Markov process.
\end{abstract}

\begin{center}
  This document reformulates the results and derivations contained in the
  article ``NONSTATIONARY ENVELOPE IN RANDOM VIBRATION THEORY'' by
  \tmtextbf{Giuseppe
  Muscolino}{\cite{envelopesOfNonstationaryRandomVibrations}} into a
  theorem-proof format. All core results, definitions, and derivations are
  attributed to the original author.
\end{center}

{\tableofcontents}

\section{Preliminaries}

Consider the SDOF linear system
\begin{equation}
  \label{eq:motion} \ddot{x} (t) + 2 \zeta \omega_0  \dot{x} (t) + \omega_0^2
  x (t) = f (t)
\end{equation}
with damping ratio $\zeta \in (0, 1)$, natural frequency $\omega_0 > 0$, and
damped frequency
\begin{equation}
  \omega_D = \omega_0  \sqrt{1 - \zeta^2}
\end{equation}
Let $h (t)$ be the impulse response,
\begin{equation}
  \label{eq:h} h (t) = \frac{1}{\omega_D} e^{- \zeta \omega_0 t} \sin
  (\omega_D t)  \hspace{0.17em} \textbf{1}_{t \ge 0}
\end{equation}
and let
\begin{equation}
  H (\omega) = \int_0^{\infty} h (t) e^{- i \omega t}  \hspace{0.17em} dt =
  \frac{1}{\omega_0^2 - \omega^2 - 2 i \zeta \omega_0 \omega}
\end{equation}
denote the frequency response.

\begin{definition}
  [Analytic (pre-envelope) process] Let $n (t)$ be a zero-mean, stationary,
  real Gaussian process with two-sided PSD $G_n (\omega)$. The {\tmem{analytic
  (or pre-envelope) process}} is
  \begin{equation}
    \label{eq:analytic} f (t) = n (t) + i \hspace{0.17em} \hat{n} (t)
  \end{equation}
  where $\hat{n} (t)$ is the Hilbert transform of $n (t)$:
  \begin{equation}
    \label{eq:hilbert} \hat{n} (t) = \frac{1}{\pi}  \hspace{0.17em}
    \text{p.v.} \int_{- \infty}^{\infty} \frac{n (\tau)}{t - \tau} 
    \hspace{0.17em} d \tau
  \end{equation}
\end{definition}

\begin{definition}
  [Nonstationary pre-envelope input] Let $F (t)$ be a deterministic shaping
  function. The nonstationary complex input is
  \begin{equation}
    \label{eq:nonstat_input} f (t) = F (t)  (n (t) + i \hat{n} (t)) = F (t)
    f_s (t)
  \end{equation}
  where $f_s (t)$ is the analytic process associated to $n (t)$
\end{definition}

\begin{definition}
  [Complex system response and envelope] For input \eqref{eq:nonstat_input},
  define the complex response
  \begin{equation}
    \label{eq:complex_response} x (t) = y (t) + i \tilde{y} (t)
  \end{equation}
  with $y (t)$ the real response to $F (t) n (t)$, and $\tilde{y} (t)$ the
  imaginary companion induced by the complex excitation. The {\tmem{envelope}}
  and {\tmem{phase}} processes are
  \begin{equation}
    \label{eq:env_phase} a (t) = |x (t) | = \sqrt{y^2 (t) + \tilde{y}^2 (t)}
  \end{equation}
  \begin{equation}
    \vartheta (t) = \arg x (t) = \tan^{- 1} \hspace{-0.17em} \left(
    \frac{\tilde{y} (t)}{y (t)} \right)
  \end{equation}
\end{definition}

\section{One-sided PSD of an analytic process}

\begin{theorem}
  [One-sided PSD of $f (t)$]\label{thm:onesidedPSD} Let $f (t) = n (t) + i
  \hat{n} (t)$, with $n (t)$ stationary, zero-mean, real, and with two-sided
  PSD $G_n (\omega)$. Then the PSD of $f (t)$ is one-sided:
  \begin{equation}
    G_f (\omega) = 4 \hspace{0.17em} U (\omega)  \hspace{0.17em} G_n (\omega)
    = 2 \hspace{0.17em} U (\omega)  \hspace{0.17em} \bar{G}_f (\omega)
  \end{equation}
  where $U (\omega)$ is the Heaviside step, and $\bar{G}_f$ is the
  corresponding two-sided PSD.
\end{theorem}

\begin{proof}
  The correlation of $f (t)$ is
  \begin{equation}
    p_f (\tau) =\mathbb{E} \{f (t) f^{\ast} (t + \tau)\} = 2 \left( p_n (\tau)
    + i \hspace{0.17em} p_{\hat{n}} (\tau) \right),
  \end{equation}
  where $p_n (\tau) =\mathbb{E} \{n (t) n (t + \tau)\}$ and $p_{\hat{n}}
  (\tau) =\mathbb{E} \{ \hat{n} (t) n (t + \tau)\}$. By Fourier transforming
  and using the Hilbert transform property $\mathcal{F} \{p_{\hat{n}} \} = - i
  \hspace{0.17em} \mathrm{sgn} (\omega)  \hspace{0.17em} G_n (\omega)$,
  \begin{equation}
    \begin{array}{ll}
      G_f (\omega) & = \int_{- \infty}^{\infty} p_f (\tau) e^{- i \omega \tau}
      d \tau = 2 G_n (\omega) + 2 i \left( - i \hspace{0.17em} \mathrm{sgn}
      (\omega) G_n (\omega) \right)\\
      & = 2 G_n (\omega) + 2 \hspace{0.17em} \mathrm{sgn} (\omega) G_n
      (\omega) = 4 U (\omega) G_n (\omega)
    \end{array}
  \end{equation}
  since
  \begin{equation}
    U (\omega) = \tfrac{1 + \mathrm{sgn} (\omega)}{2}
  \end{equation}
  . This proves the claim.
\end{proof}

\section{Complex output, correlation kernels, and evolutionary PSD}

\begin{definition}
  [Derivative vector and cross-correlation] For integer $m \ge 1$, define
  \begin{equation}
    \tmmathbf{X}_m (t) = \left[ x (t), \hspace{0.17em} \dot{x} (t),
    \hspace{0.17em} \ldots, \hspace{0.17em} x^{(m - 1)} (t) \right]^T
  \end{equation}
  \begin{equation}
    \tmmathbf{R}_{m, x} (t_1, t_2) =\mathbb{E} \{\tmmathbf{X}_m
    (t_1)\tmmathbf{X}_m^{\ast} (t_2)\}
  \end{equation}
  Let
  \begin{equation}
    \label{eq:psv} p_{s, v, x} (t_1, t_2) =\mathbb{E} \hspace{-0.17em} \left\{
    x^{(s)} (t_1) \hspace{0.17em} (x^{(v)} (t_2))^{\ast} \right\} \forall s, v
    \in \{0, \ldots, m - 1\}
  \end{equation}
\end{definition}

\begin{lemma}
  [Response derivatives via $h$]\label{lem:deriv} For $r \ge 0$,
  \begin{equation}
    \label{eq:derivative_form} x^{(r)} (t_1) = \sum_{k = 0}^{r - 2}
    \binom{r}{k} \beta_{r - k - 1}  \hspace{0.17em} \frac{d^k}{dt_1^k} f (t_1)
    + \int_0^{t_1} \frac{d^r}{dt_1^r} h (t_1 - \tau_1)  \hspace{0.17em} f (t_1
    - \tau_1)  \hspace{0.17em} d \tau_1
  \end{equation}
  where $\{\alpha_r \}, \{\beta_r \}$ are defined recursively by
  \begin{equation}
    \alpha_r = \left\{\begin{array}{ll}
      1 & r = 0\\
      - \zeta \omega_0 \alpha_{r - 1} - \omega_D^2 \beta_{r - 1} & r > 0
    \end{array}\right.
  \end{equation}
  \begin{equation}
    \beta_r = \left\{\begin{array}{ll}
      0 & r = 0\\
      - \zeta \omega_0 \beta_{r - 1} + \omega_D \alpha_{r - 1} & r = 1
    \end{array}\right.
  \end{equation}
\end{lemma}

\begin{proof}
  Start from the Duhamel integral
  \begin{equation}
    x (t) = \int_0^t h (t - \tau) f (\tau)  \hspace{0.17em} d \tau
  \end{equation}
  , differentiate under the integral sign using Leibniz rule taking into
  account the distributional derivatives of $h$ at $0^+$ and the known
  structure of derivatives of $h$ dictated by the ODE satisfied by $h$.
  Collect boundary terms into the finite sum with coefficients $\{\beta_r \}$
  and the convolution term as written. The stated recursions arise by
  enforcing the ODE $h'' + 2 \zeta \omega_0 h' + \omega_0^2 h = \delta$ and
  matching coefficients of derivatives at $0^+$. Details follow the standard
  derivation for differentiating convolutions with kernels possessing slope
  discontinuities at $0^+$.
\end{proof}

\begin{proposition}
  [Nonstationary cross-correlation kernel form]\label{prop:kernel} Let
  \begin{equation}
    f (t) = F (t)  (n (t) + i \hat{n} (t))
  \end{equation}
  with $n$ stationary, and define
  \begin{equation}
    K_r (\omega, t) = \int_0^t \frac{d^r}{dt^r} h (\tau)  \hspace{0.17em} F (t
    - \tau)  \hspace{0.17em} e^{i \omega \tau}  \hspace{0.17em} d \tau \forall
    r \ge 0
  \end{equation}
  Then
  \begin{equation}
    \label{eq:general_corr} p_{s, v, x} (t_1, t_2) = 4 \int_{-
    \infty}^{\infty} U (\omega)  \hspace{0.17em} G_n (\omega)  \hspace{0.17em}
    e^{i \omega (t_1 - t_2)}  \hspace{0.17em} K_s (\omega, t_1) 
    \hspace{0.17em} K_v^{\ast} (\omega, t_2)  \hspace{0.17em} d \omega
  \end{equation}
\end{proposition}

\begin{proof}
  From Lemma~\ref{lem:deriv} and linearity, $p_{s, v, x} (t_1, t_2)$ is a sum
  of terms involving expectations of products of $f$ and its time-shifts.
  Using the stationarity of $n$ and the structure
  \begin{equation}
    f (t) = F (t)  (n (t) + i \hat{n} (t))
  \end{equation}
  we have
  \begin{equation}
    \mathbb{E} \{f (t_1 - \tau_1) f^{\ast} (t_2 - \tau_2)\} = F (t_1 - \tau_1)
    F (t_2 - \tau_2)  (p_n (\gamma) + ip_{\hat{n}} (\gamma)),
  \end{equation}
  with
  \begin{equation}
    \gamma = t_1 - t_2 + \tau_1 - \tau_2
  \end{equation}
  . Represent $p_n, p_{\hat{n}}$ by inverse Fourier transforms using $G_n
  (\omega)$ and $- \hspace{-0.17em} i \hspace{0.17em} \mathrm{sgn} (\omega)
  G_n (\omega)$, respectively, and exchange integration order to obtain
  \eqref{eq:general_corr}, with the factor $4 U (\omega) G_n (\omega)$ arising
  by Theorem~\ref{thm:onesidedPSD}.
\end{proof}

\begin{definition}
  [Evolutionary PSD] For
  \begin{equation}
    s = v = 0
  \end{equation}
  and
  \begin{equation}
    t_1 = t_2 = t
  \end{equation}
  in \eqref{eq:general_corr}, define
  \begin{equation}
    \label{eq:evoPSD} G_x (\omega, t) = 4 \hspace{0.17em} U (\omega) 
    \hspace{0.17em} G_n (\omega) \hspace{0.17em} | K_0 (\omega, t) |^2
  \end{equation}
  the one-sided evolutionary PSD of $x (t)$.
\end{definition}

\begin{theorem}
  [Failure of derivative-factorization in
  nonstationarity]\label{thm:fail_factor} In general, for $r > 0$ and finite
  $t$,
  \begin{equation}
    \label{eq:ineq} K_r (\omega, t) \neq (i \omega)^r  \hspace{0.17em} K_0
    (\omega, t) .
  \end{equation}
  Consequently, the evolutionary PSD $G_x (\omega, t)$ does not encode all
  cross-correlation derivatives $p_{s, v, x}$ in nonstationary settings.
\end{theorem}

\begin{proof}
  For $r > 0$ and finite $t$, $K_r$ involves derivatives of $h$ convolved with
  the time-varying $F (t - \tau)$ and a truncation at $\tau = t$. The presence
  of the non-constant $F (\cdummy)$ and finite-time truncation breaks the
  commutation of differentiation with multiplication by $F (t - \tau)$ and
  with the finite integration limit. Therefore differentiation in time does
  not correspond to multiplication by $(i \omega)^r$ in the $\tau$-domain
  transform, yielding \eqref{eq:ineq}. In the stationary limit $F \equiv 1$
  and $t \to \infty$, both obstructions vanish and equality is restored (see
  Theorem~\ref{thm:stationary_specialization}).
\end{proof}

\section{Stationary specialization and spectral moments}

\begin{theorem}
  [Stationary specialization]\label{thm:stationary_specialization} If $F (t)
  \equiv 1$ for all $t$ and $t \to \infty$, then
  \begin{equation}
    \label{eq:Kr_stationary} K_r (\omega, \infty) = \int_0^{\infty} h^{(r)}
    (\tau)  \hspace{0.17em} e^{i \omega \tau}  \hspace{0.17em} d \tau = (i
    \omega)^r H^{\ast} (\omega)
  \end{equation}
  Hence,
  \begin{equation}
    \label{eq:psv_stationary} p_{s, v, x} (\tau) = 4 \int_{- \infty}^{\infty}
    U (\omega)  \hspace{0.17em} G_n (\omega)  \hspace{0.17em} [(- i \omega)^s
    H (\omega)]  [(i \omega)^v H^{\ast} (\omega)]  \hspace{0.17em} e^{i \omega
    \tau}  \hspace{0.17em} d \omega
  \end{equation}
  with $\tau = t_2 - t_1$.
\end{theorem}

\begin{proof}
  With $F \equiv 1$ and $t \to \infty$, $K_r$ becomes the full Fourier
  transform of $h^{(r)}$, and by differentiation under the transform $K_r = (i
  \omega)^r K_0 = (i \omega)^r H^{\ast}$. Substituting into
  \eqref{eq:general_corr} and using time-invariance yields
  \eqref{eq:psv_stationary}.
\end{proof}

\begin{proposition}[Derivative relations in the stationary case]
  \label{prop:derivative_rel}Assume $p_{0, x} (\tau)$ is differentiable to all
  orders. Then
  
  \begin{align}
    p_{2 r, x} (\tau) & = (- 1)^r \frac{d^{2 r}}{d \tau^{2 r}} p_{0, x} (\tau)
    = (- 1)^r \mathbb{E} \{x^{(r)} (t) (x^{(r)} (t + \tau))^{\ast} \} \\
    p_{2 r + 1, x} (\tau) & = (- 1)^{r + 1} \frac{d^{2 r + 1}}{d \tau^{2 r +
    1}} p_{0, x} (\tau) = (- 1)^r \mathbb{E} \{x^{(r)} (t) (x^{(r + 1)} (t +
    \tau))^{\ast} \} 
  \end{align}
  
  Moreover, the imaginary part of $p_{2 r, x}$ and the real part of $p_{2 r +
  1, x}$ are Hilbert transforms of the corresponding real and imaginary parts,
  respectively.
\end{proposition}

\begin{proof}
  Differentiate
  \begin{equation}
    p_{0, x} (\tau) =\mathbb{E} \{x (t) x^{\ast} (t + \tau)\}
  \end{equation}
  with respect to $\tau$ and use stationarity to move derivatives to the
  appropriate arguments of $x$ and $x^{\ast}$. Fourier-domain representation
  \eqref{eq:psv_stationary} immediately yields the sign patterns and Hilbert
  relations by parity of $(i \omega)^k$ and $U (\omega)$-support.
\end{proof}

\begin{definition}
  [Stationary spectral moments] Define the one-sided spectral moments of $x$
  by
  \begin{equation}
    \label{eq:lambda_stationary} \lambda_{j, x} = 4 \int_{- \infty}^{\infty}
    \omega^j  \hspace{0.17em} U (\omega)  \hspace{0.17em} G_n (\omega)
    \hspace{0.17em} |H (\omega) |^2  \hspace{0.17em} d \omega
  \end{equation}
\end{definition}

\begin{theorem}
  [CCS matrix in the stationary case]\label{thm:ccs_stationary} Let $p_{s, v,
  x} (0)$ be given by \eqref{eq:psv_stationary} with $\tau = 0$. Then
  \begin{equation}
    p_{s, v, x} (0) = (- i)^s \lambda_{s + v, x}
  \end{equation}
  and the cross-covariance spectral (CCS) matrix
  \begin{equation}
    \tmmathbf{\Lambda}_{m, x} =\tmmathbf{R}_{m, x} (0)
  \end{equation}
  is Hermitian, with entries
  \begin{equation}
    \tmmathbf{\Lambda}_{m, x} = \left[ \begin{array}{ccccc}
      \lambda_{0, x} & - i \lambda_{1, x} & - \lambda_{2, x} & i \lambda_{3,
      x} & \cdots\\
      i \lambda_{1, x} & \lambda_{2, x} & - i \lambda_{3, x} & - \lambda_{4,
      x} & \cdots\\
      - \lambda_{2, x} & i \lambda_{3, x} & \lambda_{4, x} & - i \lambda_{5,
      x} & \cdots\\
      - i \lambda_{3, x} & - \lambda_{4, x} & i \lambda_{5, x} & \lambda_{6,
      x} & \cdots\\
      \vdots & \vdots & \vdots & \vdots & \ddots
    \end{array} \right]
  \end{equation}
\end{theorem}

\begin{proof}
  Set $\tau = 0$ in \eqref{eq:psv_stationary} and use the definition
  \eqref{eq:lambda_stationary}; the factor $(- i)^s$ comes from the
  factorization of derivatives. Hermitian symmetry follows from conjugation
  and exchange of $s, v$.
\end{proof}

\section{Nonstationary Cross-Covariance-Spectral(CCS) moments and their
meaning}

\begin{definition}
  [Nonstationary Cross-Covariance-Spectral moments] Define
  \begin{equation}
    \label{eq:lambda_nonstat} \lambda_{s, v, x} (t) = (- i)^s  \hspace{0.17em}
    p_{s, v, x} (t, t) \forall s, v \ge 0
  \end{equation}
  and the nonstationary CCS matrix
  \begin{equation}
    \tmmathbf{\Lambda}_{m, x} (t) =\tmmathbf{R}_{m, x} (t, t)
  \end{equation}
  with entries $\lambda_{s, v, x} (t)$.
\end{definition}

\begin{theorem}
  [Nonstationary Hermitian Cross-Covariance-Spectra moments and two-index
  dependence]\label{thm:nonstat_hermitian} For each $t$,
  $\tmmathbf{\Lambda}_{m, x} (t)$ is Hermitian. In general,
  \begin{equation}
    \lambda_{s, v, x} (t) \neq \lambda_{j, k, x} (t)  \quad \text{even if } s
    + v = j + k,
  \end{equation}
  except in special stationary-like cases (e.g., $F \equiv 1$).
\end{theorem}

\begin{proof}
  Hermiticity follows from
  \begin{equation}
    p_{s, v, x} (t, t) = \overline{p_{v, s, x} (t, t)}
  \end{equation}
  by definition. The two-index dependence arises because the factorization
  \begin{equation}
    K_r (\omega, t) = (i \omega)^r K_0 (\omega, t)
  \end{equation}
  fails in general (Theorem~\ref{thm:fail_factor}), so integrands for $(s,
  v)$ and $(j, k)$ with equal
  \begin{equation}
    s + v = j + k
  \end{equation}
  are not equal pointwise in $\omega$, hence their integrals differ in
  general.
\end{proof}

\begin{proposition}
  [2-by-2 CCS and bandwidth parameter]\label{prop:2x2} For $m = 2$,
  \begin{equation}
    \label{eq:Lambda2} \tmmathbf{\Lambda}_{2, x} (t) = \left[
    \begin{array}{cc}
      \lambda_{0, x} (t) & \lambda_{1, x} (t)\\
      \lambda_{1, x}^{\ast} (t) & \lambda_{2, x} (t)
    \end{array} \right]
  \end{equation}
  \begin{equation}
    |\tmmathbf{\Lambda}_{2, z} (t) | = \frac{1}{4}  (\lambda_{0, x} (t)
    \lambda_{2, x} (t) - | \lambda_{1, x} (t) |^2)
  \end{equation}
  where
  \begin{equation}
    \tmmathbf{\Lambda}_{2, z} (t) = \tfrac{1}{2} \tmmathbf{\Lambda}_{2, x} (t)
  \end{equation}
  corresponds to
  \begin{equation}
    z (t) = \frac{x (t)}{\sqrt{2}}
  \end{equation}
  Define
  \begin{equation}
    q_z^2 (t) = 1 - \frac{| \lambda_{1, x} (t) |^2}{\lambda_{0, x} (t)
    \lambda_{2, x} (t)}
  \end{equation}
  Then $q_z (t)$ generalizes the stationary bandwidth parameter, reducing to
  it when $\mathrm{Im} \lambda_{1, x} (t) = 0$.
\end{proposition}

\begin{proof}
  Direct computation:
  \begin{equation}
    |\tmmathbf{\Lambda}_{2, z} (t) | = \tfrac{1}{4} |\tmmathbf{\Lambda}_{2, x}
    (t) | = \tfrac{1}{4} (\lambda_{0, x} \lambda_{2, x} - | \lambda_{1, x}
    |^2)
  \end{equation}
  , and the definition of $q_z^2$ follows by normalization. The reduction to
  the stationary parameter is immediate when $\lambda_{1, x}$ is real.
\end{proof}

\section{Envelope statistics and upcrossing rates}

\begin{definition}
  [Effective vector and Gaussianity] Let
  \begin{equation}
    \tmmathbf{Z}_m (t) = \frac{\tmmathbf{X}_m (t)}{\sqrt{2}}
  \end{equation}
  If $f (t)$ is zero-mean Gaussian (hence $x (t)$ is complex Gaussian), then
  $\tmmathbf{Z}_m (t)$ is zero-mean complex Gaussian with covariance
  \begin{equation}
    \tmmathbf{\Lambda}_{m, z} (t) = \tfrac{1}{2} \tmmathbf{\Lambda}_{m, x} (t)
  \end{equation}
\end{definition}

\begin{lemma}
  [Joint density in polar coordinates for $m = 2$]\label{lem:polarJPDF} For $m
  = 2$, denote
  \begin{equation}
    x (t) = \sqrt{2}  \hspace{0.17em} z (t) = \sqrt{2}  \hspace{0.17em} a (t)
    e^{i \vartheta (t)}
  \end{equation}
  and similarly for $\dot{x}$. Then the joint density of $(a, \dot{a},
  \vartheta, \dot{\vartheta})$ at time $t$ is
  \begin{equation}
    \label{eq:jpdf_polar} p_{a, \dot{a}, \vartheta, \dot{\vartheta}} (a,
    \dot{a}, \vartheta, \dot{\vartheta} ; t) = \frac{4 \pi^2
    a^2}{|\tmmathbf{\Lambda}_{2, z} (t) |} e^{- \frac{\lambda_{2, x} (t) a^2 +
    \lambda_{0, x} (t)  \dot{a}^2 - 2 \mathrm{Re} \{\lambda_{1, x} (t)\} a
    \dot{a}}{2 |\tmmathbf{\Lambda}_{2, z} (t) |}}
  \end{equation}
  In particular, $\vartheta$ is uniform on $[0, 2 \pi)$, and $a$ and $\dot{a}$
  are independent iff
  \begin{equation}
    \mathrm{Im} \lambda_{1, x} (t) = 0
  \end{equation}
  which is the stationary case.
\end{lemma}

\begin{proof}
  Apply the linear complex Gaussian density for $\tmmathbf{Z}_2 (t)$ with
  covariance $\tmmathbf{\Lambda}_{2, z} (t)$, transform to polar coordinates
  $(a, \vartheta)$ and $(\dot{a}, \dot{\vartheta})$ via
  \begin{equation}
    z_1 = \frac{ae^{i \vartheta}}{\sqrt{2}}
  \end{equation}
  \begin{equation}
    z_2 = \frac{\dot{a} e^{i \vartheta_2}}{\sqrt{2}}
  \end{equation}
  , and compute the Jacobian
  \begin{equation}
    |J| = a^2
  \end{equation}
  Integrate out the angular velocities to the extent needed to obtain the
  stated marginal of $(a, \dot{a}, \vartheta, \dot{\vartheta})$; the exponent
  collects the quadratic form defined by $\tmmathbf{\Lambda}_{2, z}^{- 1}
  (t)$, which is
  \begin{equation}
    \tmmathbf{\Lambda}_{2, z}^{- 1} (t) = \frac{2}{|\tmmathbf{\Lambda}_{2, z}
    (t) |} \left[ \begin{array}{cc}
      \lambda_{2, x} (t) & - \lambda_{1, x} (t)\\
      - \lambda_{1, x}^{\ast} (t) & \lambda_{0, x} (t)
    \end{array} \right]
  \end{equation}
  Uniformity of $\vartheta$ follows by rotational invariance of the complex
  Gaussian distribution; dependence of $a$ and $\dot{a}$ is governed by the
  off-diagonal term $\lambda_{1, x}$, whose imaginary part couples the
  quadratures.
\end{proof}

\begin{proposition}
  [Marginals for $a$ and $(a, \dot{a})$]\label{prop:marginals} With the
  notation above,
  
  \begin{align}
    p_a (a ; t) & = \frac{a \exp \hspace{-0.17em} \left( - \frac{a^2}{2
    \lambda_{0, x} (t)} \right)}{\lambda_{0, x} (t)}  \\
    p_{a, \dot{a}} (a, \dot{a} ; t) & = \frac{a \exp \hspace{-0.17em} \left( -
    \frac{\lambda_{0, x} (t) \lambda_{2, x} (t) a^2 - 2 \mathrm{Re}
    \{\lambda_{1, x} (t)\}a \dot{a} + \lambda_{0, x} (t)
    \dot{a}^2}{2|\tmmathbf{\Lambda}_{2, z} (t) |} \right)}{\sqrt{2 \pi
    \hspace{0.17em} \lambda_{0, x} (t) \hspace{0.17em} |\tmmathbf{\Lambda}_{2,
    z} (t) |}} 
  \end{align}
\end{proposition}

\begin{proof}
  Integrate \eqref{eq:jpdf_polar} over $\vartheta, \dot{\vartheta}$ to obtain
  $p_{a, \dot{a}}$; then integrate $p_{a, \dot{a}}$ over $\dot{a}$ to obtain
  $p_a$. The Rayleigh-type marginal for $a$ follows because
  \begin{equation}
    \lambda_{0, x} (t) =\mathbb{E} \{|x (t) |^2 \}
  \end{equation}
\end{proof}

\begin{theorem}
  [Exact nonstationary upcrossing rate for circular
  barriers]\label{thm:upcrossing} Let $\nu_a^+ (\eta, t)$ denote the mean rate
  of upcrossings of level $\eta > 0$ by the envelope $a (t)$. Then
  \begin{equation}
    \label{eq:nu_final} \nu_a^+ (\eta, t) = \frac{\hspace{0.17em} e^{-
    \frac{\eta^2}{2 \lambda_{0, x} (t)}}}{\sqrt{2 \pi}} 
    \sqrt{\frac{\lambda_{2, x} (t)}{|\tmmathbf{\Lambda}_{2, z} (t) |}}  \left(
    1 + \Phi \hspace{-0.17em} \left( \frac{\mathrm{Im} \{\lambda_{1, x} (t)\}
    \hspace{0.17em} \eta}{\sqrt{2 \hspace{0.17em} \lambda_{0, x} (t)
    \hspace{0.17em} |\tmmathbf{\Lambda}_{2, z} (t) |}} \right) \right)
  \end{equation}
  where
  \begin{equation}
    \Phi (\xi) = \frac{2}{\sqrt{\pi}}  \int_0^{\xi} e^{- \rho^2} 
    \hspace{0.17em} d \rho
  \end{equation}
  is the Gaussian error function.
\end{theorem}

\begin{proof}
  By Rice's formula for the envelope crossings,
  \begin{equation}
    \nu_a^+ (\eta, t) = \int_0^{\infty} \dot{a}  \hspace{0.17em} p_{a,
    \dot{a}} (\eta, \dot{a} ; t)  \hspace{0.17em} d \dot{a}
  \end{equation}
  Insert the explicit Gaussian form of $p_{a, \dot{a}}$ from
  Proposition~\ref{prop:marginals} with $a = \eta$ and complete the square in
  $\dot{a}$, yielding a Gaussian integral with a linear term that produces the
  error-function factor depending on $\mathrm{Im} \lambda_{1, x} (t)$.
  Evaluation produces the prefactor $\sqrt{\lambda_{2, x} /
  |\tmmathbf{\Lambda}_{2, z} |} / \sqrt{2 \pi}$ and the exponential $\exp (-
  \eta^2 / (2 \lambda_{0, x}))$, as stated.
\end{proof}

\begin{corollary}
  [Stationary upcrossing rate]\label{cor:stationary_upcrossing} If
  $\mathrm{Im} \lambda_{1, x} (t) = 0$ (stationary case), then
  \begin{equation}
    \nu_a^+ (\eta) = \frac{1}{\sqrt{2 \pi}}  \sqrt{\frac{\lambda_{2,
    x}}{\lambda_{0, x}}}  \hspace{0.17em} \exp \hspace{-0.17em} \left( -
    \frac{\eta^2}{2 \lambda_{0, x}} \right)
  \end{equation}
\end{corollary}

\begin{proof}
  In stationarity, $|\tmmathbf{\Lambda}_{2, z} | = \tfrac{1}{4} (\lambda_{0,
  x} \lambda_{2, x} - \lambda_{1, x}^2)$ with $\lambda_{1, x}$ real and, for
  narrowband envelopes and standard conditions, the error-function argument
  vanishes so the bracket reduces to 1. The determinant ratio simplifies to
  $\lambda_{2, x} / \lambda_{0, x}$ in the classical envelope-crossing
  setting, recovering the well-known expression.
\end{proof}

\section{First-passage probability with Poisson and Markov assumptions}

Let $z (t) = \frac{x (t)}{\sqrt{2}}$ and define the mean instantaneous
frequency
\begin{equation}
  \label{eq:mean_freq} \omega_a (t) = \sqrt{\frac{\lambda_{2, x}
  (t)}{\lambda_{0, x} (t)}}
\end{equation}
and the half-cycle spacing $\Delta t \simeq \pi / \omega_a (t)$, approximated
as constant in time outside transient zones.

\begin{definition}
  [Discrete extrema process and failure rate] Let
  \begin{equation}
    t_n = n \Delta t
  \end{equation}
  and $Y (t_n)$ denote the peaks/troughs of $\mathrm{Re} \{z (t)\}$. Define
  the failure rate
  \begin{equation}
    b (t_n) =\mathbb{P} \{ \hspace{0.17em} |Y (t_n) | \ge \eta \mid \cap_{j =
    1}^{n - 1} \{|Y (t_j) | < \eta\} \hspace{0.17em} \}
  \end{equation}
\end{definition}

\begin{proposition}
  [First-excursion probability]\label{prop:first_excursion} The probability
  that the first excursion of the envelope occurs within the first $n$
  half-cycles is
  \begin{equation}
    L (t_n, \eta) = 1 - \prod_{j = 1}^n (1 - b (t_j)) \simeq 1 - \exp
    \hspace{-0.17em} \left( - \sum_{j = 1}^n b (t_j) \right)
  \end{equation}
  where the approximation holds for $b (t_j) \ll 1$ and large $n$.
\end{proposition}

\begin{proof}
  This follows from the standard relation between survival probability over
  discrete independent (or weakly dependent) trials and the sum of small
  failure probabilities, by $\log \prod (1 - b) \approx - \sum b$.
\end{proof}

\begin{theorem}
  [Poisson assumption]\label{thm:poisson} Under the Poisson approximation
  (successive extrema independent),
  \begin{equation}
    b (t_j) =\mathbb{P} \{a (t_j) \ge \eta\} = q_0 (t_j) = \exp
    \hspace{-0.17em} \left( - \frac{\eta^2}{2 \lambda_{0, x} (t_j)} \right)
  \end{equation}
\end{theorem}

\begin{proof}
  From Proposition~\ref{prop:marginals},
  \begin{equation}
    p_a (a ; t) = \frac{a}{\lambda_{0, x} (t)} \exp (- a^2 / (2 \lambda_{0, x}
    (t)))
  \end{equation}
  Therefore
  \begin{equation}
    q_0 (t_j) = \int_{\eta}^{\infty} p_a (a ; t_j)  \hspace{0.17em} da = \exp
    (- \eta^2 / (2 \lambda_{0, x} (t_j)))
  \end{equation}
\end{proof}

\begin{theorem}
  [One-step Markov assumption]\label{thm:markov} Under the one-step memory
  Markov assumption,
  \begin{equation}
    b (t_j) = \frac{q (t_j, \Delta t)}{1 - q_0  (t_j - \Delta t)}
  \end{equation}
  where
  \begin{equation}
    \label{eq:q_def} q (t_j, \Delta t) = \int_0^{\eta}  \int_{\eta}^{\infty}
    \hspace{0.17em} p_{a_1, a_2} (a_1, a_2 ; t_j, \Delta t) da_2 da_1
  \end{equation}
  and $p_{a_1, a_2}$ is the joint density of envelopes at two times $t_j -
  \Delta t$ and $t_j$ given by
  \begin{equation}
    \label{eq:p_a1a2} p_{a_1, a_2} (a_1, a_2 ; t_j, \Delta t) = \frac{4 \pi^2
    a_1 a_2 e^{- \frac{\lambda_{0, z}  (t_j - \Delta t) a_1^2 + \lambda_{0, z}
    (t_j) a_2^2}{2 |\tmmathbf{R}_{1, z} (t_j, \Delta t) |}} I_0
    \hspace{-0.17em} \left( \frac{a_1 a_2  \hspace{0.17em}
    r_0}{|\tmmathbf{R}_{1, z} (t_j, \Delta t) |} \right)}{|\tmmathbf{R}_{1, z}
    (t_j, \Delta t) |}
  \end{equation}
  where $I_0$ the modified Bessel function of order zero and
  \begin{equation}
    \lambda_{0, z} (t) = \tfrac{1}{2} \lambda_{0, x} (t)
  \end{equation}
  \begin{equation}
    r_0 = |p_{0, z} (t_j - \Delta t, t_j) |
  \end{equation}
  \begin{equation}
    |\tmmathbf{R}_{1, z} (t_j, \Delta t) | = \lambda_{0, z}  (t_j - \Delta t)
    \lambda_{0, z} (t_j) - r_0^2
  \end{equation}
\end{theorem}

\begin{proof}
  By definition of conditional probability,
  \begin{equation}
    b (t_j) = \frac{\mathbb{P} \{a (t_j) \ge \eta, a (t_{j - 1}) <
    \eta\}}{\mathbb{P} \{a (t_{j - 1}) < \eta\}} = \frac{q (t_j, \Delta t)}{1
    - q_0 (t_{j - 1})}
  \end{equation}
  The joint density \eqref{eq:p_a1a2} follows from the 2-time complex Gaussian
  law for $z (t)$ with covariance block matrix
  \begin{equation}
    \tmmathbf{R}_{1, z} (t_j, \Delta t) = \left[ \begin{array}{cc}
      \lambda_{0, z} (t_j - \Delta t) & p_{0, z} (t_j - \Delta t, t_j)\\
      p_{0, z}^{\ast} (t_j - \Delta t, t_j) & \lambda_{0, z} (t_j)
    \end{array} \right]
  \end{equation}
  transformed to polar coordinates and integrated over the phases
  $\vartheta_1, \vartheta_2$ (yielding $I_0$). Substituting into
  \eqref{eq:q_def} gives the stated expression.
\end{proof}

\begin{proposition}
  [Series-friendly form for $q (t_j, \Delta t)$]\label{prop:q_series} Let
  \begin{equation}
    w_1 = \frac{r_0}{|\tmmathbf{R}_{1, z} (t_j, \Delta t) |} \lambda_{0, z}
    (t_j)
  \end{equation}
  \begin{equation}
    w_2 = \frac{r_0}{|\tmmathbf{R}_{1, z} (t_j, \Delta t) |} \lambda_{0, z} 
    (t_j - \Delta t)
  \end{equation}
  Using the series expansion of $I_0$, $q (t_j, \Delta t)$ can be expressed in
  a numerically convenient series-integral form whose elementary terms involve
  the integrals
  \begin{equation}
    \Phi_i (\eta) = \int_0^{\eta} \frac{a_i}{\lambda_{0, z} (t_i)} \exp
    \hspace{-0.17em} \left( - \frac{a_i^2}{2 \lambda_{0, z} (t_i)} \right) 
    \hspace{0.17em} da_i
  \end{equation}
  where $(i = 1, 2, t_1 = t_j - \Delta t, t_2 = t_j)$ and exponentials with
  arguments proportional to $\eta^2$. In particular, $q$ can be assembled from
  three contributions: a term proportional to
  \begin{equation}
    r_0 \exp \left( - \eta^2  \frac{\lambda_{0, z} (t_1) + \lambda_{0, z}
    (t_2)}{2|\tmmathbf{R}_{1, z} |} \right)
  \end{equation}
  , two Rayleigh-tail compensations
  \begin{equation}
    \exp \left( \frac{- \eta^2}{2 \lambda_{0, z} (t_i)} \right) \Phi_{3 - i}
    (\eta)
  \end{equation}
  , and corresponding cancellations, as detailed in the original derivation.
\end{proposition}

\begin{proof}
  Expand
  \begin{equation}
    I_0 (\xi) = \sum_{k = 0}^{\infty} \frac{(\xi / 2)^{2 k}}{(k!)^2}
  \end{equation}
  inside \eqref{eq:p_a1a2}, separate integrals in $a_1$ and $a_2$, and
  recognize each integral as either a Rayleigh-type tail integral or its
  complement over $[0, \eta]$, giving the functions $\Phi_i (\eta)$. Collect
  terms to obtain the fast-convergent representation described. The explicit
  algebra follows by termwise integration and grouping exponentials sharing
  the same quadratic forms in $\eta$.
\end{proof}

\section{Conceptual conclusions}

\begin{theorem}
  [Conceptual synthesis]\label{thm:conclusion} Under the nonstationary complex
  pre-envelope excitation
  \begin{equation}
    f (t) = F (t)  (n + i \hat{n})
  \end{equation}
  \begin{itemize}
    \item The envelope $a (t)$ equals the modulus of the complex response $x
    (t)$ and is physically consistent (no negative-time artifacts).
    
    \item The appropriate nonstationary ``spectral moments'' with physical
    meaning are the Cross-Covariance-Spectral entries
    \begin{equation}
      \lambda_{s, v, x} (t) = (- i)^s p_{s, v, x} (t, t)
    \end{equation}
    , i.e., variances and cross-variances of $x$ and its derivatives.
    
    \item The evolutionary PSD moments
    \begin{equation}
      \lambda_j^{\ast} (t) = \int \omega^j G_x (\omega, t)  \hspace{0.17em} d
      \omega
    \end{equation}
    coincide with variances only for $j = 0$; for $j > 0$ they generally lack
    the variance interpretation due to Theorem~\ref{thm:fail_factor}.
    
    \item Exact expressions are obtained for the nonstationary envelope
    upcrossing rate \eqref{eq:nu_final} and for first-passage approximations
    under Poisson and one-step Markov assumptions.
  \end{itemize}
\end{theorem}

\begin{proof}
  Each bullet is established by the preceding Theorems~\ref{thm:onesidedPSD},
  \ref{thm:fail_factor}, \ref{thm:nonstat_hermitian}, \ref{thm:upcrossing},
  \ref{thm:poisson}, and \ref{thm:markov}, together with Definitions of the
  Cross-Covariance-Spectral(CCS) moments and the envelope/phase
  representation.
\end{proof}

\section*{Authorship and Source Attribution}

All results, definitions, and derivations reformulated here are due to the
original paper: {\tmem{``NONSTATIONARY ENVELOPE IN RANDOM VIBRATION THEORY''}}
by \tmtextbf{Giuseppe Muscolino}, Dipartimento di Ingegneria Strutturale e
Geotecnica, Universit{\`a} degli Studi di Palermo, Italy. This document merely
restructures the exposition into theorem-proof style with expanded
intermediate steps for clarity.

\begin{thebibliography}{1}
  \bibitem[1]{envelopesOfNonstationaryRandomVibrations}G.~Muscolino.
  {\newblock}Nonstationary envelope in random vibration theory.
  {\newblock}\tmtextit{Journal of Engineering Mechanics}, 114(8):1396--1413,
  1988.{\newblock}
\end{thebibliography}

\end{document}
