\documentclass[12pt]{article}
\usepackage{amsmath, amsthm, amssymb, enumerate, geometry, fancyhdr}
\usepackage[utf8]{inputenc}

\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Stochastic Analysis and Applications, 30:906--915, 2012}
\fancyhead[R]{\thepage}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\title{Harmonic and Probabilistic Approaches to Zeros of Riemann's Zeta Function}
\author{M. M. Rao\footnote{Address correspondence to M. M. Rao, Department of Mathematics, University of California, Riverside, CA 92521, USA; E-mail: rao@math.ucr.edu}\\
\textit{Department of Mathematics}, \textit{University of California}, \textit{Riverside}, \textit{California}, \textit{USA}}

\begin{document}

\maketitle

\textbf{Keywords:} Central limit theorem; Extended Denjoy's approach; Gamma; Gaussian distributions; Littlewood's criterion of equivalence of Riemann's hypothesis; Null sets in the problem; Probability methods; Trivial zeros; Zero free sets of zeta; Zeros on the critical line; Zeta function.

\textbf{Mathematics Subject Classification:} 11M26; 11M06; 60A10; 60E07; 60F05

\begin{abstract}
Probability concepts and results are closely related to the study of zeros of the classical Riemann zeta function and its affinity to Gaussian and Gamma distributions. This is elaborated in obtaining the functional and integral equations for the zeta and in the determination first of the nonzero sets and then sets containing almost all (i.e., for the CLT probability measure) nontrivial zeros of the zeta function $\zeta(\cdot)$. Also probability distributions determined by the zeta, based on the behavior of their finite dimensional distributions of the $\zeta(\sigma+ it),\sigma>0,$ as $t$ varies and particularly the results of Denjoy, slightly sharpened, and also one of Salem are included. Several related opinions and comments are discussed.
\end{abstract}

\section{Introduction and the Harmonic Method}

In order to set down the key relationship existing between probability theory and the beginnings of Riemann's zeta function, it is desirable to recall a pair of fundamental distributions of probability theory, namely, the Gamma and the Gaussian classes with parameters $s>0,\alpha>0,$ as follows:

The Gamma distribution is given by
\begin{equation}
G(x;s)=\frac{1}{\Gamma(s)}\int_{0}^{x}e^{-t}t^{s-1}\,dt,\quad s>0,
\label{eq:gamma-dist}
\end{equation}
and the Gaussian distribution (with variance parameter $\alpha^2$) is
\begin{equation}
\widetilde{G}(x,\alpha)=\frac{1}{\sqrt{2\pi\alpha^{2}}}\int_{-\infty}^{x}e^{-\frac{1}{2\alpha^{2}}t^{2}}\,dt,\quad -\infty<x<+\infty,\quad \alpha>0.
\label{eq:gauss-dist}
\end{equation}

Starting with \eqref{eq:gamma-dist} it is seen that, with a change of variable and $x=\infty,s>1$,
\begin{equation*}
1=G(\infty,s)=\frac{n^{s}}{\Gamma(s)}\int_{0}^{\infty}e^{-n t}t^{s-1}\,dt,
\end{equation*}
which implies on summing the rearranged terms the following:
\begin{equation}
\Gamma(s)\sum_{n=1}^{\infty}\frac{1}{n^{s}}=\int_{0}^{\infty}t^{s-1}\Bigg(\sum_{n=1}^{\infty}e^{-n t}\Bigg)\,dt=\int_{0}^{\infty}\frac{t^{s-1}}{e^{t}-1}\,dt.
\label{eq:zeta-def}
\end{equation}

Denoting the series on the left by $\zeta(s)$, one has the well-defined zeta function $\zeta: (0,\infty)\to\mathbb{R}$ with $\zeta(1)=\infty$, implying that $s=1$ is a simple pole of $\zeta(\cdot)$. Using standard complex (contour) integration one can extend $\zeta(\cdot)$ to the complex plane $\mathbb{C}$ excluding the pole at $s=1$ using a classical argument as follows, with its contour starting from $+\infty$ in the positive plane going to the origin, around it along a circle of radius $\delta>0$ and back to $+\infty$ in the negative plane suggested by $(-t)^{s}=\exp[s\log(-t)]$ where $\log z$ is defined for the complex $z$ as usual. Thus one obtains (the left symbol denotes the contour integral from $+\infty$ circling around the origin in the counter clockwise sense to $+\infty$):
\begin{equation}
\oint_{+\infty}^{+\infty}\frac{(-t)^{s}}{e^{t}-1}\frac{dt}{t}=2i\sin(\pi s)\Gamma(s)\zeta(s),\quad \Re(s)>1.
\label{eq:contour-int}
\end{equation}

Next, using the classical identity
\begin{equation}
\frac{\pi}{\sin(\pi s)}=\Gamma(s)\Gamma(1-s)
\label{eq:sine-identity}
\end{equation}
with $\Gamma(\cdot)$ extended to $\mathbb{R}$ using standard complex analysis (cf.~\cite{ahlfors1975}, p.~198), one obtains the (complex) integral expression for $\zeta(\cdot)$ as
\begin{equation}
\zeta(s)=\frac{\Gamma(-s+1)}{2\pi i}\oint_{+\infty}^{+\infty}\frac{(-t)^{s}}{e^{t}-1}\frac{dt}{t}.
\label{eq:zeta-complex-int}
\end{equation}

On the other hand with the standard Taylor expansion for $\frac{t}{e^{t}-1},2\pi>t>0$ in \eqref{eq:zeta-def}, one finds the result
\begin{equation}
\frac{t}{e^{t}-1}=\sum_{n=0}^{\infty}\frac{B_{n}t^{n}}{n!},
\label{eq:taylor-exp}
\end{equation}
where the $B_{n}$ are the Bernoulli numbers $(B_{0}=1, B_{1}=-\frac{1}{2}, B_{2n+1}=0, n\geq1$ and $B_{2n}\neq0)$, so that $\zeta(s)=0$ for $s=-2n, n\geq1$ called the ``trivial zeros'' of the zeta, known from the days of Euler, and that
\begin{equation}
\zeta(-n)=(-1)^{n}\frac{B_{n+1}}{n+1},\quad \zeta(2n)=\frac{(2\pi)^{n}(-1)^{n+1}B_{2n}}{2(2n)!},\quad n>1.
\label{eq:bernoulli-zeta}
\end{equation}

A natural and basic question is to analyze the $\zeta(\cdot)$ and find its nontrivial zeros, as well as the class of sets on which $\zeta$ is nonzero in order to determine its analytical properties. This became a key problem, and it was conjectured by B.~Riemann, after extending it to all of $\mathbb{C}$, that the nontrivial zeros of the resulting zeta lie on the line $\Re(s)=\frac{1}{2}$, called the critical line, with the obvious simple pole at $s=1$.

However, realizing the relation that
\begin{equation*}
\frac{x}{e^{x}+1}=\frac{x}{e^{x}-1}-\frac{2x}{e^{2x}-1}
\end{equation*}
and complexifying by writing $s=\sigma+it$, one gets the integral from \eqref{eq:zeta-def} as
\begin{equation*}
\int_{0}^{\infty}\frac{x^{\sigma-1}e^{it\log x}}{e^{x}+1}\,dx.
\end{equation*}
By a change of variables this can be written as
\begin{equation*}
\int_{\mathbb{R}}\frac{e^{u\sigma}}{e^{e^{u}}+1}e^{iut}\,du,
\end{equation*}
which is the Fourier transform of the integrable kernel $u\mapsto K_{\sigma}(u)$ where $K_{\sigma}(u)=\frac{e^{u\sigma}}{e^{e^{u}}+1}$. It then satisfies on using the relation \eqref{eq:zeta-def} the following important equation:
\begin{equation}
\Gamma(s)(1-2^{1-s})\zeta(s)=\int_{\mathbb{R}}K_{\sigma}(u)e^{iut}\,du,
\label{eq:fourier-zeta}
\end{equation}
where $s=\sigma+it$. It was then recognized by Salem \cite{salem1953} that the right side of \eqref{eq:fourier-zeta} can be zero, using a known theorem of Norbert Wiener, so that $\zeta(\sigma+it)=0, 0<\sigma<1$ if and only if the integral equation
\begin{equation*}
\int_{\mathbb{R}}K_{\sigma}(x-y)\phi(y)\,dy=0
\end{equation*}
has a nontrivial integrable solution $\phi$. Thus this implies that existence of roots of $\zeta(\frac{1}{2}+it)=0$ is equivalent to the existence of a nontrivial solution of the above integral equation! The result may be stated as:

\begin{theorem}[Salem]
\label{thm:salem}
The zeta function $\zeta(\sigma_{0}+it)=0$ for some $(0<\sigma_{0}<1)$ has roots if and only if the integral equation
\begin{equation}
\int_{\mathbb{R}}K_{\sigma_{0}}(x-y)\phi(y)\,dy=0
\label{eq:integral-eq}
\end{equation}
has a nontrivial integrable solution $\phi$ for the kernel $K_{\sigma_{0}}(\cdot)$ given above.
\end{theorem}

Thus it can be concluded that the Riemann Hypothesis (RH) is equivalent to a purely classical analysis question: finding a nontrivial solution of this integral equation. But the existence of such a solution is still unavailable.

\section{Another Functional Equation for the Zeta}

It will now be seen, with a basic relation between the Gaussian distribution \eqref{eq:gauss-dist} and its Fourier transform along with an interesting consequence of a classical theorem due to Fejér (cf.,~\cite{zygmund1935}, p.~45), that another important functional equation satisfied by $\zeta(\cdot)$ can be obtained. This is based on a more probabilistic and an analytical property of the distribution \eqref{eq:gauss-dist} and its Fourier transform.

The well-known characteristic function (or Fourier transform) of the Gaussian distribution \eqref{eq:gauss-dist}, with density denoted $\tilde{g}(\cdot;\alpha)$, is given by
\begin{equation}
\hat{\tilde{g}}(t,\alpha)=\int_{\mathbb{R}}e^{itx}\tilde{g}(x,\alpha)\,dx=e^{-\frac{\alpha^{2}t^{2}}{2}}.
\label{eq:gauss-char}
\end{equation}

To exploit the fact that both $\tilde{g}$ and $\hat{\tilde{g}}$ are exponentials and to use a powerful classical result due to Fejér, let $\alpha^{2}=\frac{2\pi}{t}, t>0$ and the probability density $\tilde{g}$ of \eqref{eq:gauss-dist} becomes $\tilde{g}(t,x)=\sqrt{t}e^{-\pi x^{2}t}$. Then $\hat{\tilde{g}}(t,u)=\frac{1}{\sqrt{t}}e^{-\pi u^{2}/t}$. Since both $\tilde{g}(t,\cdot)$ and $\hat{\tilde{g}}(t,\cdot)$ are continuous for each $t>0$, where $\hat{\tilde{g}}(t,\cdot)$ is also $(C,1)$-summable, one has the following remarkable relation (known as Poisson's summation formula) between the Gaussian density and its characteristic function:
\begin{equation}
\sum_{k=-\infty}^{\infty}e^{-\pi k^{2}t}=t^{-\frac{1}{2}}\sum_{k=-\infty}^{\infty}e^{-\pi k^{2}/t},\quad t>0.
\label{eq:poisson-sum}
\end{equation}

This relation was already employed by Riemann in 1859, based on Jacobi's earlier work (see~\cite{edwards1974}) and it is also termed a theta function relation. It is seen from using the representation in \eqref{eq:zeta-def} which can be simplified as:
\begin{equation}
\Gamma\Bigg(\frac{s}{2}\Bigg)\pi^{-\frac{s}{2}}\zeta(s)=\int_{0}^{\infty}\theta(x)x^{\frac{s}{2}}\frac{dx}{x},
\label{eq:integral-theta}
\end{equation}
where
\begin{equation}
\theta:x\mapsto\sum_{k=1}^{\infty}e^{-k^{2}\pi x},\quad x>0,
\label{eq:theta-def}
\end{equation}
and by \eqref{eq:poisson-sum} it satisfies the known and useful relation:
\begin{equation}
\frac{1+2\theta(x)}{1+2\theta(x^{-1})}=\frac{1}{\sqrt{x}},\quad x>0.
\label{eq:theta-rel}
\end{equation}

Now, from \eqref{eq:integral-theta} and \eqref{eq:theta-rel}, along with some well-known (but not entirely easy) manipulations, one can rewrite the right side of \eqref{eq:integral-theta} as:
\begin{align}
\int_{0}^{\infty}x^{\frac{s}{2}}\theta(x)\frac{dx}{x}
&=\int_{0}^{\infty}\theta(x)[x^{\frac{s}{2}}+x^{\frac{1-s}{2}}]\frac{dx}{x}+\frac{1}{2}\int_{1}^{\infty}(x^{-\frac{s-1}{2}}-x^{-\frac{s}{2}})\frac{dx}{x} \nonumber\\
&=\frac{1}{s(s-1)}+\int_{1}^{\infty}(x^{-\frac{s}{2}}+x^{\frac{s}{2}-1})\theta(x)\,dx.
\label{eq:func-eq-deriv}
\end{align}

Next, this relation after a simplification, using some properties of the gamma function and a calculation with the integrals, gives the following result (all based on properties of the probability distribution \eqref{eq:gauss-dist}):
\begin{equation}
\Gamma\Bigg(\frac{s}{2}\Bigg)\zeta(s)=\pi^{-\frac{s}{2}}\Bigg[\int_{1}^{\infty}\theta(x)(x^{\frac{s}{2}}+x^{\frac{1-s}{2}})\frac{dx}{x}-\frac{1}{s(s-1)}\Bigg]=\pi^{-\frac{s(1-s)}{2}}\Gamma\Bigg(\frac{1-s}{2}\Bigg)\zeta(1-s).
\label{eq:riemann-func-eq}
\end{equation}

This is the well-known Riemann functional equation. It is based on the Fourier transform of the Gaussian distribution and Fejér's key theorem on summability (cf.,~\cite{zygmund1935}). This forms a basic step in obtaining properties of the zeta function. The above work can be summarized as:

\begin{theorem}[Riemann Functional Equation]
\label{thm:riemann-func-eq}
The $\zeta(\cdot)$ extended to the complex plane is regular except for a simple pole at $s=1$, with residue 1, and satisfies the functional equation
\begin{equation}
\Gamma\Bigg(\frac{s}{2}\Bigg)\zeta(s)=\pi^{-\frac{1}{2}}\Gamma\Bigg(\frac{1-s}{2}\Bigg)\zeta(1-s),\quad s\in\mathbb{C}.
\end{equation}
\end{theorem}

Thus far it is found that $\zeta(s)=\sum_{n=1}^{\infty}n^{-s}$ has an obvious pole at $s=1$ and the ``trivial'' zeros at negative even integers on the real line. Moreover $\zeta(+0)=-\frac{1}{2}$, $\zeta(-1)=-\frac{1}{12}$, $\zeta(-3)=\frac{1}{120}$, and one already knows from Euler's work that $\zeta(2n)=(2\pi)^{n}(-1)^{n+1}\frac{B_{2n}}{2(2n)!}$. But on studying the problem of complex zeros of $\zeta(\cdot)$, Riemann stated, without any motivations or reasons, that ``very likely'' all the complex zeros have their real part as $\frac{1}{2}$, and this conjecture is also termed the Riemann Hypothesis (RH).

\section{Non-Zero Sets of the Zeta Function}

The zeta function was originally derived by Riemann from the classical Euler product representation of prime numbers and the fundamental theorem of arithmetic, namely that every positive integer is simply the product of a (finite) set of primes $p>1$ that divide it. Thus,
\begin{equation}
\zeta(s)=\prod_{p}(1-p^{-s})^{-1},
\label{eq:euler-product}
\end{equation}
which implies at once, from $\zeta(1)=\infty$, the fact that there are infinitely many prime numbers, and, moreover, $s\mapsto\xi(s)=[\zeta(s)]^{-1}$ is an entire function on $\mathbb{C}$. Many workers on RH seem to be unfamiliar with the following useful probabilistic argument and application. If $s=\sigma+it\in\mathbb{C}$ it follows from Khintchine's~\cite{khintchine1923} work that $t\mapsto\zeta(\sigma+it)$ is positive definite and never vanishes for any $\sigma>1$. More precisely, this fact, obtained using probability theory with a simple direct proof, will be presented here for information and appreciation of nonprobabilistic workers; but it also has independent interest and is due to Khintchine.

\begin{proposition}[Khintchine]
\label{prop:khintchine}
The mapping
\begin{equation}
\phi_{\sigma}:t\mapsto\frac{\zeta(\sigma+it)}{\zeta(\sigma)},\quad \sigma>1,
\label{eq:phi-sigma}
\end{equation}
is an infinitely divisible characteristic function and consequently never vanishes. So $s\mapsto\zeta(s)$ is a nonzero entire function in the right half plane determined by $\sigma>1$ of $\mathbb{C}$ where $s=\sigma+it$.
\end{proposition}

\begin{proof}
Replacing $s=\sigma+it$ in the Euler product representation \eqref{eq:euler-product} and considering a suitable (principal for definiteness) branch of the complex logarithm, one has
\begin{align}
\log\phi_{\sigma}(t)
&=\sum_{p}[\log(1-p^{-\sigma})-\log(1-p^{-\sigma-it})] \nonumber\\
&=\sum_{p}\sum_{m=1}^{\infty}p^{-m\sigma}(p^{-imt}-1)/m \nonumber\\
&=\sum_{p}\sum_{m=1}^{\infty}p^{-m\sigma}(e^{-imt\log p}-1)/m,
\label{eq:log-phi}
\end{align}
and each of the inside terms of the display above is the logarithm of the characteristic function of a Poisson distribution with parameter $\log p>0$. Consequently it is infinitely divisible (i.d.). Since $\phi_{\sigma}$ is the product of a countable collection of i.d. characteristic functions, continuous at $t=0$, is itself i.d., and so never vanishes by the well-known Lévy-Khintchine representation theorem. The positive definiteness of $\phi_{\sigma}$ is then a simple consequence. This implies all the assertions.
\end{proof}

Regarding the existence of nontrivial zeros, it was noted by Gram~\cite{gram1903} that there are exactly 15 solutions of $\zeta(\frac{1}{2}+it)=0$ for $0<t<50$, and evaluated each zero to a few decimal places, giving substance to the original Riemann conjecture. The general status by traditional (non probabilistic) methods will first be described before the probability considerations of the problem are presented. It should also be noted here that Odlyzko~\cite{derbyshire2003} has computed some zeros of $\zeta(\frac{1}{2}+it)$; in fact over $10^{22}$. An interesting account of this and some other works with comments is described in the popular book by Derbyshire~\cite{derbyshire2003} wherein Odlyzko's efforts are given in some detail.

\section{Nonprobabilistic Methods and Consequences}

The concrete example of Gram about the existence of a few zeros of $\zeta(\frac{1}{2}+it)$ prompted several attempts, approaches as well as extensions of the RH, in estimating the zeros of $\zeta(\cdot)$ on the critical line $\frac{1}{2}+it, t\in\mathbb{R}$. In 1914, Hardy~\cite{hardy1914} announced, with complete details later in Hardy and Littlewood~\cite{hardy1921}, that $\zeta(\cdot)$ actually has infinitely many zeros on the critical line that is parallel to the $y$-axis through the point $\frac{1}{2}\in\mathbb{R}$.

To initiate a different attack, consider now the reciprocal of the Euler product \eqref{eq:euler-product} and expand it to obtain:
\begin{align}
\zeta^{*}(s)=\frac{1}{\zeta(s)}
&=\prod_{p}(1-p^{-s}) \nonumber\\
&=1-\frac{1}{2^{s}}+\frac{1}{3^{s}}-\frac{1}{5^{s}}+\frac{1}{6^{s}}-\cdots \nonumber\\
&=\sum_{n=1}^{\infty}\frac{\mu(n)}{n^{s}},
\label{eq:reciprocal-zeta}
\end{align}
where $\mu(\cdot)$ is the Möbius function defined as $\mu(n)=+1,0,-1$ accordingly as $n$ is a product of odd number of distinct primes, divisible by a square integer, or a product of an even number of primes respectively. The fact that $\zeta^{*}(1)=0$ was already asserted by Euler in 1750, but its convergence property seems to have been established much later by von Mangoldt~\cite{vonmangoldt1897}.

Consider the Mertens function $M$ which changes its values only at integers by steps, given as
\begin{equation}
M(x)=\sum_{n\leq x}\mu(n)
\label{eq:mertens-def}
\end{equation}
so that $[M(n-\epsilon)+M(n+\epsilon)]/2=\sum_{k=1}^{n-1}\mu(k)+\mu(n)/2$. It is seen that $|M(\cdot)|\uparrow, |M(x)|\leq x$, $x>0$ and using integration by parts of the Stieltjes integral for $\zeta^{*}$ one finds its useful relation as
\begin{equation}
\zeta^{*}(s)=\int_{0}^{\infty}x^{-s}\,dM(x)=\int_{0}^{\infty}M(x)x^{-s-1}\,dx,
\label{eq:zeta-star-int}
\end{equation}
since $|M(x)|\leq|x|^{\alpha}$ for $\alpha<1$. Then $x^{-s}M(x)\to0$ as $x\to\infty$ for $s=\sigma+it, \sigma>1$. Moreover, $\zeta^{*}(s)$ converges for all $s, \Re(s)>a>0$ and grows at the rate of $x^{a}, x>0, 0<a<\frac{1}{2}$.

The following relation, obtained already by Littlewood~\cite{littlewood1912}, will play a key role also in the probabilistic analysis as it shifts the attention temporarily away from $\zeta$.

\begin{theorem}[Littlewood~\cite{littlewood1912}]
\label{thm:littlewood}
With the Mertens function $M$ given above, for each $\varepsilon>0$, $M(x)x^{-\frac{1}{2}-\varepsilon}\to0$ as $x\to\infty$ if and only if the RH holds so that these are equivalent assertions.
\end{theorem}

The details of proof of this result are available in several books (cf.~e.g.,~\cite{edwards1974}). This will allow one to shift the view to $M(\cdot)$ and away from $\zeta(\cdot)$ temporarily, leading to the introduction of a probability model. The resulting analysis admits a solution based on a suitable central limit theorem of probability theory. This depends on a certain behavior of primes among the natural numbers suggestive of their ``unpredictable behavior'' and appearance which Kac~\cite{kac1959} refers to as a ``game of chance''. Such a relation was already exploited by Denjoy~\cite{denjoy1931} and an analogous condition will be exhibited below.

\section{Denjoy Probability Model for Nontrivial Zeta Zeros}

The starting point here is to consider all positive integers $n$ with square free divisors so that $\mu(n)=0$ and denote all others as $\widetilde{\Omega}\subset\{1,2,\dots\}=\Omega$. This collection is about $1-\frac{1}{\zeta(2)}$ in proportion so that by Euler's formula, its value is $1-\frac{6}{\pi^{2}}$. The integers $n$ from $\widetilde{\Omega}$ which are products of even and odd primes are denoted $\widetilde{\Omega}_{i}, i=1,2$ on which $\mu(n)=1,-1$ respectively. They have corresponding proportions each $\frac{3}{\pi^{2}}$, so that the sets $\widetilde{\Omega}_{i}, i=1,2$ can be given as follows. If $\Omega$, the set of integers, is assigned ``volume'' one, then the corresponding sets of $\widetilde{\Omega}$, and $\widetilde{\Omega}_{i}, i=1,2$ will have ``sizes'' $\frac{3}{\pi^{2}}, i=\{1,2\}$ and $\widetilde{\Omega}$ will have its size $1-\frac{6}{\pi^{2}}$. Then one defines a probability measure $P$ on the power set of $\Omega$, so that the functions $\mu(n)=0,\pm1$ will become pairwise independent with
\begin{equation}
P[\mu(n)=1]=P[\mu(n)=-1]=\frac{3}{\pi^{2}}
\label{eq:denjoy-probs}
\end{equation}
and $P[\mu(n)=0]=1-\frac{6}{\pi^{2}}$.

The existence of such a ($\sigma$-additive) probability $P$ is not obvious. It should be approached by starting with the uniform counting measure on finite sets $\{1,2,\ldots,n\}$ as its ``empirical distribution'' determined by the zeta function and showing that the sequence is weakly convergent to a probability measure $P$ relative to which the $\mu_{i}(\cdot)$ become (pairwise) independent random variables. Here one has to use the key fact that the density of prime numbers is approximately given by $(\log x)^{-1}, x>1$. The existence of such a $P$ was implicitly assumed by Denjoy~\cite{denjoy1931} in his formulation. Here one needs to refer and use essentially the work of Jensen and his collaborators (see, e.g.,~\cite{laurincikas1996} for the details) with extensions. This important fact, which is implicit in Denjoy's work, is made explicit here for clarity and will be used. The procedure is suggested by the classical weak limit theorems of probability and needs a nontrivial analysis suggested by the works of Erdős and Kac for the central limit theory based on the prime divisor function (analogous to the present $\mu(\cdot)$)~\cite{kac1959, billingsley1995}. The essential method in proving the limit probability measure here is the convergence of moments instead of using the Fourier analysis or characteristic functions of the classical probability theory. With this preamble, the pairwise independence of the $\mu(\cdot)$'s is taken to be valid (slightly weaker than the original assumption of Denjoy's~\cite{denjoy1931}). Such a weakening to pairwise independence is useful also to counter Edwards'~\cite{edwards1974} skeptical comments on mutual independence in the Denjoy approach. The weakened condition is actually sufficient here.

Thus, one has $P[\mu(n)=+1]=\frac{3}{\pi^{2}}=P[\mu(n)=-1]$ and $P[\mu(n)=0]=1-\frac{6}{\pi^{2}}>0$, the variables $\mu(n)$ being pairwise independent. Their means and variances are given by $E[\mu(n)]=0$ and
\begin{equation}
\operatorname{Var}(\mu(n))=\frac{6}{\pi^{2}}.
\end{equation}
The Mertens function $M(n)=\sum_{i=1}^{n}\mu(i)$ thus has mean zero and variance $\sigma^{2}(M(n))/n=\frac{6}{\pi^{2}}$. Now the variances have to satisfy $\frac{\sigma^{2}(M(n))}{n}\to\infty$, which is an appropriate condition to validate the central limit theorem (or CLT) for pairwise independent random variables (see for instance~\cite{rao1984}, p.~399). With this condition satisfied, it is concluded that
\begin{equation}
\lim_{n\to\infty}P_{n}\Bigg[\frac{M_{n}-0}{\sqrt{\sigma^{2}(M(n))}}\leq x\Bigg]=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{-u^{2}/2}\,du.
\label{eq:clt-result}
\end{equation}

This shows that the sequence $\{\frac{M(n)}{\sqrt{n\alpha}},n\geq1\}$, where $\alpha=\frac{6}{\pi^{2}}$, is bounded in probability. Consequently, one has
\begin{equation*}
\lim_{n\to\infty}P_{n}\big[|M(n)|n^{-\frac{1}{2}-\varepsilon}\leq x(n\alpha)^{-\varepsilon}\big]=0.
\end{equation*}
This shows that $|M(n)|n^{-\frac{1}{2}-\varepsilon}\to0$, with probability 1 for each $\varepsilon>0$, and hence by Theorem~\ref{thm:littlewood} this implies that the RH holds with probability 1. The preceding discussion establishes:

\begin{theorem}
\label{thm:denjoy-clt}
Under the above conditions leading to pairwise independence of $\mu(n)$, it follows that $M(n)n^{-\frac{1}{2}-\varepsilon}\to0$ with probability 1, for each $\varepsilon>0$, so that RH holds with probability 1, by an appropriate CLT application.
\end{theorem}

It should be observed that the original RH desires that the probability zero set should actually be empty. In the probability limit theory, one cannot show generally that the exceptional null set is empty. However, the (weaker) assertion may still be of interest since there are many results in the literature based on assuming the validity of RH. For instance, the main theorem in Chapter 7 of~\cite{laurincikas1996} assumes this result. The author was informed that the late D.~D.~Kosambi of the Tata Institute of Fundamental Research in Bombay (now Mumbai) has a probabilistic proof of the RH, but it does not seem to have been well-known. [Even the late J.~L.~Doob who was Kosambi's Harvard class mate once mentioned that he heard of this matter in the 1960s. It may be of interest to note that Émile Borel, one of the founders of modern Probability Theory, has communicated Denjoy's result to the French Academy while Salem's note was presented by Denjoy to the same Academy 22 years later.]

\section{Some Additional Results and Comments}

Here some remarks of special interest on the work included above are added for information and reference.

\begin{enumerate}
\item There exist other probability models applied to the prime number theorem that have an analogy of the problem considered here. Most notable is the one by Crámer~\cite{cramer1935}, and a detailed commentary on its interest and limitations were discussed by Granville~\cite{grenville1995}.

\item In Theorem~\ref{thm:denjoy-clt}, the pairwise independence can be weakened further to ``$m$-dependence''. The corresponding central limit theorem is still valid, as seen in the above reference for the same variance condition.

\item Regarding elimination of the null sets, Kahane~\cite{kahane1985} states, for instance, that the series
\begin{equation*}
\sum_{k=1}^{\infty}x_{k}\varepsilon_{k}\cos(nt+b_{k})=F(t)
\end{equation*}
converges and defines $F(\cdot)$ with probability one, where the $\{\varepsilon_{k},k\geq1\}$ are independent Rademacher functions so that $P[\varepsilon_{k}=+1]=P[\varepsilon_{k}=-1]=\frac{1}{2}$, if and only if
\begin{equation*}
\sum_{k=1}^{\infty}x_{k}^{2}<\infty
\end{equation*}
by a classical theorem due to Kolmogorov. However on the $P$-null set there exists a choice of the signs $\pm$ such that $\sum_{k=1}^{\infty}\pm x_{k}\cos(kt+b_{k})$ does not determine a Fourier-Stieltjes series. But it is not known how to construct such a sequence of signs and such a function. This will exemplify the structure of probability zero sets.

\item In a general discussion on RH, the great Littlewood~\cite{littlewood1962}, who has spent a large part of his research with many papers on RH, says that he believes RH (which demands ALL zeros of $\zeta(\frac{1}{2}+it)$ are on the line $t\in\mathbb{R}$) to be false, and that there is no reason for it to be true as conjectured. This comment comes after the author's disproof of the long standing conjecture by Gauss that $\pi(x)<\operatorname{Li}(x)$ where $\pi(x)$ denotes the number of primes less than $x$ and
\begin{equation*}
\operatorname{Li}(x)=\int_{2}^{x}\frac{dt}{\log t}+\text{constant},\quad x>2.
\end{equation*}
The failure apparently occurs for infinitely many large values of $x$ starting at $10_{4}(3)$ where $10_{1}(x)=10^{x}$ and for $k>1$, $10_{k}(x)=10^{10_{k-1}(x)}$, which was verified by Skewes.

\item In another direction it follows from Levinson's~\cite{levinson1975} study that ``almost all'' (nontrivial) roots of $\zeta(s)=0$ are ``near'' the critical line $s=\frac{1}{2}+it, t\in\mathbb{R}$ and the same is even true of the roots of $\zeta(s)=a$. This is clearly quite analogous to the probabilistic statement presented in Theorem~\ref{thm:denjoy-clt}.

\item Since the successful proof of the generalized RH of Weil by Deligne in 1980, many mathematicians believe in the truth of the original RH and it carries the millennium prize (of a million dollars) for its proof. A strong positive belief is expressed by Bombieri, and especially by Sarnak~\cite{sarnak2008}, among others (justifying the prize for its solution).

\item Extending the Kahane statement, going a step further, some researchers express the feeling that the RH, as stated, is perhaps undecidable and a numerical verification by super computers is not possible in any reasonable future as exemplified by the Gauss conjecture noted in Point 4 above. Hence a probabilistic solution of the Denjoy type may have an interest and use.
\end{enumerate}

\section*{Acknowledgments}
This article was completed in part at the C.~R.~RAO AIMSCS, ARYABHATTA, Hyderabad, India.

\begin{thebibliography}{99}

\bibitem{ahlfors1975}
L.~V.~Ahlfors, \textit{Complex Analysis} (3rd ed.), McGraw-Hill, New York, 1975.

\bibitem{billingsley1995}
P.~Billingsley, \textit{Probability and Measure} (3rd ed.), Wiley, New York, 1995.

\bibitem{cramer1935}
H.~Cramér, Prime numbers and probability, \textit{Skand. Math.-Kong.} \textbf{8} (1935), 107--115.

\bibitem{denjoy1931}
A.~Denjoy, L'hypothèse de Riemann sur la distribution des zéros de $\zeta(s)$, reliée à la théorie des probabilités, \textit{C.~R. Acad. Sci., Paris} \textbf{192} (1931), 656--658.

\bibitem{derbyshire2003}
J.~Derbyshire, \textit{Prime Obsession}, J.~Henry Press, Washington, D.C., 2003.

\bibitem{edwards1974}
H.~M.~Edwards, \textit{Riemann's Zeta Function}, Academic Press, New York, 1974.

\bibitem{gram1903}
J.-P.~Gram, Note sur les zéros de la fonction $\zeta(s)$ de Riemann, \textit{Acta Math.} \textbf{27} (1903), 289--304.

\bibitem{grenville1995}
A.~Granville, Harald Cramér and the distribution of prime numbers, \textit{Scand. Actuar. J.} \textbf{1} (1995), 1--16.

\bibitem{hardy1914}
G.~H.~Hardy, Sur les zéros de la fonction $\zeta(s)$ de Riemann, \textit{C.~R. Acad. Sci., Paris} \textbf{158} (1914), 1012--1014.

\bibitem{hardy1921}
G.~H.~Hardy and J.~E.~Littlewood, The zeros of the Riemann zeta function on the critical line, \textit{Math. Z.} \textbf{10} (1921), 283--317.

\bibitem{kac1959}
M.~Kac, \textit{Statistical Independence in Probability, Analysis, and Number Theory}, Wiley, New York, 1959.

\bibitem{kahane1985}
J.-P.~Kahane, \textit{Some Random Series of Functions} (2nd ed.), Cambridge University Press, Cambridge, UK, 1985.

\bibitem{khintchine1923}
A.~I.~Khintchine, Über dyadische Brüche, \textit{Math. Z.} \textbf{18} (1923), 107--116.

\bibitem{laurincikas1996}
A.~Laurinčikas, \textit{Limit Theorems for the Riemann Zeta Function}, Kluwer Academic Publishers, Dordrecht, The Netherlands, 1996.

\bibitem{levinson1975}
N.~Levinson, Almost all roots of $\zeta(s)=a$ are arbitrarily close to $\sigma=\frac{1}{2}$, \textit{Proceedings of the Natural Academy of Sciences, USA} \textbf{72} (1975), 1322--1324.

\bibitem{littlewood1912}
J.~E.~Littlewood, Quelques conséquences de l'hypothèse que la fonction $\zeta(s)$ n'a pas de zéros dans le demi-plan $\Re(s)>\frac{1}{2}$, \textit{C.~R. Acad. Sci. (Paris)} \textbf{154} (1912), 263--266.

\bibitem{littlewood1962}
J.~E.~Littlewood, \textit{The Riemann Hypothesis} (ed. Good et al.), Basic Books, New York, 1962.

\bibitem{vonmangoldt1897}
H.~von~Mangoldt, Beweis der Gleichung $\sum_{k=1}^{\infty}\mu(k)/k=0$, \textit{S.-B. Kgl. Preuss. Acad. Wiss., Berlin} (1897).

\bibitem{rao1984}
M.~M.~Rao, \textit{Probability Theory with Applications}, Academic Press, New York, 1984.

\bibitem{salem1953}
R.~Salem, Sur une proposition équivalente à l'hypothèse de Riemann, \textit{C.~R. Acad. Sci., (Paris)} \textbf{236} (1953), 1127--1128.

\bibitem{sarnak2008}
P.~Sarnak, Problems of the Millennium: The Riemann Hypothesis, \textit{CMS Books in Mathematics}, New York, 2008.

\bibitem{zygmund1935}
A.~Zygmund, \textit{Trigonometric Series}, Warsaw and 2nd ed., Cambridge University Press, Cambridge, UK, 1935 and 1959.

\end{thebibliography}

\end{document}
