\documentclass[10pt,twocolumn]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{cite}
\usepackage{enumitem}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

% Journal formatting
\title{Complex Processes for Envelopes of Normal Noise}
\author{RICHARD ARENS}
\date{September 1957}

\begin{document}

\maketitle

\begin{abstract}
The paper presents a brief exposition of the technique of complex normal random variables as utilized in the study of the envelopes of Gaussian noise processes. The central concept is the pre-envelope z(t) of a real normal process. The pre-envelope z(t) of a real function x(t) is a complex function whose real part is x(t) and whose absolute value is the envelope, in the sense of high-frequency theory, of x(t). The joint probability density for z(t), z'(t) is found and used to get the threshold crossing rate. Consideration of nonstationary processes is included.
\end{abstract}

\section{Introduction}
THE envelope r(t) of a real function x of one variable (``time'') is obtained, according to one conception of the envelope, by forming the conjugate (Hilbert transform) function y of x and taking the absolute value $|z(t)|$ of $z(t) = x(t) + iy(t)$. We call the function z the pre-envelope of x. Its spectrum is supported wholly by one half the frequency axis; we choose the positive half. When two ``signals'' $x_1$, $x_2$ are superposed, the pre-envelope of $x_1 + x_2$ is the sum of the pre-envelopes $z_1$, $z_2$ of $x_1$, $x_2$, respectively; and more generally if x is filtered with output at time t

\begin{equation}
\label{eq:filtered_output}
\int_0^{\infty} W(t - s)x(s) ds
\end{equation}

then the pre-envelope of the output is at time t

\begin{equation}
\label{eq:pre_envelope_output}
\int_0^{\infty} W(t - s)z(s) ds
\end{equation}

where z is the pre-envelope of x. In dealing with linear systems it is evidently better to utilize the pre-envelope, because then at each stage between successive filterings the envelope can be readily inspected. (This procedure is often employed for aesthetic reasons independent of envelopes.) We shall, however, be concerned with random processes rather than a single time series in this paper.

Accordingly, let x be a sample function from some real normal (so-called ``Gaussian'') stationary process. Form $z = x + iy$ where z is the pre-envelope of x. This gives rise to a new random process which is stationary, normal and, of course, complex. We first prove this and then illustrate the utility of such complex processes by calculating the ``alarm rate'' (see Section~\ref{sec:alarm_rate}) or threshold-crossing rate for a ``noise'' envelope. Our object is, however, more to exhibit the technique and illustrate the principles of complex valued random processes. Our experience has been that such technique is practically indispensable for treating envelope questions for nonstationary processes, and considerably more convenient for arriving at the known results than the known treatment of the stationary case.

\section{The Pre-Envelope Process Examined}
\label{sec:pre_envelope_process}

Bunimovich\cite{bunimovich} has pointed out that if we have a real valued function,

\begin{equation}
\label{eq:real_valued_function}
x(t) = \int_{-\infty}^{\infty} \exp (2\pi ift) dX(f),
\end{equation}

where X is of bounded variation and $dX(-f) = dX(f)$ since x is real, then the absolute value of

\begin{equation}
\label{eq:complex_form}
x(t) + iy(t) = x(t) \int_{0}^{+\infty} \exp (2\pi ift) dX(f) + \Delta X \int_{-0}^{+\infty}
\end{equation}

is the Rice\cite{rice} envelope of \eqref{eq:real_valued_function}. It is clear that z is linearly determined by x. Hence z is also a normal, stationary process, although, as pointed out earlier, a complex one. Here we confine ourselves to the calculation of the autocorrelation for the z process, which is by definition (E = expected value or expectation),

\begin{equation}
\label{eq:autocorrelation}
E[z(t)z(s)] = R_z(t - s),
\end{equation}

a justified notation because it depends only on $t - s$. We aim to express $R_z(t)$ in terms of the integrated power spectrum W for the x process, in terms of which the autocorrelation of the latter is given by

\begin{equation}
\label{eq:autocorrelation_x}
E[x(t)x(s)] = R(t - s) = \int_{0}^{\infty} \cos 2\pi f(t - s) dW(f).
\end{equation}

We expand the cosine, obtaining briefly an array cos cos + sin sin, where the first factor in each case depends on t and the second on s. For the expectation of $x(t)y(s)$ we remark that the expectation of a linear functional is the linear functional of the expectation. The linear functional here intended is the Hilbert transform applied to the second appearing function x, in \eqref{eq:autocorrelation_x}. Thus the expectation of $x(t)y(s)$ is an integral like \eqref{eq:autocorrelation_x} with cos cos + sin sin replaced by cos sin − sin cos, since the Hilbert transform of cos is sin and that of sin is − cos. Operating, instead, on the t-dependent factors gives the expectation of $y(t)x(s)$ with the array sin cos + cos sin, and for $y(t)y(s)$ we obtain the array sin sin + cos cos. Adding the integrals for $x(t)x(s)$, $y(t)y(s)$, $-ix(t)y(s)$, and $iy(t)x(s)$, and dividing by 2 gives the quantity called for in \eqref{eq:autocorrelation}, whence

\begin{equation}
\label{eq:rz_expression}
R_z(t - s) = \int_{0}^{\infty} \exp (2\pi if(t - s)) dW(f).
\end{equation}

\section{Complex Normal Distributions}

Just as in vector analysis, there is a point in introducing complex processes only if in subsequent operations, no further reference to the real components is necessary. In order to gain such facility, some basic ideas have to be kept in mind. The probability density F(z) of a complex random variable z is a real function defined in the complex plane. Also it is nonnegative and has integral one over the plane. If, for example,

\begin{equation}
\label{eq:prob_density}
F(z) = \frac{1}{2\pi\sigma^2} e^{-zz^*/2\sigma^2}
\end{equation}

then we call z a normally distributed random variable with zero mean and semivariance $\sigma^2$. This latter is half the mean (E) of $zz^* = |z|^2$. The probability density for $r = |z|$ can be obtained from \eqref{eq:prob_density} by changing to polar coordinates and integrating over the angle and takes the form

\begin{equation}
\label{eq:rayleigh}
F_1(r) = r\sigma^{-2} \exp (-2^{-1}r^2\sigma^{-2}) \quad (r \geq 0),
\end{equation}

sometimes called ``Rayleigh type with parameter $\sigma^2$''. If w is a complex normally distributed random variable with mean m which may, of course, be complex, then $w = m + z$ where z is distributed by \eqref{eq:prob_density}. The probability density for $r = |w|$ can be obtained via polar coordinates, and is

\begin{equation}
\label{eq:prob_density_r}
F_2(r) = r\sigma^{-2}I_0(r\sigma^{-2}\mu) \exp [-2^{-1}\sigma^{-2}(r^2 + \mu^2)]
\end{equation}

where $r \geq 0$, and $\mu = |m|$. This corresponds to the distribution of the envelope of signal plus noise. $\mu$ is the signal envelope and $\sigma^2$ is the mean square of the noise.

The typical jointly normal probability density for n complex random variables is given by

\begin{equation}
\label{eq:joint_prob}
F(z_1, \ldots, z_n) = C \exp (-2^{-1} \sum_{i,j} q_{ij} z_i z_j^*)
\end{equation}

where $q_{ij} = q_{ji}$ and $C = (2\pi)^{-n} \times$ determinant of $(q_{ij})$.

The inverse $(r_{ij})$ of the matrix $(q_{ij})$ has this property

\begin{equation}
\label{eq:inverse_property}
r_{ij} = E[z_i z_j^*].
\end{equation}

\section{Return to the Pre-Envelope Process}

In a possibly nonstationary, normal process we have a joint probability density like \eqref{eq:joint_prob} for the values $z(t_1) = z_1, \ldots, z(t_n) = z_n$ of a sample function z, and the $r_{ij}$ of \eqref{eq:inverse_property} takes the form

\begin{equation}
\label{eq:r_ij_form}
r(t_i, t_j) = E[z(t_i)z(t_j)^*]
\end{equation}

where r is a function of two variables characterizing the process. If the process is stationary, this depends only on $t_i - t_j$ and may be written $r_{ij} = r(t_i, t_j) = R_z(t_i - t_j)$. In view of \eqref{eq:autocorrelation}, this $R_z$ is given for the pre-envelope process by \eqref{eq:rz_expression}.

If a process characterized by \eqref{eq:r_ij_form} (i.e., by specifying r) is subjected to a linear operation $w = L(z)$, then w is again a normally distributed complex random variable, provided there are no convergence difficulties in the carrying out of the operation L. In carrying out the operation L on the function z, there will usually occur a dummy variable t, as in the examples

\begin{equation}
\label{eq:linear_op1}
L(z) = a \left.\frac{d}{dt} z(t)\right|_{t=t_1}
\end{equation}

\begin{equation}
\label{eq:linear_op2}
L(z) = \int_b^a A(t)z(t) dt.
\end{equation}

These and similar functional operations may be written as

\begin{equation}
\label{eq:functional_op}
L(z) = \int L(t) [z(t)].
\end{equation}

Naturally, the t in \eqref{eq:linear_op1}--\eqref{eq:functional_op} may be replaced by any other letter without altering the significance, hence the name dummy variable. Supposing that the questions alluded to do not impede us, then

\begin{equation}
\label{eq:expected_w_squared}
E[|w|^2] = E\left[\int L(t) [z(t)]\int \overline{L(s)} [z(s)]\right]
\end{equation}
\begin{equation}
= \int\int L(t)\overline{L(s)} [z(t)z(s)^*]]
\end{equation}
\begin{equation}
= \int\int L(t)\overline{L(s)} [r(t, s)].
\end{equation}

\begin{equation}
\frac{\partial}{\partial t} r(t, s)\big|_{s=t=0}
\end{equation}

\begin{equation}
\frac{\partial}{\partial s} r(t, s)\big|_{s=t=0}
\end{equation}

\begin{equation}
\frac{\partial^2}{\partial s \partial t} r(t, s)\big|_{s=t=0};
\end{equation}

i.e., we have the automatically hermitean matrix

Thus the probability density of $w = L(z)$ is given by

\begin{equation}
\label{eq:prob_density_w}
F(w) = (2\pi\tau^2)^{-1} \exp [-2^{-1}\tau^{-2} |w|^2]
\end{equation}

where $\tau^2$ is given by

\begin{equation}
\label{eq:tau_squared}
\tau^2(=E[|w|^2]) = \int\int L(t)\overline{L(s)} [r(t, s)].
\end{equation}

The full utility of the complex method (examples in Section~\ref{sec:alarm_rate}) is not attained until also two or more linear functions of a given process z can be treated. Such a generalization of \eqref{eq:tau_squared} is easily obtained. Let the linear functionals be

\begin{equation}
\label{eq:linear_functionals}
w_1 = L_1[z], \ldots, w_n = L_n[z].
\end{equation}

Then, analogous to, and generalizing \eqref{eq:tau_squared},

\begin{equation}
\label{eq:expected_wi_wj}
E[w_i w_j^*] = \int\int L_i(t) \overline{L_j(s)} [r(t, s)].
\end{equation}

\begin{equation}
\label{eq:matrix}
\begin{pmatrix}
r(0, 0) & r_1(0, 0) \\
r_2(0, 0) & r_{12}(0, 0)
\end{pmatrix}
\end{equation}

If these numbers are taken as the $r_{ij}$ and the $q_{ij}$ calculated in terms of them, then \eqref{eq:joint_prob} gives the joint probability density of the variables \eqref{eq:linear_functionals}. Of course the z's in \eqref{eq:joint_prob} have to be replaced by w's.

\section{Alarm Rate for the Envelope}
\label{sec:alarm_rate}

By this we mean the following. Let x be a stationary normal process specified by an R as in \eqref{eq:autocorrelation_x}. Let P(T) be the probability that the envelope will rise above a level V in a certain interval of time T, i.e.,

\begin{equation}
\label{eq:probability_rise}
P(T) = \text{prob. }\{|z(0)| < V < |z(T)|\}.
\end{equation}

If for small T we have

\begin{equation}
\label{eq:small_t}
P(T) = pT + q
\end{equation}

where q tends to 0 faster than T, as T tends to 0, then p is the alarm rate associated with the level V. Now \eqref{eq:probability_rise} could be evaluated by setting up the bivariate distribution \eqref{eq:joint_prob} again mentioned in Section V and integrating, but this labor is unnecessary.

Rice\cite{rice} solves this problem by using the joint distribution for $|z(t)|$ and $\frac{d}{dt}|z(t)|$. We shall, however, use the joint distribution of the two linear functionals

\begin{equation}
\label{eq:linear_functionals_specific}
w_1 = L[z] = [z(t)]_{t=0},
\end{equation}
\begin{equation}
w_2 = M(t) [z(t)] = [z'(t)]_{t=0}
\end{equation}

and hope that our performance may encourage the reader to use complex variables for other envelope problems.

We actually do not need stationarity; so let the z process be specified as by \eqref{eq:r_ij_form}. [We recapitulate: if the datum is a real process such as \eqref{eq:autocorrelation_x}, we form first \eqref{eq:rz_expression} and take $r(t, s) = R_z(t - s)$.] With the $r_{ij}$ to use in \eqref{eq:joint_prob} for $w_1$, $w_2(i, j = 1, 2)$ we have, according to \eqref{eq:matrix}, the following array:

\begin{equation}
\label{eq:array_matrix}
\begin{pmatrix}
r(0, 0) & r_1(0, 0) \\
r_2(0, 0) & r_{12}(0, 0)
\end{pmatrix}
\end{equation}

where the indices in \eqref{eq:array_matrix} denote partial derivatives. Let the determinant of \eqref{eq:array_matrix} be called D. Then the inverse is, omitting the $(0, 0)$'s from \eqref{eq:array_matrix},

\begin{equation}
\label{eq:inverse_matrix}
(q_{ij}) = \frac{1}{D} 
\begin{pmatrix}
r_{12} & -r_1 \\
-r_2 & r
\end{pmatrix}
\end{equation}

The joint probability density for $z(0)$, $z'(0)$ is, therefore,

\begin{equation}
\label{eq:joint_prob_density}
F(z, w) = D(2\pi)^{-2} \exp (-2^{-1}Q)
\end{equation}

where $Q = D^{-1}(r_{12}|z|^2 - 2\mathfrak{R} r_1 wz^* + r |w|^2)$, $\mathfrak{R}$ denoting ``real part.''

In the stationary case, \eqref{eq:array_matrix} takes the form

\begin{equation}
\label{eq:stationary_matrix}
\begin{pmatrix}
R_z(0) & -R'_z(0) \\
+R'_z(0) & -R''(0)
\end{pmatrix}.
\end{equation}

Reference to \eqref{eq:rz_expression} discloses that

\begin{equation}
\label{eq:rz_0}
R_z(0) = \int_{0}^{\infty} dW(f) = R(0) = \text{``mean power''}
\end{equation}

\begin{equation}
\label{eq:rz_prime_0}
R'_z(0) = 2\pi i \int_{0}^{\infty} f dW(f) \text{ (pure imaginary)}
\end{equation}

and

\begin{equation}
\label{eq:rz_double_prime_0}
R''(0) = -4\pi^2 \int_{0}^{\infty} f^2 dW(f) < 0.
\end{equation}

The convergence of these integrals would seem to be the only assumption needed to justify the result.

Proceeding from \eqref{eq:joint_prob_density}, a kinematic consideration provides a short cut to the alarm rate. We choose to think of \eqref{eq:joint_prob_density} as giving the position and velocity distribution of a gas in the plane; and we desire the rate at which matter diffuses out of the circle of radius V about 0. Consider the rate of diffusion over an element h of arc of this circle. The rotational invariance of the distribution makes this rate independent of where on the circle h is placed, and so we place it at $z = V$. We may think of it as a small vertical segment whose endpoints are V, V + ih. The particles that will cross with velocity w in one second are those in the parallelogram of area uh, determined by h and the vector $w = u + iv$, provided $u > 0$, and hence have relative mass $F(V, w)uh$. Integrating overall relevant $(u, v)$, we obtain

\begin{equation}
\label{eq:integral_rate}
h \int_{-\infty}^{\infty} \int_{0}^{\infty} F(V, u + iv)u \, du \, dv.
\end{equation}

This is the rate of diffusion of mass over an arc of length h of the circle. Multiplying by $2\pi V/h$ yields the diffusion rate for the whole circle. Laying aside the kinematic terminology which has now served its heuristic purpose, we say that this [i.e., \eqref{eq:integral_rate} with $h = 2\pi V$] is the probability that a particle will escape from the circle per unit of time; and this is, of course, the alarm rate.

It remains to evaluate \eqref{eq:integral_rate}. We set $z = V$, $w = u + iv$, in \eqref{eq:joint_prob_density}, and the alarm rate is readily evaluated as

\begin{equation}
\label{eq:alarm_rate}
V\tau^{-1} \exp (-2^{-1}V^2\tau^{-2}) \left[\frac{D}{2\pi\tau} \exp (-2^{-1}\tau^{-2}V^2\mathfrak{R}r_1) + V\tau^{-1}\mathfrak{R}r_1 \text{erf} (V\mathfrak{R}r_1(D\tau)^{-1})\right]
\end{equation}

where $\text{erf} (y) = (2\pi)^{-1} \int_{-y}^0 \exp (-2x^2) \, dx$.

In the stationary case, there is considerable simplification because the real part $\mathfrak{R}r_1$ of $r_1 = R'_z(0)$ is 0, and we have nothing but a constant, $(D/2\pi\tau)$, multiplied by the envelope's probability density at V [see \eqref{eq:rayleigh}]. This is then Rice's result.

\section{Mobile Filters}

The generality of \eqref{eq:alarm_rate} in going beyond the stationary case might have been pointless were it not for the fact that ``Gaussian noise'' may be encountered which is not stationary. Of course in that case a pre-envelope process cannot be constructed via \eqref{eq:rz_expression}. A consideration of the genesis of such noise is helpful, however. It usually arises because stationary noise is sent through a mobile (= nonstationary) device. It often happens that a formula of the type

\begin{equation}
\label{eq:formula_type}
\int_{-\infty}^{\infty} A^*(s)x(s) \, ds = z(t)
\end{equation}

can be discovered such that for a real input $x(t)$, $z(t)$ represents the pre-envelope for the output, i.e., $z(t)$ is the real output and $|z(t)|$ is the envelope of the output. Then the function \eqref{eq:r_ij_form} for the pre-envelope of the output is as follows

\begin{equation}
\label{eq:r_output}
r(t, s) = E \left[\int\int x(u)x(v) A^*(u) A^*(v) \, du \, dv\right] = \int\int R(u - v) A^*(v) A^*(u) \, dv \, du.
\end{equation}

If $A^*$ has a Fourier transform

\begin{equation}
\label{eq:fourier_transform}
\Phi(f) = \int A^*(v) \exp (-2\pi ifv) \, dv
\end{equation}

the Fourier inversion formula enables us to write also

\begin{equation}
\label{eq:fourier_inversion}
r(t, s) = \int \Phi^*(f)\Phi^*(f) dW(f).
\end{equation}

\section*{Acknowledgment}
Thanks are due T. L. Gottier and Dr. E. Ackerlind of the Radio Corporation of America, Los Angeles, Calif., for encouragement and to the second named especially for enlightening discussion.

\begin{thebibliography}{9}
\bibitem{rice} S. O. Rice, ``Mathematical analysis of random noise,'' Bell. Sys. Tech. J., vol. 23, p. 282; 1944, and vol. 24, p. 109; 1945.
\bibitem{bunimovich} V. I. Bunimovitch, ``The fluctuation process as a vibration with random amplitude and phase,'' J. Tech. Phys., USSR, vol. 14, p. 1231; November, 1949.
\bibitem{doob} J. L. Doob, ``Stochastic Processes,'' John Wiley \& Sons, Inc. New York, N.Y.; 1953.
\bibitem{khintchine} A. Khintchine, ``Korrelationstheorie der stationären stochastischen Prozesse,'' Math. Ann., vol. 109 p. 604; 1934.
\bibitem{lawson} J. L. Lawson and G. E. Uhlenbeck, ``Threshold Signals,'' M.I.T. Rad. Lab. Ser., McGraw-Hill Book Co., Inc., New York, N.Y., vol. 24, p. 154; 1947.
\bibitem{cramer} V. Cramer, ``On the theory of stationary random processes,'' Ann. Math., vol. 41, p. 215; 1940.
\bibitem{blanc} A. Blanc-Lapierre and R. Fortet, ``Theorie des Fonctions Aleatoires,'' Masson, Paris; 1953.
\end{thebibliography}

\end{document}
