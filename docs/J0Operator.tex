\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}

\title{The Eigenfunctions of $\int_0^{\infty} J_0 (| x - y |) f (x) \mathrm{d} x$
and a Technique For Deriving The Eigenfunctions of Stationary Gaussian Process
Integral Covariance Operators}

\author{Stephen Crowley \\ \texttt{stephencrowley214@gmail.com}}
\date{September 16, 2024}

\begin{document}

\maketitle

\begin{abstract}
The null spaces of Gaussian process kernel inner product operators are
shown to be the Fourier transforms of the polynomials orthogonal with
respect to the spectral densities of the processes and it is furthermore
shown that the orthogonal complements of the null spaces as given by the
Gram-Schmidt recursions enumerate the products $g_k (t) =
\sqrt{\frac{c_p}{c_q}} \frac{\prod_{i = 1}^{n_k} (t - \alpha_{k,
i})}{\prod_{j = 1}^{m_k} (t - \beta_{k, j})} = f_k (t) f_k (s)$ of the
eigenfunctions $f_k$ of the corresponding integral covariance operators.
\end{abstract}

Let $C (x)$ be the covariance function of a stationary Gaussian process on
$[0, \infty)$. Define the integral covariance operator $T$ by:
\begin{equation}
(Tf) (x) = \int_0^{\infty} C (x - y) f (y) \, \mathrm{d} y
\end{equation}
Let $S (\omega)$ be the spectral density related to $C (x)$ by the
Wiener-Khinchin theorem:
\begin{equation}
C (x) = \frac{1}{\pi} \int_{- \infty}^{\infty} e^{i \omega x} S (\omega) 
\, \mathrm{d} \omega
\end{equation}
\begin{equation}
S (\omega) = \int_0^{\infty} C (x) e^{- ix \omega} \, \mathrm{d} x
\end{equation}
Consider polynomials $\{p_n (\omega)\}$ orthogonal with respect to $S
(\omega)$:
\begin{equation}
\int_{- \infty}^{\infty} p_n (\omega) p_m (\omega) S (\omega) 
\, \mathrm{d} \omega = \delta_{nm}
\end{equation}
Define $r_n (x)$ as the inverse Fourier transforms of $p_n (\omega)$:
\begin{equation}
r_n (x) = \int_{- \infty}^{\infty} p_n (\omega) e^{i \omega x} 
\, \mathrm{d} \omega
\end{equation}
\begin{lemma}
The functions $r_n (x)$ form the null space of the kernel inner product:
\begin{equation}
\int_0^{\infty} C (x) r_n (x) \, \mathrm{d} x = 0
\end{equation}
\end{lemma}

\begin{proof}
Let $C (x)$ and $r_n (x)$ be defined as:
\begin{equation}
C (x) = \frac{1}{\pi} \int_{- \infty}^{\infty} e^{iax} S (a) 
\, \mathrm{d} a
\end{equation}
\begin{equation}
r_n (x) = \int_{- \infty}^{\infty} p_n (b) e^{ibx} \, \mathrm{d} b
\end{equation}
where $S (a)$ is the spectral density and $p_n (b)$ are orthogonal
polynomials with respect to $S (a)$. Note that $C (x)$ and $r_n (x)$ are
even functions, as they depend on the difference between two variables.

Substitute the definitions of $C (x)$ and $r_n (x)$, and apply Fubini's
theorem:

\begin{align*}
\int_0^{\infty} C (x) r_n (x) \, \mathrm{d} x &= \int_0^{\infty}
\frac{1}{\pi} \int_{- \infty}^{\infty} e^{iax} S (a) 
\, \mathrm{d} a \int_{- \infty}^{\infty} p_n (b) e^{ibx} 
\, \mathrm{d} b \, \mathrm{d} x \\
&= \frac{1}{\pi} \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} p_n
(b) S (a) \int_0^{\infty} e^{i (a + b) x} \, \mathrm{d} x
\, \mathrm{d} b \, \mathrm{d} a 
\end{align*}

Since $C (x)$ and $r_n (x)$ are even functions, we can write:
\begin{equation}
\int_0^{\infty} C (x) r_n (x) \, \mathrm{d} x = \frac{1}{2} \int_{-\infty}^{\infty} C (x) r_n (x) \, \mathrm{d} x
\end{equation}

Now we have:
\begin{align*}
\frac{1}{2} \int_{-\infty}^{\infty} C (x) r_n (x) \, \mathrm{d} x &= 
\frac{1}{2\pi} \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} p_n (b) S (a) \int_{-\infty}^{\infty} e^{i (a + b) x} \, \mathrm{d} x \, \mathrm{d} b \, \mathrm{d} a \\
&= \frac{1}{2} \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} p_n (b) S (a) \delta(a + b) \, \mathrm{d} b \, \mathrm{d} a \\
&= \frac{1}{2} \int_{- \infty}^{\infty} p_n (-a) S (a) \, \mathrm{d} a
\end{align*}

By the orthogonality of $p_n (a)$ with respect to $S (a)$, we conclude:
\begin{equation}
\frac{1}{2} \int_{- \infty}^{\infty} p_n (-a) S (a) \, \mathrm{d} a = 0
\end{equation}
Thus, $\int_0^{\infty} C (x) r_n (x) \, \mathrm{d} x = 0$, which completes the proof.
\end{proof}

\section{Eigenfunctions from Orthogonalized Null Space}

By orthogonalizing the null space $\{r_n (x)\}$, we obtain the eigenfunctions
$\{\psi_n (x)\}$ of the covariance operator $T$. The orthogonalization process
gives:
\[ r^{\perp}_n (y) = \psi_n (x) = \sum_{k = 0}^n a_{nk} r_k (x) = r_n (y) -
\sum_{m = 0}^{n - 1} \frac{\langle r_n (y), r^{\perp}_m (y)
\rangle}{\langle r^{\perp}_m (y), r^{\perp}_m (y) \rangle} r^{\perp}_m (y)
\]
where the coefficients $a_{nk}$ are given by:
\begin{equation}
a_{nk} = \left\{ \begin{array}{ll}
1 & \text{if } k = n\\
- \sum_{j = k}^{n - 1} a_{nj} \langle r_n, \psi_j \rangle & \text{if } k 
n\\
0 & \text{if } k > n
\end{array} \right.
\end{equation}
\begin{theorem}
Let $\{\psi_n (x)\}$ be the orthogonal complement of $\{r_n (x)\}$. Then
$\psi_n (x)$ are eigenfunctions of $T$, with eigenvalues:
\begin{equation}
\lambda_n = \int_0^{\infty} C (z) \psi_n (z) \, \mathrm{d} z
\end{equation}
\end{theorem}

\begin{proof}
This is not quite right, they have to be factorized as in Theorem
\ref{factorize}. I think the infinite-dimensional version of this is the
Hadamard product factorization?
\end{proof}

\begin{definition}
The spectral density of a stationary process is the Fourier transform of the
covariance kernel due to Wiener-Khinchin theorem.
\end{definition}

\begin{definition}
Let $S_n (x)$ be the orthogonal polynomials whose orthogonality measure is
equal to the spectral density of the process. These polynomials shall be
called the spectral polynomials corresponding to the process.
\end{definition}

\begin{remark}
If the spectral density does not equal the orthogonality measure of a known
set of orthogonal polynomials then such a set can always be generated by
applying the Gram-Schmidt process to the monomials so that they are
transformed into a set that is orthogonal with respect any given spectral
density (of a stationary process).
\end{remark}

\subsection{The Karhunen-Loeve Expansion}

The Karhunen-Loeve expansion is a spectral representation theorem which
expands the random process $w (x, \theta)$ in terms of a denumerable set of
orthogonal random variables in the form
\begin{equation}
w (x, \theta) = \sum_{i = 1}^{\infty} \mu_i (\theta) g_i (x) \label{process}
\end{equation}
where $\{\mu_i (\theta)\}$ is a set of random variable projections and $\{g_i
(x)\}$ are the functions of the integral covariance operator associated to the
Gaussian process having the covariance kernel of $w (x, \theta)$. Since
equation (\ref{process}) is a quantization of the random process it is a
representation of the process $w (x, \theta)$ as a curve in the Hilbert space
spanned by the set $\{g_i (x)\}$ expressed as a direct sum of orthogonal
projections in this Hilbert space whose projections on successive basis
vectors have magnitudes which are proportional to the corresponding
eigenvalues of the covariance function associated with the eigenfunctions of
the process $w (x, \theta)$.

\subsubsection{Derivation}

\begin{theorem}
The random process $w (x, \theta)$ can be represented by the Karhunen-Loeve
expansion defined by the Fourier-like series as
\begin{equation}
w (x, \theta) = \sum_{n = 0}^{\infty} \sqrt{\lambda_n} \xi_n (\theta) f_n
(x)
\end{equation}
where $\{\xi_n (\theta)\}$ is a said to be a set of 'random' variables to be
determined by projecting the process $w (x, \theta)$ onto the $n -
\mathrm{th}$ eigenfunction and $\lambda_n$ is the $n$-th eigenvalue
corresponding to the n-th eigenfunction of the corresponding integral
covariance operator $\{f_n (x)\}$
\end{theorem}

\begin{proof}
Let $w (x, \theta)$ be a random process, function of the position vector $x$
defined over the domain $D$, with $\theta$ belonging to the space of random
events $\Omega$. Let $\bar{w} (x)$ denote the expected value of $w (x,
\theta)$ over all possible realizations of the process, and $C (x_1, x_2)$
denote its covariance function. By definition of the covariance function, it
is bounded, symmetric and positive definite. Thus, it has the eigenfunction
expansion
\begin{equation}
C (x_1, x_2) = \sum_{n = 0}^{\infty} \lambda_n f_n (x_1) f_n (x_2)
\label{2.7}
\end{equation}
where $\lambda_n$ and $f_n (x)$ are the eigenvalue and the eigenfunction of
the covariance kernel. And, specifically, that they are the solution to the
integral equation
\begin{equation}
\int_D C (x_1, x_2) f_n (x_1) \, \mathrm{d}x_1 = \lambda_n f_n (x_2) \label{2.8}
\end{equation}
Due to the symmetry and the positive definiteness of the covariance kernel,
its eigenfunctions are orthogonal and form a complete set. They can be
normalized according to the following criterion
\begin{equation}
\int_D f_n (x) f_m (x) \, \mathrm{d}x = \delta_{nm} \label{2.9}
\end{equation}
where $\delta_{nm}$ is the Kronecker delta. Clearly, $w (x, \theta)$ can be
written as
\begin{equation}
w (x, \theta) = \bar{w} (x) + \alpha (x, \theta) \label{2.10}
\end{equation}
where $\alpha (x, \theta)$ is a process with zero mean and covariance
function $C (x_1, x_2)$. The process $\alpha (x, \theta)$ can be expanded in
terms of the eigenfunctions $f_n (x)$ as
\begin{equation}
\alpha (x, \theta) = \sum_{n = 0}^{\infty} \xi_n (\theta) \sqrt{\lambda_n}
f_n (x) \label{2.11}
\end{equation}
Second order properties of the random variables $\xi_n$ can be determined by
multiplying both sides of equation (\ref{2.11}) by $\alpha (x_2, \theta)$
and taking the expectation on both sides. Specifically, it is found that
\begin{equation}
\begin{array}{ll}
C (x_1, x_2) & = \langle \alpha (x_1, \theta) \alpha (x_2, \theta)
\rangle\\
& = \sum_{n = 0}^{\infty} \sum_{m = 0}^{\infty} \langle \xi_n (\theta)
\xi_m (\theta) \rangle \sqrt{\lambda_n \lambda_m} f_n (x_1) f_m (x_2)
\end{array} \label{2.12}
\end{equation}
Then, multiplying both sides of equation (\ref{2.12}) by $f_k (x_2)$,
integrating over the domain$D$, and making use of the orthogonality of the
eigenfunctions, yields
\begin{equation}
\begin{array}{ll}
\int_D C (x_1, x_2) f_k (x_2) \, \mathrm{d}x_2 & = \lambda_k f_k (x_1)\\
& = \sum_{n = 0}^{\infty} \langle \xi_n (\theta) \xi_k (\theta) \rangle
\sqrt{\lambda_n \lambda_k} f_n (x_1)
\end{array}
\end{equation}
Multiplying once more by $f_l (x_1)$ and integrating over $D$ gives
\begin{equation}
\int_D \int_D f_l (x_1) f_k (x_1) \, \mathrm{d}x_1 = \sum_{n = 0}^{\infty} \langle
\xi_n (\theta) \xi_k (\theta) \rangle \sqrt{\lambda_n \lambda_k}
\delta_{nl}
\end{equation}
Then, using equation (\ref{2.9}) leads to
\begin{equation}
\lambda_k \delta_{kl} = \sqrt{\lambda_k \lambda_l} \langle \xi_k (\theta)
\xi_l (\theta) \rangle \label{2.15}
\end{equation}
Equation (\ref{2.15}) can be rearranged to give
\begin{equation}
\langle \xi_k (\theta) \xi_l (\theta) \rangle = \delta_{kl}
\end{equation}
Thus, the random process $w (x, \theta)$ can be written as
\begin{equation}
w (x, \theta) = \bar{w} (x) + \sum_{n = 0}^{\infty} \xi_n (\theta)
\sqrt{\lambda_n} f_n (x) \label{2.17}
\end{equation}
where
\begin{equation}
\langle \xi_n (\theta) \rangle = 0
\end{equation}
\begin{equation}
\langle \xi_n (\theta) \xi_m (\theta) \rangle = \delta_{nm} \label{2.19}
\end{equation}
and $\lambda_n, f_n (x)$ are solution to equation (\ref{2.8}). Truncating
the series in equation (\ref{2.17}) at the $M^{th}$ term, gives
\begin{equation}
w (x, \theta) = \bar{w} (x) + \sum_{n = 0}^M \xi_n (\theta)
\sqrt{\lambda_n} f_n (x) \label{2.20}
\end{equation}
An explicit expression for $\xi_n (\theta)$ can be obtained by multiplying
equation (\ref{2.11}) by $f_n (x)$ and integrating over the domain $D$. That
is,
\begin{equation}
\xi_n (\theta) = \frac{\int_D \alpha (x, \theta) f_n (x)
\, \mathrm{d}x}{\sqrt{\lambda_n}} \label{2.21}
\end{equation}
\end{proof}

\subsubsection{Uniqueness of the Expansion}

\begin{lemma}
\textbf{Uniqueness}: The random variables appearing in an expansion of
the kind given by equation (\ref{2.10}) are orthonormal if and only if the
orthonormal functions $\{f_n (x)\}$ and the constants $\{\lambda_n \}$ are
respectively the eigenfunctions and the eigenvalues of the covariance kernel
as given by equation (\ref{2.8}).
\end{lemma}

\begin{proof}
The "if" part is an immediate consequence of equation (\ref{2.11}). To show
the "only if" part, equation (\ref{2.12}) can be used with
\begin{equation}
\langle \xi_n (\theta) \xi_m (\theta) \rangle = \delta_{nm}
\end{equation}
to obtain
\begin{equation}
C (x_1, x_2) = \sum_{n = 0}^{\infty} \lambda_n f_n (x_1) f_n (x_2)
\end{equation}
Multiplying both sides by $f_m (x_2)$ and integrating over $D$ gives
\begin{equation}
\int_D C (x_1, x_2) f_m (x_2) \, \mathrm{d}x_2 = \sum_{n = 0}^{\infty} \lambda_n f_n
(x_1) \delta_{nm} = \lambda_m f_m (x_1)
\end{equation}
\end{proof}

\begin{theorem}
Let $Y_n (y)$ be the normalized Fourier transforms of the spectral
polynomials $\begin{array}{ll}
Y_n (y) & = \frac{\hat{S}_n (y)}{| \hat{S}_n |}
\end{array}$where the sequence $\hat{S}_n (y)$ of inverse Fourier transforms
of the spectral polynomials $S_n (x)$ is given by
\begin{equation}
\hat{S}_n (y) = \int_{- 1}^1 S_n (x) e^{i x y} \, \mathrm{d}x
\end{equation}
The eigenfunctions of the integral covariance operator (\ref{T}) are given
by the products
\begin{equation}
\psi_n (x) \psi_n (y) = Y^{\perp}_n (x - y)
\end{equation}
of the elements of orthogonal complement of the normalized Fourier
transforms $Y_n (y)$ of the spectral polynomials (via the Gram-Schmidt
process)
\begin{equation}
\begin{array}{ll}
\psi_n (x) \psi_n (y) & = Y^{\perp}_n (x - y)\\
& = Y_n (x - y) - \sum_{m = 0}^{n - 1} \frac{\langle Y_m (x - y),
Y^{\perp}_m (x - y) \rangle}{\langle Y^{\perp}_m (x - y), Y^{\perp}_m (x
- y) \rangle} Y^{\perp}_m (x - y)
\end{array}
\end{equation}
\end{theorem}

\begin{proof}
\ldots there's some elegant way to do this that I can probably write down
after my back surgery.. it involves Lemma \ref{factorize}
\end{proof}

\begin{lemma}
\label{factorize}For any rational function
\begin{equation}
f (t - s) = \frac{P (t - s)}{Q (t - s)}
\end{equation}
, where $P$ and $Q$ are polynomials, there exist rational functions $g (t)$
and $g (s)$ such that
\begin{equation}
f (t - s) = g (t) g (s)
\end{equation}
\end{lemma}

\begin{proof}
Let
\begin{equation}
P (t - s) = c_p \prod_{i = 1}^n (t - s - \alpha_i)
\end{equation}
and
\begin{equation}
Q (t - s) = c_q \prod_{j = 1}^m (t - s - \beta_j)
\end{equation}
then define
\begin{equation}
g (t) = \sqrt{\frac{c_p}{c_q}} \frac{\prod_{i = 1}^n (t -
\alpha_i)}{\prod_{j = 1}^m (t - \beta_j)}
\end{equation}
such that
\begin{equation}
\begin{array}{ll}
g (t) g (s) & = \frac{c_p}{c_q} \frac{\prod_{i = 1}^n (t -
\alpha_i)}{\prod_{j = 1}^m (t - \beta_j)} \frac{\prod_{i = 1}^n (s -
\alpha_i)}{\prod_{j = 1}^m (s - \beta_j)}\\
& = \frac{c_p}{c_q} \frac{\prod_{i = 1}^n (t - \alpha_i) (s -
\alpha_i)}{\prod_{j = 1}^m (t - \beta_j) (s - \beta_j)}\\
& = \frac{c_p \prod_{i = 1}^n ((t - s) - \alpha_i)}{c_q \prod_{j =
1}^m ((t - s) - \beta_j)}\\
& = f (t - s)
\end{array}
\end{equation}
For complex roots, we pair each $\alpha_i$ or $\beta_j$ with its complex
conjugate in the factorization of $g (t)$. This ensures that the product $(t
- \alpha_i) (t - \overline{\alpha_i})$ results in a quadratic polynomial
with real coefficients, making $g (t)$ a real-valued function.
\end{proof}

\begin{example}
Let the kernel function be given by $K (t, s) = J_0 (t - s)$ then identify
the orthogonal polynomial sequence associated with the spectral density
\begin{equation}
S (\omega) = \int_0^{\infty} J_0 (x) e^{ix \omega} \,
\mathrm{d} x = \left\{ \begin{array}{ll}
\frac{1}{\sqrt{1 - \omega^2}} & \omega \in (- 1, 1)\\
0 & \text{otherwise}
\end{array} \right.
\end{equation}
as being the orthogonality measure of the Type-I Chebyshev polynomials $T_n
(x)$ so that the orthogonal polynomial sequence is identified as
\begin{equation}
S_n (x) = T_n (x)
\end{equation}
so that
\begin{equation}
\int_{- 1}^1 S_n (\omega) S_m (\omega) S (\omega) \,
\mathrm{d} \omega = \left\{ \begin{array}{ll}
0 & n \neq m\\
2 \pi & n = m = 0\\
\pi & n = m \neq 0
\end{array} \right.
\end{equation}
The finite Fourier transforms of the Chebyshev
polynomials are just the usual infinite
Fourier transforms with the integration restricted to the range $- 1 \ldots
1$ since $T_n (x) = 0 \forall x \notin [- 1, 1]$). Equivalently, the spectral
density function can be extended to take the value 0 outside the interval
$[- 1, 1]$.
\begin{equation}
\begin{array}{ll}
\hat{T}_n (y) & = \int_{- \infty}^{\infty} e^{- ix y} T_n (x) \, \mathrm{d}y = 
\int_{- 1}^1 e^{- ix y} T_n (x) \, \mathrm{d}x\\
& = \int_{- \infty}^{\infty} e^{- ix y} _2 F_1 \left( \begin{array}{cc}
n, & - n\\
& \frac{1}{2}
\end{array} | \frac{1}{2} - \frac{x}{2} \right) \, \mathrm{d}x\\
& = \frac{i}{y} (e^{- i y} F^+_n (y) - e^{i (\pi n + y)} F_n^- (y))
\end{array}
\end{equation}
where
\begin{equation}
F_n^{\pm} (y) =_3 F_1 \left( \begin{array}{ccc}
1, & n, & - n\\
&  & \frac{1}{2}
\end{array} | \frac{\pm i y}{2} \right)
\end{equation}
the spectral polynomials $S_n$ are given by the Type-I Chebyshev polynomials
\begin{equation}
S_n (x) = T_n (x)
\end{equation}
and their normalization is
\begin{equation}
\begin{array}{ll}
Y_n (y) & = \frac{\hat{T}_n (y)}{| \hat{T}_n |}\\
& = \frac{i}{y} \left( \frac{e^{- i y} F^+_n (y) - e^{i (\pi n + y)}
F_n^- (y)}{\sqrt{\frac{4 (- 1)^n \pi - (2 n^2 - 1)}{4 n^2 - 1}}} \right)
\end{array}
\end{equation}
where the $L^2$ norm of $\hat{T}_n (y)$ is given by
\begin{equation}
\begin{array}{ll}
| \hat{T}_n | & = \sqrt{\int_{- \infty}^{\infty} \hat{T}_n (y)^2 \, \mathrm{d}y}\\
& = \sqrt{\frac{4 (- 1)^n \pi - (2 n^2 - 1)}{4 n^2 - 1}}
\end{array}
\end{equation}

\end{example}

\begin{definition}
Let $j_n (x)$ is the spherical Bessel function of the first kind,
\begin{equation}
\begin{array}{ll}
j_n (z) & = \sqrt{\frac{\pi}{2 z}} J_{n + \frac{1}{2}} (x)\\
& = \frac{1}{\sqrt{z}} \left( \sin (z) R_{n, \frac{1}{2}} (z) - \cos
(z) R_{n - 1, \frac{3}{2}} (z) \right)
\end{array}
\end{equation}
where $R_{n, v} (z)$ are the (misnamed) Lommel polynomials
\begin{equation}
R_{n, v} (z) = \frac{\Gamma (n + v)}{\Gamma (v)} \left( \frac{2}{z}
\right)^n _2 F_3 \left( \left[ - \frac{n}{2}, \frac{1}{2} - \frac{n}{2}
\right] ; [v, - n, 1 - v - n] ; - z^2 \right)
\end{equation}
where $_2 F_3$ is a generalized hypergeometric function. The ``Lommel
polynomials'' are actually rational functions of z, not polynomial; but
rather ``polynomial in $\frac{1}{z}$''. 
\end{definition}

\begin{theorem}
The eigenfunctions of the stationary integral covariance operator
\begin{equation}
[T \psi_n] (x) = \int_0^{\infty} J_0 (x - y) \psi_n (x) \, \mathrm{d}x =
\lambda_n \psi_n (x) \label{T}
\end{equation}
are given by
\begin{equation}
Y^{\perp}_n (x - y) = \psi_n (x) \psi_n (y) = (- 1)^n \sqrt{\frac{8 n +
2}{\pi}} j_{2 n} (x - y)
\end{equation}
and the eigenvalues are given by

$\text{\begin{equation}
\begin{array}{ll}
\lambda_n & = \int_0^{\infty} J_0 (x) \psi_n (x) \,
\mathrm{d} x\\
& = \sqrt{\frac{2 n + \frac{1}{2}}{\pi}} \frac{\Gamma \left( n +
\frac{1}{2} \right)^2}{\Gamma (n + 1)^2}\\
& = \sqrt{\frac{2 n + \frac{1}{2}}{\pi}} (n + 1)_{- \frac{1}{2}}^2
\end{array}
\end{equation}}$

where $(n + 1)_{- \frac{1}{2}}$ is the Pochhammer symbol (ascending/rising
factorial). 
\end{theorem}

\begin{proof}
TODO: show that $Y^{\perp}_n (| x - y |) = \psi_n (x) \psi_n (y)$ converges
uniformly by demonstrating that it is compact relative to the canonical
metric induced by the process then apply Hilbert's proof of a theorem that
was initiated by Schmidt as written in
Whittaker and Watson's "A Course of Modern Analysis". Note that just
because Hilbert and Schmidt are involved this does not mean this is the
Hilbert-Schmidt class of operators; specifically, nothing in this proof
depends on the boundedness of the domain or the square integrability of the
kernel; both of which are significant limitations of the classical Mercer
theorem. To see that unLet $\phi_n (\alpha)$ be a complete set of orthogonal
functions satisfying the homogeneous integral equation with symmetric
nucleus
\begin{equation}
\phi (\alpha) = \lambda \int_a^b K (\alpha, \xi) \phi (\xi) \, \mathrm{d} \xi
\end{equation}
the corresponding characteristic numbers being $\lambda_1, \lambda_2,
\lambda_3, \ldots$ Now suppose that the series $\sum_{n = 1}^{\infty}
\frac{\phi_n (\alpha) \phi_n (\gamma)}{\lambda_n}$ is uniformly convergent
when $0 \leqslant a \leq \alpha \leq b \leqslant \infty$, $0 \leqslant a
\leq \gamma \leq b \leqslant \infty$. Then it will be shown that
\begin{equation}
K (\alpha, \gamma) = \sum_{n = 1}^{\infty} \frac{\phi_n (\alpha) \phi_n
(\gamma)}{\lambda_n}
\end{equation}
For consider the symmetric nucleus
\begin{equation}
H (\alpha, \gamma) = K (\alpha, \gamma) - \sum_{n = 1}^{\infty}
\frac{\phi_n (\alpha) \phi_n (\gamma)}{\lambda_n}
\end{equation}
If this nucleus is not identically zero, it will possess at least one
characteristic number $\mu$. Let $\psi (\gamma)$ be any solution of the
equation
\begin{equation}
\psi (\alpha) = \mu \int_a^b H (\alpha, \xi) \psi (\xi) \, \mathrm{d} \xi
\end{equation}
which does not vanish identically. Multiply by $\phi_m (\alpha)$ and
integrate term-by-term (which we may do since the series converges uniformly
by hypothesis proved by other means), and get
\begin{equation}
\int_a^b \psi (\alpha) \phi_m (\alpha) \, \mathrm{d} \alpha = \mu \int_a^b \int_a^b
\left[ K (\alpha, \xi) - \sum_{n = 1}^{\infty} \frac{\phi_n (\alpha)
\phi_n (\xi)}{\lambda_n} \right] \psi (\xi) \phi_m (\alpha) \, \mathrm{d} \xi \, \mathrm{d} \alpha
= 0
\end{equation}
Therefore $\psi (\alpha)$ is orthogonal to $\phi_1 (\alpha), \phi_2
(\alpha), \ldots$; and so taking the equation
\begin{equation}
\psi (\alpha) = \mu \int_a^b \left[ K (\alpha, \xi) - \sum_{n =
1}^{\infty} \frac{\phi_n (\alpha) \phi_n (\xi)}{\lambda_n} \right] \psi
(\xi) \, \mathrm{d} \xi
\end{equation}
we have
\begin{equation}
\psi (\alpha) = \mu \int_a^b K (\alpha, \xi) \psi (\xi) \, \mathrm{d} \xi
\end{equation}
Therefore $\mu$ is a characteristic number of $K (\alpha, \gamma)$, and so
$\psi (\alpha)$ must be a linear combination of the functions $\phi_n
(\alpha)$ corresponding to this number; let
\begin{equation}
\psi (\alpha) = \sum_m a_m \phi_m (\alpha)
\end{equation}
Multiply by $\phi_m (\gamma)$ and integrate; then since $\psi (\alpha)$ is
orthogonal to all the functions $\phi_m (\alpha)$, we see that $a_m = 0$,
so, contrary to hypothesis, $\psi (\alpha) = 0$. The contradiction implies
that the nucleus $H (\alpha, \gamma)$ must be identically zero; that is to
say, $K (\alpha, \gamma)$ can be expanded in the given series, if it is
uniformly convergent.

\section{Explanation}

\textbf{Given:}
\begin{itemize}
\item An orthogonal set of functions $\phi_n (\alpha)$ defined over an
interval $[a, b]$.

\item A symmetric kernel $K (\alpha, \gamma)$ defined over $[a, b] \times
[a, b]$.

\item The series $\sum_{n = 1}^{\infty} \frac{\phi_n (\alpha) \phi_n
(\gamma)}{\lambda_n}$ uniformly converges to $K (\alpha, \gamma)$.
\end{itemize}
\textbf{To Prove:}
\begin{itemize}
\item The functions $\phi_n (\alpha)$ are the unique eigenfunctions of the
integral operator with kernel $K (\alpha, \gamma)$.
\end{itemize}
\textbf{Proof:}
\begin{enumerate}
\item \textbf{Uniform Convergence of Series Representation:}
\begin{itemize}
\item By hypothesis, the series
\begin{equation}
K (\alpha, \gamma) = \sum_{n = 0}^{\infty} \frac{\phi_n (\alpha)
\phi_n (\gamma)}{\lambda_n}
\end{equation}
converges uniformly to $K (\alpha, \gamma)$.

\item This uniform convergence ensures that the series represents $K$
accurately over the entire domain $[a, b] \times [a, b]$.
\end{itemize}
\item \textbf{Orthogonality and Completeness:}
\begin{itemize}
\item The functions $\phi_n$ are orthogonal, meaning:
\begin{equation}
\int_a^b \phi_m (\alpha) \phi_n (\alpha) \, \mathrm{d} \alpha = 0 \quad \text{for
all } m \neq n
\end{equation}
\item Orthogonality implies that no $\phi_n$ can be represented by a
linear combination of other $\phi_m$s in the set.
\end{itemize}
\item \textbf{Eigenfunction Equation:}
\begin{itemize}
\item Each function $\phi_n$ satisfies the integral equation:
\begin{equation}
\phi_n (\alpha) = \lambda_n \int_a^b K (\alpha, \xi) \phi_n (\xi) \, \mathrm{d}
\xi
\end{equation}
defining them as eigenfunctions of $K$ with corresponding eigenvalues
$\lambda_n$.
\end{itemize}
\item \textbf{Uniqueness:}
\begin{itemize}
\item Assume there exists another function $\psi (\alpha)$ that is not a
linear combination of $\phi_n$ and also satisfies the integral equation
for some $\lambda$:
\begin{equation}
\psi (\alpha) = \lambda \int_a^b K (\alpha, \xi) \psi (\xi) \, \mathrm{d} \xi
\end{equation}
\item Multiply both sides by $\phi_m (\alpha)$ and integrate:
\begin{equation}
\int_a^b \psi (\alpha) \phi_m (\alpha) \, \mathrm{d} \alpha = \lambda \int_a^b
\int_a^b K (\alpha, \xi) \psi (\xi) \phi_m (\alpha) \, \mathrm{d} \xi \, \mathrm{d} \alpha
\end{equation}
\item Since $\psi$ is orthogonal to all $\phi_n$, the left-hand side is
zero, implying $\psi (\alpha)$ must be zero by the completeness of
$\phi_n$.
\end{itemize}
\item \textbf{Conclusion:}
\begin{itemize}
\item The set $\phi_n$ uniquely represents the kernel $K$ via their
series expansion. No other function set orthogonal to $\phi_n$ can
satisfy the kernel's integral equation unless it is zero.

\item Therefore, $\phi_n (\alpha)$ are the unique eigenfunctions of the
integral operator defined by $K (\alpha, \gamma)$.
\end{itemize}
\end{enumerate}
\end{proof}

\begin{theorem}
The series
\begin{equation}
\begin{array}{ll}
J_0 (t) & = \sum_{k = 0 }^{\infty} \lambda_k \psi_k (t)\\
& = \sum_{k = 0 }^{\infty} \sqrt{\frac{2 n + \frac{1}{2}}{\pi}} 
\sqrt{\frac{8 n + 2}{\pi}} (n + 1)_{- \frac{1}{2}}^2 (- 1)^n j_{2 n}
(t)\\
& = \sum_{k = 0 }^{\infty} \frac{4 n + 1}{\pi} (n + 1)_{-
\frac{1}{2}}^2 (- 1)^n j_{2 n} (t)
\end{array}
\end{equation}
converges uniformly for all complex $t$ except the origin where it has a
regular singular point where $\lim_{t \rightarrow 0} J_0 (t) = 1$.
\end{theorem}

\begin{proof}

\begin{equation}
\sqrt{\frac{2 n + \frac{1}{2}}{\pi}} \sqrt{\frac{8 n + 2}{\pi}} =
\frac{\sqrt{16 n^2 + 8 n + 1}}{\sqrt{\pi^2}} = \frac{\sqrt{(4 n + 1) (4 n
+ 1)}}{\sqrt{\pi \pi}} = \frac{4 n + 1}{\pi}
\end{equation}
TODO..show that operator defined by Equation (\ref{T}) is compact relative
to the canonical metric induced by the covariance kernel $J_0 (| x - y |)$
which can be demonstrated by calculating the metric entropy integral and
showing it is finite for all positive epsilon
\end{proof}

\end{document}
