\documentclass{article}
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{hyperref}

\geometry{margin=1in}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\title{The Canonical Metric of Stationary Gaussian Processes: A Mathematical Exposition}
\author{}
\date{}

\begin{document}

\maketitle

\section{Introduction and Preliminaries}

Let $\{X(t), t \in T\}$ be a stationary Gaussian process defined on an index set $T$ (typically $T \subseteq \mathbb{R}^d$). By stationarity, we mean that:
\begin{itemize}
    \item The mean function $\mu(t) = \mathbb{E}[X(t)]$ is constant
    \item The covariance function $C(s,t) = \text{Cov}(X(s),X(t))$ depends only on the difference $s-t$
\end{itemize}

For simplicity, we assume the process is centered ($\mu(t) = 0$) and normalized ($\text{Var}[X(t)] = 1$). The covariance function can thus be written as $C(h) = \mathbb{E}[X(t+h)X(t)]$, depending only on the lag $h$.

\section{The Variance Structure Function}

\begin{definition}
The variance structure function (also called variogram) of a stationary Gaussian process is defined as:
\begin{equation}
D(h) = \text{Var}[X(t+h) - X(t)]
\end{equation}
\end{definition}

\begin{proposition}
The variance structure function can be expressed in terms of the covariance function as:
\begin{equation}
D(h) = 2(C(0) - C(h))
\end{equation}
\end{proposition}

\begin{proof}
\begin{align}
D(h) &= \text{Var}[X(t+h) - X(t)] \\
&= \mathbb{E}[(X(t+h) - X(t))^2] \quad \text{(since $\mathbb{E}[X(t+h) - X(t)] = 0$)} \\
&= \mathbb{E}[X(t+h)^2] - 2\mathbb{E}[X(t+h)X(t)] + \mathbb{E}[X(t)^2] \\
&= \text{Var}[X(t+h)] + \text{Var}[X(t)] - 2\text{Cov}(X(t+h), X(t)) \\
&= C(0) + C(0) - 2C(h) \\
&= 2(C(0) - C(h))
\end{align}
\end{proof}

\section{The Canonical Metric}

\begin{definition}
The canonical metric associated with a stationary Gaussian process is defined as:
\begin{equation}
d(h) = \sqrt{D(h)} = \sqrt{2(C(0) - C(h))}
\end{equation}
\end{definition}

\begin{theorem}
The function $d(h) = \sqrt{D(h)}$ is a metric on the index set $T$.
\end{theorem}

\begin{proof}
We need to verify the four metric axioms:

\medskip
\noindent 1. \textbf{Non-negativity}: Since $D(h)$ is a variance, $D(h) \geq 0$, thus $d(h) = \sqrt{D(h)} \geq 0$.

\medskip
\noindent 2. \textbf{Identity of indiscernibles}: 
\begin{itemize}
    \item If $h = 0$, then $D(0) = \text{Var}[X(t) - X(t)] = 0$, thus $d(0) = 0$.
    \item Conversely, if $d(h) = 0$, then $D(h) = 0$, which means $\text{Var}[X(t+h) - X(t)] = 0$. This implies $X(t+h) - X(t)$ is constant almost surely. Since we're dealing with a stationary Gaussian process with continuous paths, this constant must be zero, which can only happen when $h = 0$.
\end{itemize}

\medskip
\noindent 3. \textbf{Symmetry}: Since $D(-h) = \text{Var}[X(t-h) - X(t)] = \text{Var}[X(t) - X(t+h)] = D(h)$ by stationarity, we have $d(-h) = d(h)$.

\medskip
\noindent 4. \textbf{Triangle inequality}: This is the most challenging part to prove. We need to show:
\begin{equation}
d(h_1 + h_2) \leq d(h_1) + d(h_2)
\end{equation}

Let's approach this via the Hilbert space structure underlying Gaussian processes.

Consider the Hilbert space $H$ generated by the random variables $\{X(t), t \in T\}$ with inner product $\langle X(s), X(t) \rangle = \mathbb{E}[X(s)X(t)] = C(s-t)$.

The squared distance between $X(s)$ and $X(t)$ in this Hilbert space is:
\begin{align}
\|X(s) - X(t)\|^2 &= \mathbb{E}[(X(s) - X(t))^2] \\
&= \text{Var}[X(s) - X(t)] \\
&= D(s-t)
\end{align}

Therefore, $d(s-t) = \|X(s) - X(t)\|$.

Using the triangle inequality in Hilbert space:
\begin{align}
\|X(t+h_1+h_2) - X(t)\| &\leq \|X(t+h_1+h_2) - X(t+h_1)\| + \|X(t+h_1) - X(t)\| \\
d(h_1+h_2) &\leq d(h_2) + d(h_1)
\end{align}

The last step follows from stationarity, which gives:
\begin{itemize}
    \item $\|X(t+h_1+h_2) - X(t+h_1)\| = \|X(t+h_2) - X(t)\| = d(h_2)$
    \item $\|X(t+h_1) - X(t)\| = d(h_1)$
\end{itemize}

Therefore, $d(h)$ satisfies the triangle inequality, completing the proof that it is a metric.
\end{proof}

\section{Implications and Applications}

\begin{theorem}[Dudley]
Let $\{X(t), t \in T\}$ be a centered Gaussian process with canonical metric $d$. If the metric entropy integral converges:
\begin{equation}
\int_0^{\delta} \sqrt{\log N(T, d, \varepsilon)} \, d\varepsilon < \infty
\end{equation}
where $N(T, d, \varepsilon)$ is the minimum number of $d$-balls of radius $\varepsilon$ needed to cover $T$, then the process has continuous sample paths almost surely.
\end{theorem}

This theorem connects the canonical metric to sample path regularity, demonstrating why this metric is fundamental in Gaussian process theory.

\begin{corollary}
If $D(h) \sim |h|^{2\alpha}$ as $h \to 0$ for some $\alpha > 0$, then the sample paths of the process are Hölder continuous with exponent $\beta$ for any $\beta < \alpha$.
\end{corollary}

\section{Examples}

\begin{enumerate}
    \item \textbf{Brownian motion}: $D(h) = |h|$, thus $d(h) = \sqrt{|h|}$. This corresponds to sample paths that are Hölder continuous with exponent $\beta < 1/2$.
    
    \item \textbf{Ornstein-Uhlenbeck process}: $D(h) = 2(1 - e^{-\lambda|h|}) \sim 2\lambda|h|$ as $h \to 0$, resulting in the same regularity as Brownian motion.
    
    \item \textbf{Squared exponential covariance}: $C(h) = e^{-|h|^2/2}$, thus $D(h) = 2(1 - e^{-|h|^2/2}) \sim |h|^2$ as $h \to 0$, corresponding to differentiable sample paths.
\end{enumerate}

\section{Conclusion}

We have established that the square root of the variance structure function of a stationary Gaussian process indeed defines a canonical metric associated with the process. This metric not only provides a natural distance measure in the parameter space but also characterizes fundamental properties of the process, including sample path regularity, continuity, and differentiability.

The canonical metric forms a bridge between the probabilistic properties of Gaussian processes and the geometry of their index sets, making it a central concept in the theory of Gaussian processes with applications ranging from stochastic analysis to spatial statistics and machine learning.

\end{document}