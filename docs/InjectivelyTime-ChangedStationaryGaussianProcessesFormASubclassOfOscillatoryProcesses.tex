\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,latexsym}

%%%%%%%%%% Start TeXmacs macros
\newcommand{\assign}{:=}
\newcommand{\mathd}{\mathrm{d}}
\newcommand{\tmaffiliation}[1]{\\ #1}
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
\newcommand{\tmtextbf}[1]{\text{{\bfseries{#1}}}}
\newenvironment{proof}{\noindent\textbf{Proof\ }}{\hspace*{\fill}$\Box$\medskip}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
%%%%%%%%%% End TeXmacs macros

\begin{document}

\title{Injectively Time-Changed Stationary Processes Form a Subclass of
Oscillatory Processes}

\author{
  Stephen Crowley
  \tmaffiliation{July 8, 2025}
}

\maketitle

\section*{Abstract}

Monotonic time changes of stationary Gaussian processes yield oscillatory
processes in the sense of Priestley which are a subclass of non-stationary
Gaussian processes having the spectral representation $K (t, s) = \int_{-
\infty}^{\infty} \phi_t (\lambda) \phi_s (\lambda) \mathd \mu (\lambda)$ where
$\mu (\lambda)$ is the spectral measure which has the same interpretation as
it does in the case of stationary processes and the oscillatory function
$\phi_t (\lambda) = e^{i t \lambda} A_t (\lambda)$ where $A_t (\lambda)$ is a
$\tmop{time} - \tmop{dependent}$ gain function. The monotonicity is necessary
and sufficient for the resulting covariance operator to be self-adjoint and
positive definite. Bidirectional inversion formulas for reconstructing
stationary processes from oscillatory sample paths, random measure inversion
from sample paths, and inversion of the underslying stationary random white
noise measure are derived and the covariance kernel is expressed as an
integral of the ergodic sample paths of the process.

{\tableofcontents}

\section{Preliminaries}

\begin{definition}
  [Stationary Gaussian Process] Let $Y (u)$ be a mean-zero stationary Gaussian
  process on $\mathbb{R}$ with covariance kernel
  \[ K_0  (u - v) =\mathbb{E} [Y (u) Y (v)] . \]
  The spectral representation is
  \[ Y (u) = \int_{- \infty}^{\infty} e^{i \lambda u}  \sqrt{f_0 (\lambda)} 
     \hspace{0.17em} dW (\lambda), \]
  where $f_0 (\lambda)$ is the spectral density and $dW (\lambda)$ is a
  standard complex white noise random measure:
  \[ \mathbb{E} [dW (\lambda) \overline{dW (\mu)}] = \delta (\lambda - \mu) 
     \hspace{0.17em} d \lambda \hspace{0.17em} d \mu . \]
\end{definition}

\begin{definition}
  [Time Change] Let $\theta : \mathbb{R} \to \mathbb{R}$ be strictly
  increasing, $C^2$, with $\dot{\theta} (t) > 0$ for all $t$. Define the
  time-changed process
  \[ X (t) \assign Y (\theta (t)) \hspace{0.17em} \sqrt{\dot{\theta} (t)} . \]
\end{definition}

\section{Main Theorem and Proof}

\begin{theorem}
  [Monotonic Time Change Yields Oscillatory Process] Let $Y (u)$ be a
  stationary Gaussian process as above, and let $\theta$ be strictly
  increasing, $C^2$, with $\dot{\theta} (t) > 0$. Then $X (t)$ is a mean-zero
  Gaussian process with covariance
  \[ K (t, s) = K_0 (| \theta (t) - \theta (s) |) \sqrt{\dot{\theta} (t) 
     \dot{\theta} (s)}, \]
  and spectral representation
  \[ X (t) = \int_{- \infty}^{\infty} e^{i \lambda \theta (t)} 
     \sqrt{\dot{\theta} (t)}  \hspace{0.17em} \sqrt{f_0 (\lambda)} 
     \hspace{0.17em} dW (\lambda) . \]
  The process $X$ is oscillatory in the sense of Priestley, and the covariance
  operator is self-adjoint and positive definite.
\end{theorem}

\begin{proof}
  \tmtextbf{Step 1: Covariance Structure.}
  
  By construction,
  \[ \mathbb{E} [X (t) X (s)] =\mathbb{E} [Y (\theta (t)) Y (\theta (s))]
     \sqrt{\dot{\theta} (t)  \dot{\theta} (s)} = K_0 (| \theta (t) - \theta
     (s) |) \sqrt{\dot{\theta} (t)  \dot{\theta} (s)} . \]
  \tmtextbf{Step 2: Spectral Representation.}
  
  The spectral representation for $Y$ gives
  \[ Y (\theta (t)) = \int_{- \infty}^{\infty} e^{i \lambda \theta (t)} 
     \sqrt{f_0 (\lambda)}  \hspace{0.17em} dW (\lambda) . \]
  Thus,
  \[ X (t) = Y (\theta (t)) \hspace{0.17em} \sqrt{\dot{\theta} (t)} = \int_{-
     \infty}^{\infty} e^{i \lambda \theta (t)}  \sqrt{\dot{\theta} (t)} 
     \hspace{0.17em} \sqrt{f_0 (\lambda)}  \hspace{0.17em} dW (\lambda) . \]
  \tmtextbf{Step 3: Oscillatory Class (Priestley).}
  
  The process $X (t)$ admits the continuous spectral representation
  \[ X (t) = \int_{- \infty}^{\infty} A (t, \lambda)  \hspace{0.17em} d \Phi
     (\lambda), \]
  where
  \[ A (t, \lambda) = e^{i \lambda \theta (t)}  \sqrt{\dot{\theta} (t)}, \]
  and $d \Phi (\lambda) = \sqrt{f_0 (\lambda)}  \hspace{0.17em} dW (\lambda)$.
  This matches Priestley's definition of an oscillatory process.
  
  \tmtextbf{Step 4: Self-Adjointness and Positive Definiteness.}
  
  The covariance operator $K$ is positive definite because for any finite
  collection $\{t_j \}$ and scalars $\{c_j \}$,
  \[ \sum_{j, k} c_j \overline{c_k} K (t_j, t_k) = \sum_{j, k} c_j
     \overline{c_k} K_0 (| \theta (t_j) - \theta (t_k) |) \sqrt{\dot{\theta}
     (t_j)  \dot{\theta} (t_k)} \geq 0, \]
  since $K_0$ is positive definite and $\dot{\theta} (t) > 0$.
  
  Self-adjointness follows because $K (t, s) = \overline{K (s, t)}$ and the
  operator is defined by integration against a real, symmetric kernel.
  
  \tmtextbf{Step 5: Necessity of Monotonicity.}
  
  If $\dot{\theta} (t)$ vanishes or changes sign, the scaling factor
  $\sqrt{\dot{\theta} (t)}$ becomes ill-defined or nonreal, and the kernel can
  lose positive definiteness. For non-monotonic $\theta$, the deficiency
  indices $n_+$ and $n_-$ of the associated operator are unequal, so no
  self-adjoint extension exists (von Neumann's theorem).
\end{proof}

\section{Bidirectional Inversion Formulas}

\begin{theorem}
  [Stationary to Oscillatory Sample Path Inversion] Let $X (t)$ be an
  oscillatory process constructed from a stationary process $Y (u)$ via
  monotonic time change $\theta$. Then the following bidirectional inversion
  formulas hold:
  
  \tmtextbf{(a) Recovering Stationary Process from Oscillatory Sample Path:}
  Given oscillatory sample path $X (t)$ and time change $\theta$, the
  stationary process is recovered by:
  \[ Y (u) = X (\theta^{- 1} (u)) \frac{1}{\sqrt{\dot{\theta} (\theta^{- 1}
     (u))}} \]
  \tmtextbf{(b) Recovering Random Measure from Stationary Process:} Given
  stationary process $Y (u)$ with spectral density $f_0 (\lambda)$, the random
  measure is:
  \[ d \Phi (\lambda) = \lim_{T \to \infty}  \frac{1}{2 T}  \int_{- T}^T Y (u)
     \sqrt{f_0 (\lambda)}  \hspace{0.17em} e^{- i \lambda u}  \hspace{0.17em}
     du \]
  \tmtextbf{(c) Recovering White Noise from Random Measure:} Given random
  measure $d \Phi (\lambda) = \sqrt{f_0 (\lambda)}  \hspace{0.17em} dW
  (\lambda)$, the white noise is:
  \[ dW (\lambda) = \frac{d \Phi (\lambda)}{\sqrt{f_0 (\lambda)}} \]
  \tmtextbf{(d) Direct White Noise Recovery from Oscillatory Sample Path:}
  Combining the above, the white noise is recovered directly from oscillatory
  sample path $X (t)$ by:
  \[ dW (\lambda) = \lim_{T \to \infty}  \frac{1}{2 T}  \int_{- T}^T X
     (\theta^{- 1} (u)) \frac{1}{\sqrt{\dot{\theta} (\theta^{- 1} (u))}} 
     \frac{1}{\sqrt{f_0 (\lambda)}} e^{- i \lambda u}  \hspace{0.17em} du \]
\end{theorem}

\begin{proof}
  \tmtextbf{Part (a): Stationary Process Recovery.}
  
  By construction of the time-changed process:
  \[ X (t) = Y (\theta (t)) \sqrt{\dot{\theta} (t)} \]
  Setting $u = \theta (t)$, so $t = \theta^{- 1} (u)$:
  \[ X (\theta^{- 1} (u)) = Y (\theta (\theta^{- 1} (u))) \sqrt{\dot{\theta}
     (\theta^{- 1} (u))} = Y (u) \sqrt{\dot{\theta} (\theta^{- 1} (u))} \]
  Therefore:
  \[ Y (u) = X (\theta^{- 1} (u)) \frac{1}{\sqrt{\dot{\theta} (\theta^{- 1}
     (u))}} \]
  \tmtextbf{Part (b): Random Measure Recovery.}
  
  For a stationary process with spectral representation $Y (u) = \int e^{i
  \lambda u}  \sqrt{f_0 (\lambda)}  \hspace{0.17em} dW (\lambda)$, the Fourier
  inversion formula gives:
  \[ \sqrt{f_0 (\lambda)}  \hspace{0.17em} dW (\lambda) = \lim_{T \to \infty} 
     \frac{1}{2 T}  \int_{- T}^T Y (u) e^{- i \lambda u}  \hspace{0.17em} du
  \]
  Thus:
  \[ d \Phi (\lambda) = \sqrt{f_0 (\lambda)}  \hspace{0.17em} dW (\lambda) =
     \lim_{T \to \infty}  \frac{1}{2 T}  \int_{- T}^T Y (u) e^{- i \lambda u} 
     \hspace{0.17em} du \]
  \tmtextbf{Part (c): White Noise Extraction.}
  
  By definition, $d \Phi (\lambda) = \sqrt{f_0 (\lambda)}  \hspace{0.17em} dW
  (\lambda)$, so:
  \[ dW (\lambda) = \frac{d \Phi (\lambda)}{\sqrt{f_0 (\lambda)}} \]
  \tmtextbf{Part (d): Direct Recovery Chain.}
  
  Combining parts (a), (b), and (c):
  
  \begin{align}
    dW (\lambda) & = \frac{1}{\sqrt{f_0 (\lambda)}} \lim_{T \to \infty} 
    \frac{1}{2 T}  \int_{- T}^T Y (u) e^{- i \lambda u}  \hspace{0.17em} du \\
    & = \frac{1}{\sqrt{f_0 (\lambda)}} \lim_{T \to \infty}  \frac{1}{2 T} 
    \int_{- T}^T X (\theta^{- 1} (u)) \frac{1}{\sqrt{\dot{\theta} (\theta^{-
    1} (u))}} e^{- i \lambda u}  \hspace{0.17em} du \\
    & = \lim_{T \to \infty}  \frac{1}{2 T}  \int_{- T}^T X (\theta^{- 1} (u))
    \frac{1}{\sqrt{\dot{\theta} (\theta^{- 1} (u))}}  \frac{1}{\sqrt{f_0
    (\lambda)}} e^{- i \lambda u}  \hspace{0.17em} du 
  \end{align}
  
  \tmtextbf{Convergence Analysis.}
  
  The convergence in part (d) is guaranteed because:
  \begin{enumerate}
    \item The inverse scaling $\frac{1}{\sqrt{\dot{\theta} (\theta^{- 1}
    (u))}}$ converts the oscillatory process back to a stationary process
    
    \item The stationary process has well-defined spectral inversion by the
    classical theory
    
    \item The spectral density normalization $\frac{1}{\sqrt{f_0 (\lambda)}}$
    ensures proper extraction of white noise
  \end{enumerate}
\end{proof}

\section{Kernel Recovery from Infinite Sample Path}

\begin{theorem}
  [Kernel Recovery from Infinite Sample Path] Let $X (t)$ be an oscillatory
  process on $\mathbb{R}$ with continuous spectral representation
  \[ X (t) = \int_{- \infty}^{\infty} A (t, \lambda)  \hspace{0.17em} d \Phi
     (\lambda), \]
  where $A (t, \lambda)$ is the deterministic amplitude function and $d \Phi
  (\lambda)$ is the orthogonal random measure with spectral density $f
  (\lambda)$. If the process is mean-square continuous and ergodic, then the
  covariance kernel $K (t, s) =\mathbb{E} [X (t) X (s)]$ can be recovered from
  a single infinitely long sample path by the formula
  \[ K (t, s) = \lim_{T \to \infty}  \frac{1}{2 T}  \int_{- T}^T X (t + u) X
     (s + u)  \hspace{0.17em} du \]
  almost surely.
\end{theorem}

\begin{proof}
  \tmtextbf{Step 1: Ergodic Decomposition.}
  
  For an oscillatory process $X (t)$ with continuous spectral representation,
  the covariance function is
  \[ K (t, s) =\mathbb{E} [X (t) X (s)] = \int_{- \infty}^{\infty} A (t,
     \lambda) \overline{A (s, \lambda)} \hspace{0.17em} f (\lambda) 
     \hspace{0.17em} d \lambda . \]
  \tmtextbf{Step 2: Empirical Average.}
  
  Consider the empirical average along a single infinite sample path:
  \[ \hat{K}_T (t, s) = \frac{1}{2 T}  \int_{- T}^T X (t + u) X (s + u) 
     \hspace{0.17em} du. \]
  \tmtextbf{Step 3: Spectral Substitution.}
  
  Substituting the spectral representation:
  
  \begin{align}
    \hat{K}_T (t, s) & = \frac{1}{2 T}  \int_{- T}^T \left( \int_{-
    \infty}^{\infty} A (t + u, \lambda) \hspace{0.17em} d \Phi (\lambda)
    \right) \left( \int_{- \infty}^{\infty} \overline{A (s + u, \mu)}
    \hspace{0.17em} d \overline{\Phi (\mu)} \right) du \\
    & = \frac{1}{2 T}  \int_{- T}^T \int_{- \infty}^{\infty} \int_{-
    \infty}^{\infty} A (t + u, \lambda) \overline{A (s + u, \mu)}
    \hspace{0.17em} d \Phi (\lambda) d \overline{\Phi (\mu)} \hspace{0.17em}
    du. 
  \end{align}
  
  \tmtextbf{Step 4: Orthogonality of Random Measure.}
  
  Using the orthogonality property $\mathbb{E} [d \Phi (\lambda) d
  \overline{\Phi (\mu)}] = f (\lambda) \delta (\lambda - \mu) d \lambda d
  \mu$:
  
  \begin{align}
    \mathbb{E} [\hat{K}_T (t, s)] & = \frac{1}{2 T}  \int_{- T}^T \int_{-
    \infty}^{\infty} A (t + u, \lambda) \overline{A (s + u, \lambda)}
    \hspace{0.17em} f (\lambda)  \hspace{0.17em} d \lambda \hspace{0.17em} du
    \\
    & = \int_{- \infty}^{\infty} f (\lambda)  \left[ \frac{1}{2 T}  \int_{-
    T}^T A (t + u, \lambda) \overline{A (s + u, \lambda)} \hspace{0.17em} du
    \right] d \lambda . 
  \end{align}
  
  \tmtextbf{Step 5: Ergodicity and Convergence.}
  
  For oscillatory processes arising from monotonic time changes of stationary
  processes, the amplitude function $A (t, \lambda) = e^{i \lambda \theta (t)}
  \sqrt{\dot{\theta} (t)}$ ensures that the process is ergodic. By the
  ergodic theorem:
  \[ \lim_{T \to \infty}  \frac{1}{2 T}  \int_{- T}^T A (t + u, \lambda)
     \overline{A (s + u, \lambda)} \hspace{0.17em} du = A (t, \lambda)
     \overline{A (s, \lambda)} \]
  almost surely.
  
  \tmtextbf{Step 6: Dominated Convergence.}
  
  Under the assumption of mean-square continuity and finite spectral measure,
  the dominated convergence theorem applies:
  \[ \lim_{T \to \infty}  \hat{K}_T (t, s) = \int_{- \infty}^{\infty} A (t,
     \lambda) \overline{A (s, \lambda)} \hspace{0.17em} f (\lambda) 
     \hspace{0.17em} d \lambda = K (t, s) \]
  almost surely.
\end{proof}

\section{Expected Zero Count}

If $K_0$ is twice differentiable and $- \ddot{K}_0 (0) > 0$, the expected
number of zeros of $X$ in $[0, T]$ is
\[ \mathbb{E} [N ([0, T])] = \sqrt{- \ddot{K}_0 (0)}  \hspace{0.17em} (\theta
   (T) - \theta (0)) . \]

\section{Summary of Inversion Chain}

The complete inversion chain for recovering the underlying white noise from an
oscillatory sample path is:

\begin{align}
  \text{Oscillatory Process } X (t) & \xrightarrow{\text{Inverse Time Change}}
  \text{Stationary Process } Y (u) \\
  Y (u) & \xrightarrow{\text{Spectral Inversion}} \text{Random Measure } d
  \Phi (\lambda) \\
  d \Phi (\lambda) & \xrightarrow{\text{Spectral Density Normalization}}
  \text{White Noise } dW (\lambda) 
\end{align}

Each step is mathematically rigorous and involves well-defined limiting
procedures. The existence of the spectral representation guarantees
convergence at each stage.

\section{Conclusion}

The complete framework establishes a bijective correspondence between
stationary Gaussian processes and oscillatory processes via monotonic time
changes. All transformations are invertible with explicit formulas, and the
underlying white noise can be recovered from oscillatory sample paths through
a well-defined chain of operations involving inverse time changes, spectral
inversions, and spectral density normalizations.

\r{}

\end{document}
