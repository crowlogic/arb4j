<TeXmacs|2.1.4>

<style|<tuple|generic|framed-theorems>>

<\body>
  \;

  <\theorem>
    [Theoretical Limitations of Large Language Models] Let <math|\<cal-M\>>
    be any large language model trained on a finite corpus with finite
    computational resources, producing outputs <math|Y> in response to inputs
    <math|X> according to a conditional probability distribution
    <math|P<rsub|\<cal-M\>><around|(|Y\|X|)>>. Then the following limitations
    hold:

    <\enumerate>
      <item><math|\<cal-M\>> cannot guarantee the generation of factually
      correct or logically valid statements for arbitrary input <math|X>.

      <item><math|\<cal-M\>> cannot exceed the information or concepts
      present in its training data, nor can it create fundamentally new
      knowledge without external input.

      <item><math|\<cal-M\>> may produce outputs exhibiting bias or error if
      such patterns are present in its data or training procedure.

      <item><math|\<cal-M\>> lacks self-awareness, consciousness, or genuine
      understanding, and does not possess intent.
    </enumerate>
  </theorem>

  <\proof>
    These statements follow from the design of <math|\<cal-M\>> as a
    statistical pattern-matching system, which:

    <\itemize>
      <item>Relies on approximating <math|P<around|(|Y\|X|)>> as learned from
      finite samples, so cannot generalize perfectly or reason infallibly.

      <item>Is limited to correlations learnable from its dataset and the
      knowledge distribution within that data.

      <item>Is subject to inheriting limitations, errors, and biases found in
      the training data and the algorithms used.

      <item>Is not designed with the faculties of sentient beings, so cannot
      have attributes such as understanding or intent.
    </itemize>
  </proof>
</body>

<\initial>
  <\collection>
    <associate|magnification|1.2>
    <associate|page-medium|paper>
  </collection>
</initial>