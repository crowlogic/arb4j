Foundations of Hamilton-Jacobi-Bellman Equations and Stochastic Optimal Control




1  Foundations of Hamilton-Jacobi-Bellman Equations 

The Hamilton-Jacobi-Bellman (HJB) equation provides necessary and sufficient conditions for optimality in stochastic control problems. Let's develop the mathematical framework systematically.
2  Problem Formulation 

Consider a stochastic dynamical system with state x(t) evolving according to:
d*x(t)=f(x(t),u(t),t)*d*t+σ*(x(t),u(t),t)*d*W(t) 
where:
•  u(t) is the control input
•  W(t) is a Wiener process
•  f represents the drift term
•  σ represents the diffusion term
The objective is to minimize a cost functional:
J(x,t,u)=E*[int_t^TL(x(s),u(s),s)*d*s+ϕ(x(T))] 
where:
•  L is the running cost
•  ϕ is the terminal cost
•  T is the terminal time
3  Dynamic Programming Principle 

The fundamental principle states that the value function V*(x,t) satisfies:
V*(x,t)=min_(u(⋅)) E*[V*(x*(t+d*t),t+d*t)+L(x(t),u(t),t)*d*t] 
This leads to the HJB equation through infinitesimal analysis.
4  The HJB Equation 

For a smooth value function, the HJB equation is:
-V_t=min_u{L(x,u,t)+V_x*f(x,u,t)+1/2*t*r*(σ*σ^T*V_(x*x))} 
with terminal condition:
V*(x,T)=ϕ(x) 
where:
•  V_t is the partial derivative with respect to time
•  V_x is the gradient with respect to state
•  V_(x*x) is the Hessian matrix
5  Verification Theorem 

For a smooth solution V of the HJB equation, if:
1.V satisfies the terminal condition
2.V is twice continuously differentiable
3.The minimizer u^∗*(x,t) exists
Then V is the optimal value function and u^∗ is the optimal control.
6  Stochastic Maximum Principle 

The optimal control can be characterized through:
u^∗*(x,t)=arg min_u{L(x,u,t)+V_x*f(x,u,t)+1/2*t*r*(σ*σ^T*V_(x*x))} 
This provides a feedback law when the value function is known.
7  Viscosity Solutions 

When classical solutions don't exist, the concept of viscosity solutions becomes crucial:
1.A continuous function V is a viscosity subsolution if for any smooth test function ϕ:
-ϕ_t(x_0,t_0)-H(x_0,D*ϕ(x_0,t_0),D^2*ϕ(x_0,t_0))≤0 
2.A continuous function V is a viscosity supersolution if for any smooth test function ϕ:
-ϕ_t(x_0,t_0)-H(x_0,D*ϕ(x_0,t_0),D^2*ϕ(x_0,t_0))≥0 
where H is the Hamiltonian.
8  Linear-Quadratic Case 

For linear systems with quadratic costs:
d*x=(A*x+B*u)*d*t+σ*d*W 
L(x,u)=x^T*Q*x+u^T*R*u 
The value function takes the form:
V*(x,t)=x^T*P(t)*x 
where P(t) satisfies the Riccati equation.
9  Numerical Methods 

Common approaches include:
•  Finite difference methods for the HJB PDE
•  Policy iteration
•  Value iteration
•  Approximate dynamic programming for high-dimensional problems
This framework provides the theoretical foundation for solving stochastic optimal control problems, though practical implementation often requires problem-specific approximations or numerical methods.