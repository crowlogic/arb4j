\documentclass{article}
\usepackage[english]{babel}
\usepackage{geometry,amsmath,amssymb,latexsym,theorem}
\geometry{letterpaper}

%%%%%%%%%% Start TeXmacs macros
\newcommand{\assign}{:=}
\newcommand{\tmmathbf}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
\newcommand{\tmtextbf}[1]{\text{{\bfseries{#1}}}}
\newcommand{\tmtextit}[1]{\text{{\itshape{#1}}}}
\newenvironment{proof}{\noindent\textbf{Proof\ }}{\hspace*{\fill}$\Box$\medskip}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
{\theorembodyfont{\rmfamily}\newtheorem{remark}{Remark}}
\newtheorem{theorem}{Theorem}
%%%%%%%%%% End TeXmacs macros

\begin{document}

\title{
  Unitary Time Changes of Stationary Processes Yield Oscillatory Processes\\
  
}

\author{Stephen Crowley}

\date{September 16, 2025}

\maketitle

\begin{abstract}
  A unitary time-change operator $U_{\theta}$ is constructed for absolutely
  continuous, strictly increasing time reparametrizations $\theta$, acting on
  functions that are locally square-integrable(meaning over compact sets).
  Applying $U_{\theta}$ to the Cram{\'e}r spectral representation of a
  stationary process yields an oscillatory process in the sense of Priestley
  with oscillatory function $\varphi_t (\lambda) = \sqrt{\dot{\theta} (t)}
  e^{i \lambda \theta (t)}$, evolutionary spectrum $dF_t (\lambda) =
  \dot{\theta} (t) dF (\lambda)$ and expected zero-counting function
  $\mathbb{E} [N_{[0, T]}] = \sqrt{- \ddot{K} (0)}  \hspace{0.17em} [\theta
  (T) - \theta (0)] \tmmathbf{}$. The sample paths of any non-degenerate
  second-order stationary process are locally square integrable, making the
  unitary time-change operator $U_{\theta}$ applicable to typical
  realizations. A zero-localization measure $d \mu (t) = \delta (Z (t)) |
  \dot{Z} (t) | dt$ induces a Hilbert space $L^2 (\mu)$ on the zero set of
  each oscillatory process realization $Z (t)$, and the multiplication
  operator $(Lf) (t) = tf (t)$ has simple pure point spectrum equal to the
  zero crossing set of $Z$.
\end{abstract}

{\tableofcontents}

\section{Gaussian Processes}

\subsection{Definition}

\begin{definition}
  \label{def:gaussian_process}\tmtextbf{(Gaussian process)} Let $(\Omega,
  \mathcal{F}, \mathbb{P})$ be a probability space and $T$ a nonempty index
  set. A family $\{X_t : t \in T\}$ of real-valued random variables on
  $(\Omega, \mathcal{F}, \mathbb{P})$ is called a Gaussian process if for
  every finite subset $\{t_1, \ldots, t_n \} \subset T$ the random vector
  $(X_{t_1}, \ldots, X_{t_n})$ is multivariate normal (possibly degenerate).
  Equivalently, every finite linear combination $\sum_{i = 1}^n a_i X_{t_i}$
  is either almost surely constant or Gaussian. The mean function is $m (t)
  \assign \mathbb{E} [X_t]$ and the covariance kernel is
  \begin{equation}
    \label{eq:covariance_kernel} K (s, t) = \mathrm{Cov} (X_s, X_t)
  \end{equation}
  For any finite $(t_i)_{i = 1}^n \subset T$, the matrix $K_{ij} = K (t_i,
  t_j)$ is symmetric positive semidefinite, and a Gaussian process is
  completely determined in law by $m$ and $K$.
\end{definition}

\begin{definition}
  \label{def:canonical_metric}The canonical metric associated with a Gaussian
  process is
  \begin{equation}
    d (s, t) = \sqrt{\mathbb{E} [(X_s - X_t)^2]} = \sqrt{K (s, s) + K (t, t) -
    2 K (s, t)}
  \end{equation}
\end{definition}

\subsection{Sample Path Realizations}

\begin{definition}
  \label{def:L2loc}\tmtextbf{[Locally square-integrable functions]} Define
  \begin{equation}
    L^2_{\tmop{loc}} (\mathbb{R}) \assign \left\{ f : \mathbb{R} \to
    \mathbb{C}: \int_K |f (t) |^2 dt < \infty \text{for every compact } K
    \subseteq \mathbb{R} \right\}
  \end{equation}
\end{definition}

\begin{remark}
  \label{rem:L2loc_properties}Every bounded measurable set in $\mathbb{R}$ is
  compact or contained in a compact set; hence $L^2_{\tmop{loc}} (\mathbb{R})$
  contains functions that are square-integrable on every bounded interval,
  including functions with polynomial growth at infinity.
\end{remark}

\begin{theorem}
  \label{thm:paths_loc}\tmtextbf{[Sample paths in $L^2_{\tmop{loc}}
  (\mathbb{R})$]} Let $\{X (t)\}_{t \in \mathbb{R}}$ be a second-order
  stationary process with
  \begin{equation}
    \label{eq:finite_variance} \sigma^2 \assign \mathbb{E} [X (t)^2] < \infty
  \end{equation}
  Then almost every sample path lies in $L^2_{\tmop{loc}} (\mathbb{R})$.
  However, for non-degenerate processes with $\sigma^2 > 0$, sample paths are
  not globally square-integrable.
\end{theorem}

\begin{proof}
  Fix an arbitrary bounded interval $[a, b] \subset \mathbb{R}$ with $a < b$.
  Define the random variable
  \begin{equation}
    \label{eq:Yab_def} Y_{[a, b]} \assign \int_a^b X (t)^2 dt
  \end{equation}
  \begin{enumerate}
    \item By Tonelli's theorem, since $X (t)^2 \geq 0$,
    \begin{equation}
      \label{eq:tonelli_application} \mathbb{E} [Y_{[a, b]}] =\mathbb{E}
      \left[ \int_a^b X (t)^2 dt \right] = \int_a^b \mathbb{E} [X (t)^2] dt
    \end{equation}
    \item By stationarity of $X$, $\mathbb{E} [X (t)^2] = \sigma^2$ for all $t
    \in \mathbb{R}$. Therefore
    \begin{equation}
      \label{eq:expectation_Yab} \mathbb{E} [Y_{[a, b]}] = \int_a^b \sigma^2
      dt = \sigma^2  (b - a)
    \end{equation}
    \item Since $b - a < \infty$ and $\sigma^2 < \infty$ by assumption
    \eqref{eq:finite_variance},
    \begin{equation}
      \label{eq:Yab_finite_expectation} \mathbb{E} [Y_{[a, b]}] < \infty
    \end{equation}
    \item By Markov's inequality, for any $M > 0$,
    \begin{equation}
      \label{eq:markov_inequality} \mathbb{P} (Y_{[a, b]} > M) \leq
      \frac{\mathbb{E} [Y_{[a, b]}]}{M} = \frac{\sigma^2  (b - a)}{M}
    \end{equation}
    \item Taking $M \to \infty$ in \eqref{eq:markov_inequality},
    \begin{equation}
      \label{eq:Yab_finite_probability} \mathbb{P} (Y_{[a, b]} < \infty) = 1
    \end{equation}
    \item Now let $K \subset \mathbb{R}$ be an arbitrary compact set. Since
    $K$ is compact in $\mathbb{R}$, it is closed and bounded. Therefore there
    exists $N > 0$ such that
    \begin{equation}
      \label{eq:compact_bounded} K \subseteq [- N, N]
    \end{equation}
    \item By \eqref{eq:Yab_finite_probability} applied to $[a, b] = [- N, N]$,
    \begin{equation}
      \label{eq:interval_N_finite} \mathbb{P} \left( \int_{- N}^N X (t)^2 dt <
      \infty \right) = 1
    \end{equation}
    \item Since $K \subseteq [- N, N]$ by \eqref{eq:compact_bounded},
    \begin{equation}
      \label{eq:K_integral_bound} \int_K X (t)^2 dt \leq \int_{- N}^N X (t)^2
      dt
    \end{equation}
    \item Combining \eqref{eq:interval_N_finite} and
    \eqref{eq:K_integral_bound},
    \begin{equation}
      \label{eq:K_finite} \mathbb{P} \left( \int_K X (t)^2 dt < \infty \right)
      = 1
    \end{equation}
    \item Since $K$ was arbitrary, \eqref{eq:K_finite} holds for every compact
    set $K \subset \mathbb{R}$. Therefore, almost every sample path $t \mapsto
    X (t, \omega)$ satisfies
    \begin{equation}
      \label{eq:sample_path_L2loc} \int_K |X (t, \omega) |^2 dt < \infty \quad
      \forall \text{compact } K \subset \mathbb{R}
    \end{equation}
    which means almost every sample path lies in $L^2_{\tmop{loc}}
    (\mathbb{R})$.
    
    \item For the global divergence statement, assume $\sigma^2 > 0$. For each
    $n \in \mathbb{N}$, by \eqref{eq:expectation_Yab},
    \begin{equation}
      \label{eq:expectation_growth} \mathbb{E} \left[ \int_{- n}^n X (t)^2 dt
      \right] = 2 n \sigma^2
    \end{equation}
    \item As $n \to \infty$, the right side of \eqref{eq:expectation_growth}
    diverges:
    \begin{equation}
      \label{eq:expectation_divergence} \lim_{n \to \infty} \mathbb{E} \left[
      \int_{- n}^n X (t)^2 dt \right] = \lim_{n \to \infty} 2 n \sigma^2 =
      \infty
    \end{equation}
    \item By monotone convergence theorem, since $\int_{- n}^n X (t)^2 dt$
    increases monotonically with $n$,
    \begin{equation}
      \label{eq:MCT_application} \mathbb{E} \left[ \lim_{n \to \infty} 
      \int_{- n}^n X (t)^2 dt \right] = \lim_{n \to \infty} \mathbb{E} \left[
      \int_{- n}^n X (t)^2 dt \right] = \infty
    \end{equation}
    \item Therefore
    \begin{equation}
      \label{eq:global_divergence} \mathbb{P} \left( \int_{- \infty}^{\infty}
      X (t)^2 dt = \infty \right) = 1
    \end{equation}
    \item Thus sample paths are not in $L^2 (\mathbb{R})$, only in
    $L^2_{\tmop{loc}} (\mathbb{R})$.
  \end{enumerate}
\end{proof}

\subsection{Stationary processes}

\begin{definition}
  \label{def:cramer}\tmtextbf{[Cram{\'e}r spectral
  representation]}{\cite{stationaryAndRelatedStochasticProcesses}} A zero-mean
  stationary process $X$ with spectral measure $F$ admits the sample path
  representation
  \begin{equation}
    \label{eq:cramer_representation} X (t) = \int_{\mathbb{R}} e^{i \lambda t}
    d \Phi (\lambda)
  \end{equation}
  which has covariance
  \begin{equation}
    \label{eq:stationary_covariance} R_X  (t - s) = \int_{\mathbb{R}} e^{i
    \lambda (t - s)} dF (\lambda)
  \end{equation}
\end{definition}

\subsection{Oscillatory Processes}\label{sec:oscillatory}

\begin{definition}
  \label{def:osc_proc}\tmtextbf{[Oscillatory
  process]{\cite{evolutionarySpectraAndNonStationaryProcesses}}} Let $F$ be a
  finite nonnegative Borel measure on $\mathbb{R}$. Let
  \begin{equation}
    \label{eq:gain_L2} A_t \in L^2 (F) \quad \forall t \in \mathbb{R}
  \end{equation}
  be the gain function and
  \begin{equation}
    \label{eq:oscillatory_function} \varphi_t (\lambda) = A_t (\lambda) e^{i
    \lambda t}
  \end{equation}
  be the corresponding oscillatory function then an oscillatory process is a
  stochastic process which can be represented as
  \begin{equation}
    \label{eq:oscillatory_process} Z (t) = \int_{\mathbb{R}} \varphi_t
    (\lambda) d \Phi (\lambda) = \int_{\mathbb{R}} A_t (\lambda) e^{i \lambda
    t} d \Phi (\lambda)
  \end{equation}
  where $\Phi$ is a complex orthogonal random measure with spectral measure
  $F$ which satisfies the relation
  \begin{equation}
    \label{eq:orthogonality_phi} d\mathbb{E} [\Phi (\lambda) \overline{\Phi
    (\mu)}] = \delta (\lambda - \mu) dF (\lambda)
  \end{equation}
  and has the corresponding covariance kernel
  \begin{equation}
    \label{eq:oscillatory_covariance} R_Z (t, s) =\mathbb{E} [Z (t)
    \overline{Z (s)}] = \int_{\mathbb{R}} A_t (\lambda) \overline{A_s
    (\lambda)} e^{i \lambda (t - s)} dF (\lambda) = \int_{\mathbb{R}}
    \varphi_t (\lambda) \overline{\varphi_s (\lambda)} dF (\lambda)
  \end{equation}
\end{definition}

\begin{theorem}
  \label{thm:realvaluedness}\tmtextbf{[Real-valuedness criterion for
  oscillatory processes]} Let $Z$ be an oscillatory process with oscillatory
  function
  \begin{equation}
    \label{eq:osc_func_def} \varphi_t (\lambda) = A_t (\lambda) e^{i \lambda
    t}
  \end{equation}
  and spectral measure $F$. Then $Z$ is real-valued if and only if
  \begin{equation}
    \label{eq:gain_symmetry} A_t  (- \lambda) = \overline{A_t (\lambda)}
  \end{equation}
  for $F$-almost every $\lambda \in \mathbb{R}$, equivalently
  \begin{equation}
    \label{eq:osc_symmetry} \varphi_t  (- \lambda) = \overline{\varphi_t
    (\lambda)}
  \end{equation}
  for $F$-almost every $\lambda \in \mathbb{R}$.
\end{theorem}

\begin{proof}
  \begin{enumerate}
    \item Assume $Z$ is real-valued. Then for all $t \in \mathbb{R}$,
    \begin{equation}
      \label{eq:real_valued_condition} Z (t) = \overline{Z (t)}
    \end{equation}
    \item From the oscillatory representation \eqref{eq:oscillatory_process},
    \begin{equation}
      \label{eq:Z_representation} Z (t) = \int_{\mathbb{R}} A_t (\lambda) e^{i
      \lambda t} d \Phi (\lambda)
    \end{equation}
    \item Taking the complex conjugate of both sides of
    \eqref{eq:Z_representation},
    \begin{equation}
      \label{eq:Z_conjugate} \overline{Z (t)} = \overline{\int_{\mathbb{R}}
      A_t (\lambda) e^{i \lambda t} d \Phi (\lambda)} = \int_{\mathbb{R}}
      \overline{A_t (\lambda)} e^{- i \lambda t} d \overline{\Phi (\lambda)}
    \end{equation}
    \item For a real-valued process, the orthogonal random measure must
    satisfy the symmetry property
    \begin{equation}
      \label{eq:phi_symmetry} d \overline{\Phi (\lambda)} = - d \Phi (-
      \lambda)
    \end{equation}
    \item Substituting \eqref{eq:phi_symmetry} into \eqref{eq:Z_conjugate},
    \begin{equation}
      \label{eq:Z_conjugate_substituted} \overline{Z (t)} = -
      \int_{\mathbb{R}} \overline{A_t (\lambda)} e^{- i \lambda t} d \Phi (-
      \lambda)
    \end{equation}
    \item Apply the change of variables $\mu = - \lambda$, so $d \Phi (-
    \lambda) = - d \Phi (\mu)$ and $e^{- i \lambda t} = e^{i \mu t}$:
    \begin{equation}
      \label{eq:change_of_variables} \overline{Z (t)} = - \int_{\mathbb{R}}
      \overline{A_t  (- \mu)} e^{i \mu t}  (- d \Phi (\mu)) =
      \int_{\mathbb{R}} \overline{A_t  (- \mu)} e^{i \mu t} d \Phi (\mu)
    \end{equation}
    \item By \eqref{eq:real_valued_condition}, the right sides of
    \eqref{eq:Z_representation} and \eqref{eq:change_of_variables} must be
    equal:
    \begin{equation}
      \label{eq:integrand_equality} \int_{\mathbb{R}} A_t (\mu) e^{i \mu t} d
      \Phi (\mu) = \int_{\mathbb{R}} \overline{A_t  (- \mu)} e^{i \mu t} d
      \Phi (\mu)
    \end{equation}
    \item Since the stochastic integral representation is unique in $L^2 (F)$,
    the integrands must be equal $F$-almost everywhere:
    \begin{equation}
      \label{eq:gain_equality} A_t (\lambda) = \overline{A_t  (- \lambda)}
      \quad \text{for } F \text{-a.e. } \lambda
    \end{equation}
    \item This is equivalent to \eqref{eq:gain_symmetry}. From
    \eqref{eq:osc_func_def},
    \begin{equation}
      \label{eq:osc_func_neg} \varphi_t  (- \lambda) = A_t  (- \lambda) e^{- i
      \lambda t}
    \end{equation}
    \item Using \eqref{eq:gain_symmetry},
    \begin{equation}
      \label{eq:osc_func_conjugate} \varphi_t  (- \lambda) = \overline{A_t
      (\lambda)} e^{- i \lambda t} = \overline{A_t (\lambda) e^{i \lambda t}}
      = \overline{\varphi_t (\lambda)}
    \end{equation}
    establishing \eqref{eq:osc_symmetry}.
    
    \item Conversely, assume \eqref{eq:gain_symmetry} holds. Reversing the
    steps from \eqref{eq:change_of_variables} to
    \eqref{eq:real_valued_condition} shows that $\overline{Z (t)} = Z (t)$ for
    all $t$, so $Z$ is real-valued.
  \end{enumerate}
\end{proof}

\begin{theorem}
  \label{thm:existence_osc}\tmtextbf{[Existence of Oscillatory Processes]} Let
  $F$ be an absolutely continuous spectral measure and the gain function
  \begin{equation}
    \label{eq:gain_condition} A_t (\lambda) \in L^2 (F) \quad \forall t \in
    \mathbb{R}
  \end{equation}
  be measurable in both time and frequency then the time-dependent spectral
  density is defined by
  \begin{equation}
    \label{eq:time_dependent_spectrum} S_t (\lambda) = \int_{\mathbb{R}} |A_t
    (\lambda) |^2 dF (\lambda) < \infty = \int_{\mathbb{R}} |A_t (\lambda) |^2
    S (\lambda) d \lambda
  \end{equation}
  and there exists a complex orthogonal random measure $\Phi$ with spectral
  measure $F$ such that for each sample path $\omega_0 \in \Omega$
  \begin{equation}
    \label{eq:oscillatory_well_defined} Z (t, \omega_0) = \int_{\mathbb{R}}
    A_t (\lambda) e^{i \lambda t} d \Phi (\lambda, \omega_0)
  \end{equation}
  is well-defined in $L^2 (\Omega)$ and has covariance $R_Z$ as in
  \eqref{eq:oscillatory_covariance}.
\end{theorem}

\begin{proof}
  \begin{enumerate}
    \item Define the space of simple functions on $\mathbb{R}$: for disjoint
    Borel sets $\{E_j \}_{j = 1}^n$ with $F (E_j) < \infty$ and coefficients
    $\{c_j \}_{j = 1}^n \subset \mathbb{C}$,
    \begin{equation}
      \label{eq:simple_function} g (\lambda) = \sum_{j = 1}^n c_j 
      \textbf{1}_{E_j} (\lambda)
    \end{equation}
    \item For simple functions, define the stochastic integral
    \begin{equation}
      \label{eq:integral_simple} \int_{\mathbb{R}} g (\lambda) d \Phi
      (\lambda) \assign \sum_{j = 1}^n c_j \Phi (E_j)
    \end{equation}
    \item Compute the second moment:
    \begin{equation}
      \label{eq:second_moment_simple} \mathbb{E} \left[ \left|
      \int_{\mathbb{R}} g (\lambda) d \Phi (\lambda) \right|^2 \right]
      =\mathbb{E} \left[ \left| \sum_{j = 1}^n c_j \Phi (E_j) \right|^2
      \right] =\mathbb{E} \left[ \sum_{j = 1}^n \sum_{k = 1}^n c_j
      \overline{c_k} \Phi (E_j) \overline{\Phi (E_k)} \right]
    \end{equation}
    \item By linearity of expectation,
    \begin{equation}
      \label{eq:linearity_expectation} \mathbb{E} \left[ \sum_{j = 1}^n
      \sum_{k = 1}^n c_j \overline{c_k} \Phi (E_j) \overline{\Phi (E_k)}
      \right] = \sum_{j = 1}^n \sum_{k = 1}^n c_j \overline{c_k} \mathbb{E}
      [\Phi (E_j) \overline{\Phi (E_k)}]
    \end{equation}
    \item By the orthogonality relation \eqref{eq:orthogonality_phi}, since
    $E_j \cap E_k = \emptyset$ for $j \neq k$,
    \begin{equation}
      \label{eq:orthogonality_application} \mathbb{E} [\Phi (E_j)
      \overline{\Phi (E_k)}] = \left\{\begin{array}{ll}
        F (E_j) & \text{if } j = k\\
        0 & \text{if } j \neq k
      \end{array}\right.
    \end{equation}
    \item Substituting \eqref{eq:orthogonality_application} into
    \eqref{eq:linearity_expectation},
    \begin{equation}
      \label{eq:isometry_simple} \sum_{j = 1}^n \sum_{k = 1}^n c_j
      \overline{c_k} \mathbb{E} [\Phi (E_j) \overline{\Phi (E_k)}] = \sum_{j =
      1}^n |c_j |^2 F (E_j)
    \end{equation}
    \item The right side of \eqref{eq:isometry_simple} equals
    \begin{equation}
      \label{eq:L2_norm_simple} \sum_{j = 1}^n |c_j |^2 F (E_j) =
      \int_{\mathbb{R}} |g (\lambda) |^2 dF (\lambda)
    \end{equation}
    \item Therefore the isometry property holds for simple functions:
    \begin{equation}
      \label{eq:isometry_established} \mathbb{E} \left[ \left|
      \int_{\mathbb{R}} g (\lambda) d \Phi (\lambda) \right|^2 \right] =
      \int_{\mathbb{R}} |g (\lambda) |^2 dF (\lambda)
    \end{equation}
    \item The space of simple functions is dense in $L^2 (F)$. For any $h \in
    L^2 (F)$ and $\epsilon > 0$, there exists a simple function $g$ such that
    \begin{equation}
      \label{eq:density_simple} \int_{\mathbb{R}} |h (\lambda) - g (\lambda)
      |^2 dF (\lambda) < \epsilon
    \end{equation}
    \item By the isometry \eqref{eq:isometry_established} and completeness of
    $L^2 (\Omega)$, the integral extends uniquely by continuity to all $h \in
    L^2 (F)$.
    
    \item Since $A_t \in L^2 (F)$ by assumption \eqref{eq:gain_condition}, and
    $|e^{i \lambda t} | = 1$,
    \begin{equation}
      \label{eq:varphi_L2} \int_{\mathbb{R}} | \varphi_t (\lambda) |^2 dF
      (\lambda) = \int_{\mathbb{R}} |A_t (\lambda) |^2 dF (\lambda) < \infty
    \end{equation}
    so $\varphi_t \in L^2 (F)$.
    
    \item Therefore
    \begin{equation}
      \label{eq:Z_well_defined} Z (t) = \int_{\mathbb{R}} \varphi_t (\lambda)
      d \Phi (\lambda) = \int_{\mathbb{R}} A_t (\lambda) e^{i \lambda t} d
      \Phi (\lambda)
    \end{equation}
    is well-defined in $L^2 (\Omega)$.
    
    \item To compute the covariance, use the sesquilinearity of the stochastic
    integral:
    \begin{equation}
      \label{eq:covariance_computation} R_Z (t, s) =\mathbb{E} [Z (t)
      \overline{Z (s)}] =\mathbb{E} \left[ \int_{\mathbb{R}} \varphi_t
      (\lambda) d \Phi (\lambda) \overline{\int_{\mathbb{R}} \varphi_s (\mu) d
      \Phi (\mu)} \right]
    \end{equation}
    \item By Fubini's theorem for stochastic integrals,
    \begin{equation}
      \label{eq:fubini_stochastic} \mathbb{E} \left[ \int_{\mathbb{R}}
      \varphi_t (\lambda) d \Phi (\lambda) \overline{\int_{\mathbb{R}}
      \varphi_s (\mu) d \Phi (\mu)} \right] = \int_{\mathbb{R}}
      \int_{\mathbb{R}} \varphi_t (\lambda) \overline{\varphi_s (\mu)}
      \mathbb{E} [d \Phi (\lambda) \overline{d \Phi (\mu)}]
    \end{equation}
    \item Using the orthogonality relation \eqref{eq:orthogonality_phi},
    \begin{equation}
      \label{eq:orthogonality_integral} \int_{\mathbb{R}} \int_{\mathbb{R}}
      \varphi_t (\lambda) \overline{\varphi_s (\mu)} \delta (\lambda - \mu) dF
      (\lambda) dF (\mu) = \int_{\mathbb{R}} \varphi_t (\lambda)
      \overline{\varphi_s (\lambda)} dF (\lambda)
    \end{equation}
    \item Substituting the definition \eqref{eq:oscillatory_function},
    \begin{equation}
      \label{eq:covariance_final} R_Z (t, s) = \int_{\mathbb{R}} A_t (\lambda)
      \overline{A_s (\lambda)} e^{i \lambda (t - s)} dF (\lambda)
    \end{equation}
    as claimed in \eqref{eq:oscillatory_covariance}.
  \end{enumerate}
\end{proof}

\section{Unitarily Time-Changed Stationary
Processes}\label{sec:stationary_timechange}

\subsection{Unitary Time-Change Operator $U_{\theta} f$}

\begin{theorem}
  \label{thm:local_unitarity}\tmtextbf{[Unitary time-change and local
  unitarity]} Let the time-scaling function $\theta : \mathbb{R} \to
  \mathbb{R}$ be absolutely continuous, strictly increasing, and bijective,
  with
  \begin{equation}
    \dot{\theta} (t) > 0 \label{pd}
  \end{equation}
  almost everywhere and $\dot{\theta} (t) = 0$ only on sets of Lebesgue
  measure zero. For $f$ measurable, define
  \begin{equation}
    \label{eq:U_theta_def} (U_{\theta} f) (t) = \sqrt{\dot{\theta} (t)} f
    (\theta (t))
  \end{equation}
  Its inverse is given by
  \begin{equation}
    \label{eq:U_theta_inverse} (U_{\theta}^{- 1} g) (s) = \frac{g (\theta^{-
    1} (s))}{\sqrt{\dot{\theta} (\theta^{- 1} (s))}}
  \end{equation}
  For every compact set $K \subseteq \mathbb{R}$ and $f \in L^2_{\tmop{loc}}
  (\mathbb{R})$,
  \begin{equation}
    \label{eq:local_isometry} \int_K | (U_{\theta} f) (t) |^2 dt =
    \int_{\theta (K)} |f (s) |^2 ds
  \end{equation}
  Moreover, $U_{\theta}^{- 1}$ is the inverse of $U_{\theta}$ on
  $L^2_{\tmop{loc}} (\mathbb{R})$.
\end{theorem}

\begin{proof}
  \begin{enumerate}
    \item Let $f \in L^2_{\tmop{loc}} (\mathbb{R})$ and let $K \subset
    \mathbb{R}$ be compact. From the definition \eqref{eq:U_theta_def},
    \begin{equation}
      \label{eq:Utheta_norm_start} \int_K | (U_{\theta} f) (t) |^2 dt = \int_K
      \left| \sqrt{\dot{\theta} (t)} f (\theta (t)) \right|^2 dt
    \end{equation}
    \item Expanding the square,
    \begin{equation}
      \label{eq:expand_square} \int_K \left| \sqrt{\dot{\theta} (t)} f (\theta
      (t)) \right|^2 dt = \int_K \dot{\theta} (t) |f (\theta (t)) |^2 dt
    \end{equation}
    \item Since $\theta$ is absolutely continuous and strictly increasing,
    $\theta' = \dot{\theta}$ exists almost everywhere and $\dot{\theta} (t) >
    0$ a.e.
    
    \item Apply the change of variables $s = \theta (t)$. Then
    \begin{equation}
      \label{eq:change_var_differential} ds = \dot{\theta} (t) dt
    \end{equation}
    \item The inverse function $t = \theta^{- 1} (s)$ exists since $\theta$ is
    strictly increasing and bijective.
    
    \item As $t$ ranges over $K$, the variable $s = \theta (t)$ ranges over
    $\theta (K)$.
    
    \item Since $\theta$ is continuous and $K$ is compact, $\theta (K)$ is
    compact.
    
    \item Substituting \eqref{eq:change_var_differential} into
    \eqref{eq:expand_square},
    \begin{equation}
      \label{eq:after_substitution} \int_K \dot{\theta} (t)  |f (\theta (t))
      |^2 dt = \int_{\theta (K)} |f (s) |^2 ds
    \end{equation}
    \item This establishes the local isometry \eqref{eq:local_isometry}.
    
    \item To verify $U_{\theta}^{- 1}$ is the inverse, compute:
    \begin{equation}
      \label{eq:composition_1} (U_{\theta}^{- 1} U_{\theta} f) (s) =
      U_{\theta}^{- 1}  (U_{\theta} f) (s)
    \end{equation}
    \item By definition \eqref{eq:U_theta_inverse},
    \begin{equation}
      \label{eq:apply_inverse_def} U_{\theta}^{- 1}  (U_{\theta} f) (s) =
      \frac{(U_{\theta} f) (\theta^{- 1} (s))}{\sqrt{\dot{\theta} (\theta^{-
      1} (s))}}
    \end{equation}
    \item By definition \eqref{eq:U_theta_def},
    \begin{equation}
      \label{eq:apply_forward_def} (U_{\theta} f) (\theta^{- 1} (s)) =
      \sqrt{\dot{\theta} (\theta^{- 1} (s))} f (\theta (\theta^{- 1} (s)))
    \end{equation}
    \item Since $\theta \circ \theta^{- 1} =$id,
    \begin{equation}
      \label{eq:theta_inverse_composition} f (\theta (\theta^{- 1} (s))) = f
      (s)
    \end{equation}
    \item Substituting \eqref{eq:apply_forward_def} and
    \eqref{eq:theta_inverse_composition} into \eqref{eq:apply_inverse_def},
    \begin{equation}
      \label{eq:simplify_composition} \frac{\sqrt{\dot{\theta} (\theta^{- 1}
      (s))} f (s)}{\sqrt{\dot{\theta} (\theta^{- 1} (s))}} = f (s)
    \end{equation}
    \item Therefore
    \begin{equation}
      \label{eq:left_inverse} U_{\theta}^{- 1} U_{\theta} = \text{id}
    \end{equation}
    \item Similarly, compute:
    \begin{equation}
      \label{eq:composition_2} (U_{\theta} U_{\theta}^{- 1} g) (t) =
      \sqrt{\dot{\theta} (t)}  (U_{\theta}^{- 1} g) (\theta (t))
    \end{equation}
    \item By definition \eqref{eq:U_theta_inverse},
    \begin{equation}
      \label{eq:apply_inverse_second} (U_{\theta}^{- 1} g) (\theta (t)) =
      \frac{g (\theta^{- 1} (\theta (t)))}{\sqrt{\dot{\theta} (\theta^{- 1}
      (\theta (t)))}}
    \end{equation}
    \item Since $\theta^{- 1} \circ \theta =$id,
    \begin{equation}
      \label{eq:theta_composition} g (\theta^{- 1} (\theta (t))) = g (t),
      \quad \theta^{- 1} (\theta (t)) = t
    \end{equation}
    \item Substituting \eqref{eq:theta_composition} into
    \eqref{eq:apply_inverse_second},
    \begin{equation}
      \label{eq:simplify_second} \frac{g (t)}{\sqrt{\dot{\theta} (t)}}
    \end{equation}
    \item Therefore from \eqref{eq:composition_2},
    \begin{equation}
      \label{eq:right_inverse} (U_{\theta} U_{\theta}^{- 1} g) (t) =
      \sqrt{\dot{\theta} (t)} \cdot \frac{g (t)}{\sqrt{\dot{\theta} (t)}} = g
      (t)
    \end{equation}
    \item Thus
    \begin{equation}
      \label{eq:both_inverses} U_{\theta} U_{\theta}^{- 1} = \text{id}
    \end{equation}
    \item Combining \eqref{eq:left_inverse} and \eqref{eq:both_inverses},
    $U_{\theta}^{- 1}$ is the two-sided inverse of $U_{\theta}$ on
    $L^2_{\tmop{loc}} (\mathbb{R})$.
  \end{enumerate}
\end{proof}

\subsection{Inverse Filter for Unitary Time Transformations}

\begin{theorem}
  \label{thm:inverse_filter}\tmtextbf{[Inverse Filter for Unitary Time
  Transformations]} Let $\theta : \mathbb{R} \to \mathbb{R}$ be absolutely
  continuous, strictly increasing, and bijective with $\theta' (t) > 0$ almost
  everywhere. Let $Y (u)$ be a stationary process with unit variance, and
  define
  \begin{equation}
    \label{eq:Z_transformation} Z (t) = \sqrt{\dot{\theta} (t)} Y (\theta (t))
  \end{equation}
  as the oscillatory process obtained by the unitary time transformation.
  Then:
  \begin{enumerate}
    \item The forward filter kernel is
    \begin{equation}
      \label{eq:forward_kernel} h (t, u) = \sqrt{\dot{\theta} (t)} \delta (u -
      \theta (t))
    \end{equation}
    \item The inverse filter kernel is
    \begin{equation}
      \label{eq:inverse_kernel} g (t, s) = \frac{\delta (s - \theta^{- 1}
      (t))}{\sqrt{\dot{\theta} (\theta^{- 1} (t))}}
    \end{equation}
    \item The composition $(g \circ h)$ recovers the identity:
    \begin{equation}
      \label{eq:filter_identity} Y (t) = \int_{\mathbb{R}} g (t, s) Z (s) ds =
      \frac{Z (\theta^{- 1} (t))}{\sqrt{\dot{\theta} (\theta^{- 1} (t))}}
    \end{equation}
  \end{enumerate}
\end{theorem}

\begin{proof}
  \begin{enumerate}
    \item From \eqref{eq:Z_transformation}, the forward transformation is
    \begin{equation}
      \label{eq:forward_integral} Z (t) = \int_{\mathbb{R}} h (t, u) Y (u) du
    \end{equation}
    \item Substituting \eqref{eq:forward_kernel},
    \begin{equation}
      \label{eq:forward_substitution} \int_{\mathbb{R}} h (t, u) Y (u) du =
      \int_{\mathbb{R}} \sqrt{\dot{\theta} (t)} \delta (u - \theta (t)) Y (u)
      du
    \end{equation}
    \item By the sifting property of the Dirac delta,
    \begin{equation}
      \label{eq:sifting_forward} \int_{\mathbb{R}} \sqrt{\dot{\theta} (t)}
      \delta (u - \theta (t)) Y (u) du = \sqrt{\dot{\theta} (t)} Y (\theta
      (t))
    \end{equation}
    \item This confirms \eqref{eq:Z_transformation}.
    
    \item For the inverse, compute:
    \begin{equation}
      \label{eq:inverse_integral} \int_{\mathbb{R}} g (t, s) Z (s) ds =
      \int_{\mathbb{R}} \frac{\delta (s - \theta^{- 1}
      (t))}{\sqrt{\dot{\theta} (\theta^{- 1} (t))}} Z (s) ds
    \end{equation}
    \item By the sifting property,
    \begin{equation}
      \label{eq:sifting_inverse} \int_{\mathbb{R}} \frac{\delta (s - \theta^{-
      1} (t))}{\sqrt{\dot{\theta} (\theta^{- 1} (t))}} Z (s) ds = \frac{Z
      (\theta^{- 1} (t))}{\sqrt{\dot{\theta} (\theta^{- 1} (t))}}
    \end{equation}
    \item Substituting \eqref{eq:Z_transformation} with $t$ replaced by
    $\theta^{- 1} (t)$,
    \begin{equation}
      \label{eq:Z_at_inverse} Z (\theta^{- 1} (t)) = \sqrt{\dot{\theta}
      (\theta^{- 1} (t))} Y (\theta (\theta^{- 1} (t)))
    \end{equation}
    \item Since $\theta \circ \theta^{- 1} =$id,
    \begin{equation}
      \label{eq:theta_composition_Y} Y (\theta (\theta^{- 1} (t))) = Y (t)
    \end{equation}
    \item Substituting \eqref{eq:Z_at_inverse} and
    \eqref{eq:theta_composition_Y} into \eqref{eq:sifting_inverse},
    \begin{equation}
      \label{eq:final_inverse} \frac{Z (\theta^{- 1} (t))}{\sqrt{\dot{\theta}
      (\theta^{- 1} (t))}} = \frac{\sqrt{\dot{\theta} (\theta^{- 1} (t))} Y
      (t)}{\sqrt{\dot{\theta} (\theta^{- 1} (t))}} = Y (t)
    \end{equation}
    \item This establishes \eqref{eq:filter_identity}, confirming that $g
    \circ h =$id.
  \end{enumerate}
\end{proof}

\subsection{Transformation of Stationary $\to$ Oscillatory Processes via
$U_{\theta}$}

\begin{theorem}
  \label{thm:Utheta_to_osc}\tmtextbf{[Unitary time change yields oscillatory
  process]} Let $X$ be zero-mean stationary as in Definition \ref{def:cramer}.
  For scaling function $\theta$ as in Theorem \ref{thm:local_unitarity},
  define
  \begin{equation}
    \label{eq:Z_def} Z (t) = (U_{\theta} X) (t) = \sqrt{\dot{\theta} (t)} X
    (\theta (t))
  \end{equation}
  Then $Z$ is a realization of an oscillatory process with oscillatory
  function
  \begin{equation}
    \label{eq:oscillatory_function_Z} \varphi_t (\lambda) = \sqrt{\dot{\theta}
    (t)} e^{i \lambda \theta (t)}
  \end{equation}
  gain function
  \begin{equation}
    \label{eq:gain_function_Z} A_t (\lambda) = \sqrt{\dot{\theta} (t)} e^{i
    \lambda (\theta (t) - t)}
  \end{equation}
  and covariance
  \begin{equation}
    \begin{array}{ll}
      R_Z (t, s) & =\mathbb{E} [Z (t) \overline{Z (s)}]\\
      & =\mathbb{E} \left[ \sqrt{\dot{\theta} (t)} X (\theta (t))
      \overline{\sqrt{\dot{\theta} (s)} X (\theta (s))} \right]\\
      & = \sqrt{\dot{\theta} (t)  \dot{\theta} (s)} \mathbb{E} [X (\theta
      (t)) \overline{X (\theta (s))}]\\
      & = \sqrt{\dot{\theta} (t)  \dot{\theta} (s)} R_X  (\theta (t) - \theta
      (s))\\
      & = \sqrt{\dot{\theta} (t)  \dot{\theta} (s)}  \int_{\mathbb{R}} e^{i
      \lambda (\theta (t) - \theta (s))}  \hspace{0.17em} dF (\lambda)
    \end{array} \label{UTCcovar}  \label{eq:covariance_Z} R_Z (t, s) =
    \sqrt{\dot{\theta} (t)  \dot{\theta} (s)}  \int_{\mathbb{R}} e^{i \lambda
    (\theta (t) - \theta (s))} dF (\lambda)
  \end{equation}
\end{theorem}

\begin{proof}
  \begin{enumerate}
    \item From the Cram{\"\i}{\textsterling}{\textexclamdown}r representation
    \eqref{eq:cramer_representation},
    \begin{equation}
      \label{eq:X_cramer} X (u) = \int_{\mathbb{R}} e^{i \lambda u} d \Phi
      (\lambda)
    \end{equation}
    \item Substituting $u = \theta (t)$ into \eqref{eq:X_cramer},
    \begin{equation}
      \label{eq:X_theta_t} X (\theta (t)) = \int_{\mathbb{R}} e^{i \lambda
      \theta (t)} d \Phi (\lambda)
    \end{equation}
    \item From the definition \eqref{eq:Z_def},
    \begin{equation}
      \label{eq:Z_expanded} Z (t) = \sqrt{\dot{\theta} (t)} X (\theta (t)) =
      \sqrt{\dot{\theta} (t)}  \int_{\mathbb{R}} e^{i \lambda \theta (t)} d
      \Phi (\lambda)
    \end{equation}
    \item By linearity of the stochastic integral,
    \begin{equation}
      \label{eq:Z_integral} Z (t) = \int_{\mathbb{R}} \sqrt{\dot{\theta} (t)}
      e^{i \lambda \theta (t)} d \Phi (\lambda)
    \end{equation}
    \item Define
    \begin{equation}
      \label{eq:varphi_t_explicit} \varphi_t (\lambda) \assign
      \sqrt{\dot{\theta} (t)} e^{i \lambda \theta (t)}
    \end{equation}
    \item Then \eqref{eq:Z_integral} becomes
    \begin{equation}
      \label{eq:Z_oscillatory_form} Z (t) = \int_{\mathbb{R}} \varphi_t
      (\lambda) d \Phi (\lambda)
    \end{equation}
    which is the oscillatory representation \eqref{eq:oscillatory_process}.
    
    \item To express this in terms of the standard oscillatory function form,
    write
    \begin{equation}
      \label{eq:varphi_factored} \varphi_t (\lambda) = \sqrt{\dot{\theta} (t)}
      e^{i \lambda \theta (t)} = \sqrt{\dot{\theta} (t)} e^{i \lambda (\theta
      (t) - t)} e^{i \lambda t}
    \end{equation}
    \item Define the gain function
    \begin{equation}
      \label{eq:A_t_explicit} A_t (\lambda) \assign \sqrt{\dot{\theta} (t)}
      e^{i \lambda (\theta (t) - t)}
    \end{equation}
    \item Then
    \begin{equation}
      \label{eq:varphi_as_gain} \varphi_t (\lambda) = A_t (\lambda) e^{i
      \lambda t}
    \end{equation}
    confirming the oscillatory function form \eqref{eq:oscillatory_function}.
    
    \item To compute the covariance, use \eqref{eq:oscillatory_covariance}:
    \begin{equation}
      \label{eq:R_Z_start} R_Z (t, s) =\mathbb{E} [Z (t) \overline{Z (s)}]
    \end{equation}
    \item Substituting \eqref{eq:Z_def},
    \begin{equation}
      \label{eq:R_Z_substituted} R_Z (t, s) =\mathbb{E} \left[
      \sqrt{\dot{\theta} (t)} X (\theta (t)) \overline{\sqrt{\dot{\theta} (s)}
      X (\theta (s))} \right]
    \end{equation}
    \item Since $\dot{\theta}$ is deterministic,
    \begin{equation}
      \label{eq:R_Z_factored} R_Z (t, s) = \sqrt{\dot{\theta} (t)} 
      \sqrt{\dot{\theta} (s)} \mathbb{E} [X (\theta (t)) \overline{X (\theta
      (s))}]
    \end{equation}
    \item By stationarity of $X$, using \eqref{eq:stationary_covariance},
    \begin{equation}
      \label{eq:X_covariance} \mathbb{E} [X (\theta (t)) \overline{X (\theta
      (s))}] = R_X  (\theta (t) - \theta (s)) = \int_{\mathbb{R}} e^{i \lambda
      (\theta (t) - \theta (s))} dF (\lambda)
    \end{equation}
    \item Substituting \eqref{eq:X_covariance} into \eqref{eq:R_Z_factored},
    \begin{equation}
      \label{eq:R_Z_final} R_Z (t, s) = \sqrt{\dot{\theta} (t)  \dot{\theta}
      (s)}  \int_{\mathbb{R}} e^{i \lambda (\theta (t) - \theta (s))} dF
      (\lambda)
    \end{equation}
    establishing \eqref{eq:covariance_Z}.
  \end{enumerate}
\end{proof}

\begin{corollary}
  \label{cor:evol_spec}\tmtextbf{[Evolutionary spectrum of unitarily
  time-changed stationary process]} The evolutionary spectrum is
  \begin{equation}
    \label{eq:evolutionary_spectrum} dF_t (\lambda) = \dot{\theta} (t) dF
    (\lambda)
  \end{equation}
\end{corollary}

\begin{proof}
  \begin{enumerate}
    \item The evolutionary spectrum is defined by
    \begin{equation}
      \label{eq:evol_spec_def} dF_t (\lambda) = |A_t (\lambda) |^2 dF
      (\lambda)
    \end{equation}
    \item From \eqref{eq:gain_function_Z},
    \begin{equation}
      \label{eq:A_t_magnitude_start} |A_t (\lambda) |^2 = \left|
      \sqrt{\dot{\theta} (t)} e^{i \lambda (\theta (t) - t)} \right|^2
    \end{equation}
    \item Since $|e^{i \alpha} | = 1$ for all real $\alpha$,
    \begin{equation}
      \label{eq:exp_magnitude} | e^{i \lambda (\theta (t) - t)} |^2 = 1
    \end{equation}
    \item Therefore
    \begin{equation}
      \label{eq:A_t_magnitude} |A_t (\lambda) |^2 = \left( \sqrt{\dot{\theta}
      (t)} \right)^2 \cdot 1 = \dot{\theta} (t)
    \end{equation}
    \item Substituting \eqref{eq:A_t_magnitude} into \eqref{eq:evol_spec_def},
    \begin{equation}
      \label{eq:evol_spec_final} dF_t (\lambda) = \dot{\theta} (t) dF
      (\lambda)
    \end{equation}
  \end{enumerate}
\end{proof}

\subsection{Covariance operator conjugation}

\begin{proposition}
  \label{prop:conjugation}\tmtextbf{[Operator conjugation]} Let
  \begin{equation}
    \label{eq:T_K_def} (T_K f) (t) \assign \int_{\mathbb{R}} K (|t - s|) f (s)
    ds
  \end{equation}
  with stationary kernel
  \begin{equation}
    \label{eq:K_def} K (h) = \int_{\mathbb{R}} e^{i \lambda h} dF (\lambda)
  \end{equation}
  Define the transformed kernel
  \begin{equation}
    \label{eq:K_theta_def} K_{\theta} (s, t) \assign \sqrt{\dot{\theta} (t) 
    \dot{\theta} (s)} K (| \theta (t) - \theta (s) |)
  \end{equation}
  then the corresponding integral covariance operator is conjugated for all $f
  \in L^2_{\tmop{loc}} (\mathbb{R})$ by
  \begin{equation}
    \label{eq:conjugation} (T_{K_{\theta}} f) (t) = (U_{\theta} T_K
    U_{\theta}^{- 1} f) (t)
  \end{equation}
\end{proposition}

\begin{proof}
  \begin{enumerate}
    \item From \eqref{eq:conjugation}, expand the right side:
    \begin{equation}
      \label{eq:conjugation_expand} (U_{\theta} T_K U_{\theta}^{- 1} f) (t) =
      \sqrt{\dot{\theta} (t)}  (T_K U_{\theta}^{- 1} f) (\theta (t))
    \end{equation}
    \item By definition \eqref{eq:T_K_def},
    \begin{equation}
      \label{eq:T_K_application} (T_K U_{\theta}^{- 1} f) (\theta (t)) =
      \int_{\mathbb{R}} K (| \theta (t) - s|)  (U_{\theta}^{- 1} f) (s) ds
    \end{equation}
    \item By definition \eqref{eq:U_theta_inverse},
    \begin{equation}
      \label{eq:U_inv_application} (U_{\theta}^{- 1} f) (s) = \frac{f
      (\theta^{- 1} (s))}{\sqrt{\dot{\theta} (\theta^{- 1} (s))}}
    \end{equation}
    \item Substituting \eqref{eq:U_inv_application} into
    \eqref{eq:T_K_application},
    \begin{equation}
      \label{eq:integral_substitution} \int_{\mathbb{R}} K (| \theta (t) - s|)
      \frac{f (\theta^{- 1} (s))}{\sqrt{\dot{\theta} (\theta^{- 1} (s))}} ds
    \end{equation}
    \item Apply the change of variables $s = \theta (u)$, so $ds =
    \dot{\theta} (u) du$ and $\theta^{- 1} (s) = u$:
    \begin{equation}
      \label{eq:change_var_s} \int_{\mathbb{R}} K (| \theta (t) - \theta (u)
      |) \frac{f (u)}{\sqrt{\dot{\theta} (u)}}  \dot{\theta} (u) du
    \end{equation}
    \item Simplify:
    \begin{equation}
      \label{eq:simplify_integral} \int_{\mathbb{R}} K (| \theta (t) - \theta
      (u) |) \frac{\dot{\theta} (u)}{\sqrt{\dot{\theta} (u)}} f (u) du =
      \int_{\mathbb{R}} K (| \theta (t) - \theta (u) |) \sqrt{\dot{\theta}
      (u)} f (u) du
    \end{equation}
    \item Substituting \eqref{eq:simplify_integral} into
    \eqref{eq:conjugation_expand},
    \begin{equation}
      \label{eq:full_expression} \sqrt{\dot{\theta} (t)}  \int_{\mathbb{R}} K
      (| \theta (t) - \theta (u) |) \sqrt{\dot{\theta} (u)} f (u) du
    \end{equation}
    \item Bring the constant inside the integral:
    \begin{equation}
      \label{eq:factor_inside} \int_{\mathbb{R}} \sqrt{\dot{\theta} (t)} 
      \sqrt{\dot{\theta} (u)} K (| \theta (t) - \theta (u) |) f (u) du
    \end{equation}
    \item By definition \eqref{eq:K_theta_def},
    \begin{equation}
      \label{eq:K_theta_recognition} \sqrt{\dot{\theta} (t)} 
      \sqrt{\dot{\theta} (u)} K (| \theta (t) - \theta (u) |) = K_{\theta} (u,
      t)
    \end{equation}
    \item Therefore
    \begin{equation}
      \label{eq:final_conjugation} \int_{\mathbb{R}} K_{\theta} (u, t) f (u)
      du = (T_{K_{\theta}} f) (t)
    \end{equation}
    establishing \eqref{eq:conjugation}.
  \end{enumerate}
\end{proof}

\section{Zero Localization}\label{sec:HP}

\begin{definition}
  \label{def:zeromeasure}\tmtextbf{[Zero localization measure]} Let $Z$ be
  real-valued with $Z \in C^1 (\mathbb{R})$ having only simple zeros
  \begin{equation}
    \label{eq:simple_zeros} Z (t_0) = 0 \Rightarrow \dot{Z} (t_0) \neq 0
  \end{equation}
  Define, for Borel $B \subset \mathbb{R}$,
  \begin{equation}
    \label{eq:mu_def} \mu (B) = \int_{\mathbb{R}} \textbf{1}_B (t) \delta (Z
    (t)) | \dot{Z} (t) | dt
  \end{equation}
\end{definition}

\begin{theorem}
  \label{thm:atomicity}\tmtextbf{[Atomicity and local finiteness of zeros and
  delta decomposition]} Under the assumptions of Definition
  \ref{def:zeromeasure}, zeros are locally finite and one has
  \begin{equation}
    \label{eq:delta_decomposition} \delta (Z (t)) = \sum_{t_0 : Z (t_0) = 0}
    \frac{\delta (t - t_0)}{| \dot{Z} (t_0) |}
  \end{equation}
  whence
  \begin{equation}
    \label{eq:mu_atomic} \mu = \sum_{t_0 : Z (t_0) = 0} \delta_{t_0}
  \end{equation}
\end{theorem}

\begin{proof}
  \begin{enumerate}
    \item For any smooth test function $\phi$ with compact support, apply the
    standard change of variables formula for the delta function. Let
    $\{t_0^{(1)}, t_0^{(2)}, \ldots\}$ denote the zeros of $Z$.
    
    \item By the change of variables formula for distributions,
    \begin{equation}
      \label{eq:delta_change_var} \int_{\mathbb{R}} \phi (t) \delta (Z (t)) dt
      = \sum_{t_0 : Z (t_0) = 0} \frac{\phi (t_0)}{| \dot{Z} (t_0) |}
    \end{equation}
    \item The right side of \eqref{eq:delta_change_var} equals
    \begin{equation}
      \label{eq:sum_form} \sum_{t_0 : Z (t_0) = 0} \frac{\phi (t_0)}{| \dot{Z}
      (t_0) |} = \sum_{t_0 : Z (t_0) = 0} \int_{\mathbb{R}} \phi (t)
      \frac{\delta (t - t_0)}{| \dot{Z} (t_0) |} dt
    \end{equation}
    \item By Fubini's theorem (justified since the sum has locally finite
    terms due to $C^1$ regularity and simple zeros),
    \begin{equation}
      \label{eq:interchange_sum_integral} \sum_{t_0 : Z (t_0) = 0}
      \int_{\mathbb{R}} \phi (t) \frac{\delta (t - t_0)}{| \dot{Z} (t_0) |} dt
      = \int_{\mathbb{R}} \phi (t)  \sum_{t_0 : Z (t_0) = 0} \frac{\delta (t -
      t_0)}{| \dot{Z} (t_0) |} dt
    \end{equation}
    \item Comparing \eqref{eq:delta_change_var} and
    \eqref{eq:interchange_sum_integral},
    \begin{equation}
      \label{eq:delta_equality} \int_{\mathbb{R}} \phi (t) \delta (Z (t)) dt =
      \int_{\mathbb{R}} \phi (t)  \sum_{t_0 : Z (t_0) = 0} \frac{\delta (t -
      t_0)}{| \dot{Z} (t_0) |} dt
    \end{equation}
    \item Since $\phi$ is arbitrary,
    \begin{equation}
      \label{eq:delta_Z_decomp} \delta (Z (t)) = \sum_{t_0 : Z (t_0) = 0}
      \frac{\delta (t - t_0)}{| \dot{Z} (t_0) |}
    \end{equation}
    establishing \eqref{eq:delta_decomposition}.
    
    \item Substituting \eqref{eq:delta_Z_decomp} into the definition
    \eqref{eq:mu_def},
    \begin{equation}
      \label{eq:mu_substitution} \mu (B) = \int_{\mathbb{R}} \textbf{1}_B (t) 
      \sum_{t_0 : Z (t_0) = 0} \frac{\delta (t - t_0)}{| \dot{Z} (t_0) |} |
      \dot{Z} (t) | dt
    \end{equation}
    \item By the sifting property of the delta function, $| \dot{Z} (t) |$
    evaluated at $t = t_0$ gives $| \dot{Z} (t_0) |$:
    \begin{equation}
      \label{eq:cancel_derivative} \int_{\mathbb{R}} \textbf{1}_B (t)
      \frac{\delta (t - t_0)}{| \dot{Z} (t_0) |} | \dot{Z} (t) | dt =
      \frac{\textbf{1}_B (t_0) | \dot{Z} (t_0) |}{| \dot{Z} (t_0) |} =
      \textbf{1}_B (t_0)
    \end{equation}
    \item Summing over all zeros,
    \begin{equation}
      \label{eq:mu_sum} \mu (B) = \sum_{t_0 : Z (t_0) = 0} \textbf{1}_B (t_0)
      = \sum_{t_0 \in B : Z (t_0) = 0} 1
    \end{equation}
    \item This is precisely the atomic measure
    \begin{equation}
      \label{eq:mu_atomic_final} \mu = \sum_{t_0 : Z (t_0) = 0} \delta_{t_0}
    \end{equation}
    establishing \eqref{eq:mu_atomic}.
  \end{enumerate}
\end{proof}

\begin{definition}
  \label{def:Hmu}\tmtextbf{[Hilbert space on the zero set]} Let $\mathcal{H}=
  L^2 (\mu)$ with inner product
  \begin{equation}
    \label{eq:inner_product_mu} \langle f, g \rangle = \int_{-
    \infty}^{\infty} f (t) \overline{g (t)} d \mu (t)
  \end{equation}
\end{definition}

\begin{proposition}
  \label{prop:atomic}\tmtextbf{[Atomic structure]} Let
  \begin{equation}
    \label{eq:mu_atomic_assumption} \mu = \sum_{t_0 : Z (t_0) = 0}
    \delta_{t_0}
  \end{equation}
  then
  \begin{equation}
    \label{eq:H_isomorphism} \mathcal{H} \cong \left\{ f : \{t_0 : Z (t_0) =
    0\} \to \mathbb{C}: \sum_{t_0 : Z (t_0) = 0} |f (t_0) |^2 < \infty
    \right\} \cong \ell^2
  \end{equation}
  with orthonormal basis $\{e_{t_0} \}_{t_0 : Z (t_0) = 0}$ where
  \begin{equation}
    \label{eq:basis_vectors} e_{t_0} (t_1) = \delta_{t_0, t_1}
  \end{equation}
\end{proposition}

\begin{proof}
  \begin{enumerate}
    \item By \eqref{eq:mu_atomic_assumption}, $\mu$ is a purely atomic measure
    with atoms at the zero set.
    
    \item For any $f \in L^2 (\mu)$, the $L^2$ norm is
    \begin{equation}
      \label{eq:L2_norm_mu} \|f\|_{L^2 (\mu)}^2 = \int_{\mathbb{R}} |f (t) |^2
      d \mu (t)
    \end{equation}
    \item Substituting \eqref{eq:mu_atomic_assumption},
    \begin{equation}
      \label{eq:norm_sum} \int_{\mathbb{R}} |f (t) |^2 d \mu (t) =
      \int_{\mathbb{R}} |f (t) |^2  \sum_{t_0 : Z (t_0) = 0} \delta_{t_0} 
      (dt) = \sum_{t_0 : Z (t_0) = 0} |f (t_0) |^2
    \end{equation}
    \item Therefore
    \begin{equation}
      \label{eq:norm_equivalence} \|f\|_{L^2 (\mu)}^2 = \sum_{t_0 : Z (t_0) =
      0} |f (t_0) |^2
    \end{equation}
    \item This is precisely the $\ell^2$ norm on the zero set.
    
    \item Define the map $\Psi : L^2 (\mu) \to \ell^2$ by
    \begin{equation}
      \label{eq:isomorphism_map} \Psi (f) = (f (t_0))_{t_0 : Z (t_0) = 0}
    \end{equation}
    \item From \eqref{eq:norm_equivalence}, $\Psi$ is an isometry:
    \begin{equation}
      \label{eq:isometry_Psi} \| \Psi (f)\|_{\ell^2}^2 = \sum_{t_0 : Z (t_0) =
      0} |f (t_0) |^2 = \|f\|_{L^2 (\mu)}^2
    \end{equation}
    \item $\Psi$ is surjective: for any sequence $(c_{t_0}) \in \ell^2$,
    define $f (t) = \sum_{t_0} c_{t_0} \delta_{t_0} (t)$, which is in $L^2
    (\mu)$.
    
    \item Therefore $\Psi$ is a Hilbert space isomorphism, establishing
    \eqref{eq:H_isomorphism}.
    
    \item For the orthonormal basis, define $e_{t_0}$ by
    \eqref{eq:basis_vectors}.
    
    \item Then
    \begin{equation}
      \label{eq:basis_inner_product} \langle e_{t_0}, e_{t_1} \rangle =
      \int_{\mathbb{R}} e_{t_0} (t) \overline{e_{t_1} (t)} d \mu (t) = \sum_{s
      : Z (s) = 0} \delta_{t_0, s} \delta_{t_1, s} = \delta_{t_0, t_1}
    \end{equation}
    \item Therefore $\{e_{t_0} \}$ is an orthonormal set.
    
    \item Since every $f \in L^2 (\mu)$ can be written as
    \begin{equation}
      \label{eq:basis_expansion} f = \sum_{t_0 : Z (t_0) = 0} f (t_0) e_{t_0}
    \end{equation}
    the set $\{e_{t_0} \}$ is complete, hence an orthonormal basis.
  \end{enumerate}
\end{proof}

\begin{definition}
  \label{def:L}\tmtextbf{[Multiplication operator]} Define the linear operator
  \begin{equation}
    \label{eq:L_def} L : \mathcal{D} (L) \subset \mathcal{H} \to \mathcal{H}
  \end{equation}
  by
  \begin{equation}
    \label{eq:L_action} (Lf) (t) = tf (t)
  \end{equation}
  on the support of $\mu$ with domain
  \begin{equation}
    \label{eq:L_domain} \mathcal{D} (L) \assign \left\{ f \in \mathcal{H}:
    \int |tf (t) |^2 d \mu (t) < \infty \right\}
  \end{equation}
\end{definition}

\begin{theorem}
  \label{thm:spectrum}\tmtextbf{[Self-adjointness and spectrum]} $L$ is
  self-adjoint on $\mathcal{H}$ and has pure point, simple spectrum
  \begin{equation}
    \label{eq:spectrum} \sigma (L) = \overline{\{t \in \mathbb{R}: Z (t) =
    0\}}
  \end{equation}
  with eigenvalues $\lambda = t_0$ for each zero $t_0$ and corresponding
  eigenvectors $e_{t_0}$.
\end{theorem}

\begin{proof}
  \begin{enumerate}
    \item For $f, g \in \mathcal{D} (L)$, compute the inner product:
    \begin{equation}
      \label{eq:Lf_g_inner} \langle Lf, g \rangle = \int_{\mathbb{R}} (Lf) (t)
      \overline{g (t)} d \mu (t)
    \end{equation}
    \item By definition \eqref{eq:L_action},
    \begin{equation}
      \label{eq:Lf_substitution} \int_{\mathbb{R}} tf (t) \overline{g (t)} d
      \mu (t)
    \end{equation}
    \item Since $t$ is real-valued, $\bar{t} = t$, so
    \begin{equation}
      \label{eq:conjugate_t} \int_{\mathbb{R}} tf (t) \overline{g (t)} d \mu
      (t) = \int_{\mathbb{R}} f (t) \overline{tg (t)} d \mu (t)
    \end{equation}
    \item The right side of \eqref{eq:conjugate_t} is
    \begin{equation}
      \label{eq:f_Lg_inner} \int_{\mathbb{R}} f (t) \overline{(Lg) (t)} d \mu
      (t) = \langle f, Lg \rangle
    \end{equation}
    \item Therefore
    \begin{equation}
      \label{eq:self_adjoint} \langle Lf, g \rangle = \langle f, Lg \rangle
    \end{equation}
    for all $f, g \in \mathcal{D} (L)$, establishing that $L$ is symmetric.
    
    \item Since $L$ is a multiplication operator on $L^2 (\mu)$, it is
    self-adjoint (by standard functional analysis).
    
    \item To determine the spectrum, compute the action on basis vectors. From
    \eqref{eq:L_action} and \eqref{eq:basis_vectors},
    \begin{equation}
      \label{eq:L_basis} (Le_{t_0}) (t) = te_{t_0} (t) = t \delta_{t_0} (t)
    \end{equation}
    \item By the sifting property,
    \begin{equation}
      \label{eq:sift_basis} t \delta_{t_0} (t) = t_0 \delta_{t_0} (t) = t_0
      e_{t_0} (t)
    \end{equation}
    \item Therefore
    \begin{equation}
      \label{eq:eigenvalue_equation} Le_{t_0} = t_0 e_{t_0}
    \end{equation}
    \item This shows that each $t_0$ is an eigenvalue with eigenvector
    $e_{t_0}$.
    
    \item Since the $\{e_{t_0} \}$ form a complete orthonormal basis
    (Proposition \ref{prop:atomic}), the spectrum is pure point.
    
    \item Each eigenspace is one-dimensional (spanned by $e_{t_0}$), so the
    spectrum is simple and given by the closure of the zero set
    \begin{equation}
      \label{eq:spectrum_result} \sigma (L) = \{t_0 : Z (t_0) = 0\} =
      \overline{\{t \in \mathbb{R}: Z (t) = 0\}}
    \end{equation}
  \end{enumerate}
\end{proof}

\subsection{The Kac-Rice Formula For The Expected Zero Counting Function}

\begin{theorem}
  \label{thm:kac_rice}\tmtextbf{[Kac-Rice Formula for Zero Crossings]} Let $Z
  (t)$ be a centered Gaussian process on $[a, b]$ with covariance $K (s, t)
  =\mathbb{E} [Z (s) Z (t)]$ then the expected number of zeros in $[a, b]$ is
  \begin{equation}
    \label{eq:kac_rice} \mathbb{E} [N_{[a, b]}] = \int_a^b
    \sqrt{\frac{2}{\pi}}  \frac{\sqrt{K (t, t) K_{\dot{Z}} (t, t) - K_{Z,
    \dot{Z}} (t, t)^2}}{K (t, t)} dt
  \end{equation}
  where
  \begin{equation}
    \label{eq:K_tt} K (t, t) =\mathbb{E} [Z (t)^2]
  \end{equation}
  \begin{equation}
    \label{eq:K_dotZ} K_{\dot{Z}} (t, t) = - \partial^2_s \partial_t K (s, t)
    |_{s = t}
  \end{equation}
  and
  \begin{equation}
    \label{eq:K_Z_dotZ} K_{Z, \dot{Z}} (t, t) = \partial_s K (s, t) |_{s = t}
  \end{equation}
\end{theorem}

\begin{proof}
  \begin{enumerate}
    \item The expected number of zeros is given by the Kac-Rice formula for
    the level crossing density at level zero.
    
    \item For a Gaussian process $Z (t)$, the joint distribution of $(Z (t),
    \dot{Z} (t))$ is bivariate normal with covariance matrix
    \begin{equation}
      \label{eq:covariance_matrix} \Sigma (t) = \left(\begin{array}{cc}
        K (t, t) & K_{Z, \dot{Z}} (t, t)\\
        K_{Z, \dot{Z}} (t, t) & K_{\dot{Z}} (t, t)
      \end{array}\right)
    \end{equation}
    \item The determinant of $\Sigma (t)$ is
    \begin{equation}
      \label{eq:determinant} \det \Sigma (t) = K (t, t) K_{\dot{Z}} (t, t) -
      K_{Z, \dot{Z}} (t, t)^2
    \end{equation}
    \item The Kac-Rice formula states that the expected number of zeros in
    $[a, b]$ is
    \begin{equation}
      \label{eq:kac_rice_general} \mathbb{E} [N_{[a, b]}] = \int_a^b p_{Z (t),
      \dot{Z} (t)} (0, v) |v| dv \hspace{0.17em} dt
    \end{equation}
    where $p_{Z (t), \dot{Z} (t)}$ is the joint density of $(Z (t), \dot{Z}
    (t))$.
    
    \item For a centered bivariate normal distribution with covariance $\Sigma
    (t)$,
    \begin{equation}
      \label{eq:joint_density} p_{Z (t), \dot{Z} (t)} (z, v) = \frac{1}{2 \pi
      \sqrt{\det \Sigma (t)}} \exp \left( - \frac{1}{2} \left(\begin{array}{c}
        z\\
        v
      \end{array}\right)^T \Sigma (t)^{- 1} \left(\begin{array}{c}
        z\\
        v
      \end{array}\right) \right)
    \end{equation}
    \item At $z = 0$,
    \begin{equation}
      \label{eq:density_at_zero} p_{Z (t), \dot{Z} (t)} (0, v) = \frac{1}{2
      \pi \sqrt{\det \Sigma (t)}} \exp \left( - \frac{v^2 K (t, t)}{2 \det
      \Sigma (t)} \right)
    \end{equation}
    \item Integrating \eqref{eq:density_at_zero} against $|v|$,
    \begin{equation}
      \label{eq:integrate_v} \int_{- \infty}^{\infty} p_{Z (t), \dot{Z} (t)}
      (0, v) |v| dv = \frac{1}{2 \pi \sqrt{\det \Sigma (t)}}  \int_{-
      \infty}^{\infty} |v| \exp \left( - \frac{v^2 K (t, t)}{2 \det \Sigma
      (t)} \right) dv
    \end{equation}
    \item Using symmetry, $\int_{- \infty}^{\infty} |v| e^{- av^2} dv = 2
    \int_0^{\infty} ve^{- av^2} dv = \frac{1}{a}  \sqrt{\frac{\pi}{a}}$:
    \begin{equation}
      \label{eq:gaussian_integral} \int_{- \infty}^{\infty} |v| \exp \left( -
      \frac{v^2 K (t, t)}{2 \det \Sigma (t)} \right) dv = \sqrt{\frac{2 \pi
      \det \Sigma (t)}{K (t, t)}}
    \end{equation}
    \item Substituting \eqref{eq:gaussian_integral} into
    \eqref{eq:integrate_v},
    \begin{equation}
      \label{eq:density_integral} \frac{1}{2 \pi \sqrt{\det \Sigma (t)}} \cdot
      \sqrt{\frac{2 \pi \det \Sigma (t)}{K (t, t)}} = \sqrt{\frac{2}{\pi}} 
      \frac{\sqrt{\det \Sigma (t)}}{K (t, t)}
    \end{equation}
    \item Using \eqref{eq:determinant},
    \begin{equation}
      \label{eq:final_density} \sqrt{\frac{2}{\pi}}  \frac{\sqrt{K (t, t)
      K_{\dot{Z}} (t, t) - K_{Z, \dot{Z}} (t, t)^2}}{K (t, t)}
    \end{equation}
    \item Integrating \eqref{eq:final_density} over $[a, b]$ yields
    \eqref{eq:kac_rice}.
  \end{enumerate}
\end{proof}

\subsubsection{The Expected Zero Counting Function for Unitarily Time-Changed
Stationary Processes}

\

\begin{theorem}[Expected Zero-Counting Function Of The Oscillatory Process
Subclass of Unitarily Time-Changed Stationary Processes]
  \label{thmexpectedzerocount}Let $\theta : \mathbb{R} \to \mathbb{R}$ be
  absolutely continuous, strictly increasing, and bijective with $\dot{\theta}
  (t) > 0$ almost everywhere. Let $X$ be a centered stationary Gaussian
  process with spectral measure $F$ and covariance function
  \begin{equation}
    \label{eqstationarycov} K (h) = \int_{\mathbb{R}} e^{i \omega h} 
    \hspace{0.17em} dF (\omega)
  \end{equation}
  twice differentiable at $h = 0$. Define the unitarily time-changed process
  \begin{equation}
    \label{eqZprocess} Z (t) = \sqrt{\dot{\theta} (t)}  \hspace{0.17em} X
    (\theta (t))
  \end{equation}
  Then $Z$ is a centered Gaussian process with covariance
  \begin{equation}
    \label{eqZcovariance} K_Z (t, s) = \sqrt{\dot{\theta} (t) \dot{\theta}
    (s)}  \hspace{0.17em} K (\theta (t) - \theta (s))
  \end{equation}
  and the expected number of zeros in $[0, T]$ is
  \begin{equation}
    \label{eqexpectedzeros} \mathbb{E} [N_{[0, T]}] = \sqrt{\frac{- \ddot{K}
    (0)}{\pi K (0)}}  \hspace{0.17em} [\theta (T) - \theta (0)]
  \end{equation}
\end{theorem}

\begin{proof}
  \begin{enumerate}
    \item By the Kac-Rice formula:
    \begin{equation}
      \label{eqkacrice} \mathbb{E} [N_{[0, T]}] = \int_0^T \sqrt{- \lim_{s \to
      t}  \frac{\partial^2}{\partial s \partial t} K_Z (s, t)} 
      \hspace{0.17em} dt
    \end{equation}
    \item Differentiate \eqref{eqZcovariance} with respect to $s$:
    \begin{equation}
      \label{eqfirstpartial} \begin{array}{l}
        \frac{\partial}{\partial s} K_Z (s, t) = \frac{\ddot{\theta} (s)}{2
        \sqrt{\dot{\theta} (s) \dot{\theta} (t)}}  \hspace{0.17em} K (\theta
        (t) - \theta (s)) - \dot{\theta} (s) \sqrt{\dot{\theta} (s)
        \dot{\theta} (t)}  \hspace{0.17em} \dot{K}  (\theta (t) - \theta (s))
      \end{array}
    \end{equation}
    \item Differentiate \eqref{eqfirstpartial} with respect to $t$ and take $s
    \to t$. Since $\dot{K} (0) = 0$ by stationarity:
    \begin{equation}
      \label{eqlimit} \lim_{s \to t}  \frac{\partial^2}{\partial s \partial t}
      K_Z (s, t) = - \dot{\theta} (t)^2  \ddot{K} (0)
    \end{equation}
    \item Substitute \eqref{eqlimit} into \eqref{eqkacrice}:
    \begin{equation}
      \label{eqsubstitute} \mathbb{E} [N_{[0, T]}] = \int_0^T \sqrt{- (-
      \dot{\theta} (t)^2  \ddot{K} (0))}  \hspace{0.17em} dt = \int_0^T
      \sqrt{\dot{\theta} (t)^2  (- \ddot{K} (0))}  \hspace{0.17em} dt
    \end{equation}
    \item Since $\dot{\theta} (t) > 0$ and $\ddot{K} (0) < 0$:
    \begin{equation}
      \label{eqsimplify} \mathbb{E} [N_{[0, T]}] = \int_0^T \dot{\theta} (t)
      \sqrt{- \ddot{K} (0)}  \hspace{0.17em} dt = \sqrt{- \ddot{K} (0)} 
      \int_0^T \dot{\theta} (t)  \hspace{0.17em} dt
    \end{equation}
    \item Evaluate the integral:
    \begin{equation}
      \label{eqfinal} \mathbb{E} [N_{[0, T]}] = \sqrt{- \ddot{K} (0)} 
      \hspace{0.17em} [\theta (T) - \theta (0)]
    \end{equation}
  \end{enumerate}
\end{proof}

\begin{theorem}
  \label{thm:deterministic_zero}\tmtextbf{[Deterministic zero-crossing at
  vanishing derivative]} Let $X$ be a zero-mean stationary process with
  spectral measure $F$ as in Definition \ref{def:cramer} and finite variance
  $\sigma^2 =\mathbb{E} [X (t)^2] < \infty$. Let $\theta : \mathbb{R} \to
  \mathbb{R}$ be the time-change function from Theorem
  \ref{thm:local_unitarity}, which is absolutely continuous (has derivative
  $\dot{\theta}$ that exists almost everywhere and is Lebesgue integrable),
  strictly increasing (so $\theta (t_1) < \theta (t_2)$ whenever $t_1 < t_2$),
  and bijective (one-to-one and onto). The derivative $\dot{\theta} (t)$ is
  strictly positive almost everywhere, meaning $\dot{\theta} (t) > 0$ for all
  $t$ except possibly on a set of Lebesgue measure zero. Define the
  transformed process
  \begin{equation}
    \label{eq:Z_def_det} Z (t) = (U_{\theta} X) (t) = \sqrt{\dot{\theta} (t)}
    X (\theta (t))
  \end{equation}
  as in equation \eqref{eq:U_theta_def}. Consider a point $t_0 \in \mathbb{R}$
  where the derivative vanishes: $\dot{\theta} (t_0) = 0$. Then
  \tmtextbf{every sample path of $Z$ passes through zero at $t_0$}: for all
  $\omega \in \Omega$,
  \begin{equation}
    \label{eq:deterministic_zero_statement} Z (t_0, \omega) = 0
  \end{equation}
  This is a \tmtextbf{deterministic zero-crossing}: unlike the random
  zero-crossings of the stationary process $X$, which occur probabilistically
  according to Bulinskaya's statistics, the zero at $t_0$ occurs with
  certainty in every realization of $Z$. The randomness of $X$ is completely
  suppressed at $t_0$ by the vanishing amplitude factor $\sqrt{\dot{\theta}
  (t_0)} = 0$.
\end{theorem}

\begin{proof}
  \begin{enumerate}
    \item Consider a point $t_0 \in \mathbb{R}$ where $\dot{\theta} (t_0) =
    0$.
    
    \item From the definition \eqref{eq:Z_def_det}, the value of $Z$ at $t_0$
    for any sample path $\omega \in \Omega$ is
    \begin{equation}
      \label{eq:Z_at_t0_expanded} Z (t_0, \omega) = \sqrt{\dot{\theta} (t_0)}
      \cdot X (\theta (t_0), \omega)
    \end{equation}
    \item Since $\dot{\theta} (t_0) = 0$ by hypothesis,
    \begin{equation}
      \label{eq:sqrt_zero_eval} \sqrt{\dot{\theta} (t_0)} = \sqrt{0} = 0
    \end{equation}
    \item Substituting \eqref{eq:sqrt_zero_eval} into
    \eqref{eq:Z_at_t0_expanded},
    \begin{equation}
      \label{eq:zero_product} Z (t_0, \omega) = 0 \cdot X (\theta (t_0),
      \omega) = 0
    \end{equation}
    regardless of the value of $X (\theta (t_0), \omega)$.
    
    \item Since $\omega \in \Omega$ was arbitrary, equation
    \eqref{eq:zero_product} holds for every sample path:
    \begin{equation}
      \label{eq:all_sample_paths} Z (t_0, \omega) = 0 \quad \forall \omega \in
      \Omega
    \end{equation}
    \item Therefore $t_0$ is a deterministic zero-crossing: the process $Z$
    reaches zero at $t_0$ in every realization, not probabilistically.
    
    \item As a direct consequence, the variance of $Z$ at $t_0$ is zero:
    \begin{equation}
      \label{eq:variance_zero_consequence} \text{Var} [Z (t_0)] =\mathbb{E}
      [(Z (t_0) -\mathbb{E}[Z (t_0)])^2] =\mathbb{E} [0^2] = 0
    \end{equation}
    \item By Corollary \ref{cor:evol_spec}, the evolutionary spectrum at $t_0$
    vanishes:
    \begin{equation}
      \label{eq:evol_spectrum_vanish} dF_{t_0} (\lambda) = \dot{\theta} (t_0)
      dF (\lambda) = 0 \cdot dF (\lambda) = 0
    \end{equation}
    meaning there is no spectral energy at $t_0$.
    
    \item The point $t_0$ belongs to the zero set $\{t \in \mathbb{R}: Z (t,
    \omega) = 0\}$ for every $\omega \in \Omega$. By Definition
    \ref{def:L2loc}, this deterministic zero-crossing differs fundamentally
    from the random zero-crossings governed by the statistics of the
    stationary process $X$: it occurs because the amplitude factor
    $\sqrt{\dot{\theta} (t_0)}$ vanishes, completely eliminating the influence
    of the random process $X$ at that instant.
  \end{enumerate}
\end{proof}

\begin{thebibliography}{1}
  \bibitem[1]{stationaryAndRelatedStochasticProcesses}Harald Cram{\'e}r  and 
  M.R.~Leadbetter. {\newblock}\tmtextit{Stationary and Related Processes:
  Sample Function Properties and Their Applications}. {\newblock}Wiley Series
  in Probability and Mathematical Statistics. 1967.{\newblock}
  
  \bibitem[2]{evolutionarySpectraAndNonStationaryProcesses}Maurice~B
  Priestley. {\newblock}Evolutionary spectra and non-stationary processes.
  {\newblock}\tmtextit{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 27(2):204--229, 1965.{\newblock}
\end{thebibliography}

\

\end{document}
