\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=1in}

\title{On the Prediction of Non-Stationary Processes}
\author{
  N.~A.~Abdrabbo and M.~B.~Priestley \\
  University of Manchester \\
  \small{[Received August 1966. Revised January 1967]}
}
\date{}

\begin{document}

\maketitle

\begin{abstract}
We consider the problem of linear least-squares prediction for a class of nonstationary processes which possess \emph{evolutionary spectral representations}. It is shown that, under certain conditions, such processes admit moving-average representations in terms of time-dependent coefficients. This feature enables us to develop a close analogue of the Wiener-Kolmogorov approach to the corresponding problem for stationary processes.
\end{abstract}

\section{Introduction}

The Wiener-Kolmogorov ``linear least-squares'' approach to the prediction and filtering of stationary processes is now well established, and may, in principle, be applied to any practical problem involving stationary processes whose second-order properties are known. Accounts of this theory have been written at various levels of mathematical rigour (see, for example, \cite{Doob1953,Bartlett1955,GrenanderRosenblatt1957,Yaglom1962,Whittle1965}), but so far the corresponding problem for non-stationary processes has received little attention.

There have been a few isolated attempts to deal with this topic, but in the main the approaches have been either too general or too restricted to be useful in practical applications. For example, Parzen~\cite{Parzen1961} has solved the non-stationary prediction problem in principle, but his approach is a somewhat abstract one, and his ``solution'' for the optimum predictor is expressed as a certain inner-product in a Hilbert space. Cram√©r~\cite{Cramer1961a,Cramer1961b} considered the same problem, and obtained some interesting results in the form of ``existence theorems'', but did not present a method for determining the explicit form of a predictor in terms of the observed variables. Similar remarks apply to the work of Davis~\cite{Davis1952}. On the other hand, there have appeared several papers written from an engineering standpoint (see, for example, \cite{Booton1952,Zadeh1953,Bendat1956}), but in most cases the ``general solution'' stops with the construction of the well-known integral equation (cf.~\eqref{eq:covariance-integral}) involving the covariance function of the process and the unknown ``coefficients'' of the optimal predictor, the solution of which is obtainable only when the process obeys some very simple model.

On a different level, Kalman~\cite{Kalman1960} has studied multivariate non-stationary processes corresponding to dynamical systems which are governed by known linear differential equations, and has established some basic results regarding the analytic structure of optimal predictors for such models. However, from the point of view of practical application, it would appear that the most useful results so far obtained are due to Whittle~\cite{Whittle1965}, who considered non-stationary processes generated by autoregressive models with time-dependent coefficients, and obtained explicit recursive relations for the optimal predictors. In fact, some of our results for these particular models correspond very closely to those obtained by Whittle.

The success of classical prediction theory for stationary processes is due essentially to the fact that such processes admit a spectral representation in terms of an orthogonal process. This feature not only simplifies the solution of the prediction problem, but also enables one to treat a general class of stationary processes by means of a ``canonical'' representation, so that the discussion need not be restricted to particular models, such as the autoregressive, moving-average, etc. Hitherto, the lack of a similar spectral representation for a general class of non-stationary processes has no doubt been one of the major stumbling blocks in attempts to generalize the classical theory. However, it turns out that the recently developed theory of evolutionary spectral representations~\cite{Priestley1965} provides an ideal framework for the formulation and solution of non-stationary prediction problems. In fact, by using evolutionary spectral representations one obtains a prediction theory which is almost an exact parallel of the Wiener-Kolmogorov theory.

The basic idea underlying this approach is the introduction of the evolutionary (i.e., time-dependent) spectrum of a nonstationary process, whose form completely determines the values of the ``coefficients'' of the optimal linear predictor. This means that even if we are presented with observations from a process whose structure is completely unknown we may still usefully apply this prediction theory by first estimating the evolutionary spectrum (methods for estimating evolutionary spectra have been discussed by \cite{Priestley1965,Priestley1966}). Of course, this estimation procedure introduces further complications which will not be discussed here (see also Section~\ref{sec:discussion}). In this paper we assume throughout that the second-order properties of the process are known a priori. We should point out, further, that in this account we are not aiming at complete mathematical rigour: rather we aim to show how our approach may be used to obtain explicit results for non-stationary processes.

Before discussing the prediction problem, we summarize briefly relevant parts of the theory of evolutionary spectra.

\section{Evolutionary Spectral Representations}

We consider a class of continuous parameter processes, $\{X(t)\}$, with $E\{X(t)\}=0$, $E\{X^2(t)\}<\infty$ for all $t$, for which there exists a family $\mathscr{F}$ of functions $\{\phi_t(\omega)\}$ (defined on the real line and indexed by $t$) and a measure $\mu$ on the real line such that $\{X(t)\}$ admits a spectral representation of the form
\begin{equation}
X(t) = \int_{-\infty}^{\infty} \phi_t(\omega) dZ(\omega)
\label{eq:evspec-cont}
\end{equation}
where $\{Z(\omega)\}$ is an orthogonal process with $E|dZ(\omega)|^2 = \mu(d\omega)$ (see also \cite{GrangerHatanaka1964}).

If there exists a family $\mathscr{F}$ for which each $\phi_t$ may be written in the form
\begin{equation}
\phi_t(\omega) = e^{i\omega t} A_t(\omega)
\label{eq:phiA}
\end{equation}
where, for each fixed $\omega$, the generalized Fourier transform of $A_t(\omega)$ (considered as a function of $t$) has an absolute maximum at the origin, $X(t)$ is termed an \emph{oscillatory process} (cf.~\cite{Priestley1965}), and the evolutionary spectrum at time $t$ with respect to the family $\mathscr{F}$ is defined by
\begin{equation}
dF_t(\omega) = |A_t(\omega)|^2 \mu(d\omega), \quad -\infty < \omega < \infty
\label{eq:evspec}
\end{equation}

For discrete parameter oscillatory processes (i.e., processes defined only for $t=0, \pm1, \pm2, \ldots$), we have a corresponding evolutionary spectral representation,
\begin{equation}
X(t) = \int_{-\pi}^{\pi} e^{i\omega t} A_t(\omega) dZ(\omega)
\label{eq:evspec-disc}
\end{equation}
and the evolutionary spectrum at time $t$ with respect to $\mathscr{F}$ is defined by
\begin{equation}
dF_t(\omega) = |A_t(\omega)|^2 \mu(d\omega), \quad -\pi \leq \omega \leq \pi
\label{eq:evspec-disc2}
\end{equation}

When the measure $\mu$ is absolutely continuous (with respect to Lebesgue measure), the derivative $f_t(\omega) = F_t'(\omega)$ exists for all $\omega$, and is termed the \emph{evolutionary spectral density function} at time $t$.

In both the continuous and discrete cases we may normalize the functions $A_t(\omega)$ so that $A_0(\omega) = 1$ for all $\omega$. With this convention, $\mu(d\omega) \equiv dF_0(\omega)$, the evolutionary spectrum at time zero.

\section{Moving-average Representations for Oscillatory Processes}

As in the prediction theory of stationary processes, we begin by constructing a one-sided moving-average representation for a class of oscillatory processes, the distinction with the stationary case being that the coefficients in the moving-average scheme are now time-dependent.

\subsection{Discrete-parameter Processes}

Suppose that $X(t)$ has a representation of the form~\eqref{eq:evspec-disc}, and that $\mu$ is absolutely continuous with respect to $d\omega$. Then we may write $f_t(\omega)$ in the form
\begin{equation}
f_t(\omega) = |A_t(\omega)|^2 f(\omega)
\label{eq:ftA}
\end{equation}
where $f(\omega) = d\mu/d\omega$. Note that $f(\omega) \equiv f_0(\omega)$ must be integrable. Suppose now that
\begin{equation}
\int_{-\pi}^{\pi} \log f(\omega) d\omega > -\infty
\label{eq:C1}
\end{equation}
(Condition~C1)

Then it follows that (see~\cite{Doob1953}, p.~160) there exists a function $\psi(\omega)$ such that
\begin{equation}
|\psi(\omega)|^2 = f(\omega)
\label{eq:psi}
\end{equation}
where $\psi(z)$, considered as a function of the complex variable $z = e^{i\omega}$, has no poles or zeros inside the unit circle, $|z| < 1$. The function $\psi(\omega)$ may be written as a one-sided Fourier transform, viz.
\begin{equation}
\psi(\omega) = \sum_{u=0}^{\infty} e^{-i\omega u} g^*(u)
\label{eq:psiFT}
\end{equation}
for some suitable sequence $g^*(u)$.

Suppose further that:
\begin{equation}
\text{(C2)} \quad \text{the family } \mathscr{F} \text{ can be chosen so that, for each } t, A_t(z) \text{ (as a function of } z) \text{ also has no poles or zeros inside the unit circle, so that, for each } t, 
\end{equation}
\begin{equation}
A_t(\omega) = \sum_{u=0}^{\infty} e^{-i\omega u} g_t(u)
\label{eq:At}
\end{equation}

A necessary condition for the validity of~\eqref{eq:At} is
\begin{equation}
\int_{-\pi}^{\pi} \log |A_t(\omega)|^2 d\omega > -\infty
\label{eq:C3}
\end{equation}
for all $t$ (Condition~C3).

Now write $X(t)$ in the form
\begin{equation}
X(t) = \int_{-\pi}^{\pi} e^{it\omega} \alpha_t(\omega) dz(\omega)
\label{eq:Xtalpha}
\end{equation}
where $\alpha_t(\omega) = A_t(\omega)\psi(\omega)$, so that
\begin{equation}
|\alpha_t(\omega)|^2 = f_t(\omega)
\label{eq:alphaft}
\end{equation}
and $z(\omega)$ is an orthogonal process on $(-\pi, \pi)$ with $E|dz(\omega)|^2 = d\omega$.

Since both $\psi(\omega)$ and $A_t(\omega)$ have one-sided Fourier transforms, it follows that, for each $t$, $\alpha_t(\omega)$ has a one-sided Fourier transform, i.e.,
\begin{equation}
\alpha_t(\omega) = \sum_{u=0}^{\infty} e^{i\omega u} h_t(u)
\label{eq:alphah}
\end{equation}

A necessary condition for the validity of~\eqref{eq:alphah} is
\begin{equation}
\int_{-\pi}^{\pi} \log f_t(\omega) d\omega > -\infty
\label{eq:C4}
\end{equation}
for each $t$ (Condition~C4).

\begin{theorem}\label{thm:discMA}
Let $\{X(t)\}$ be a discrete-parameter oscillatory process. If there exists a family $\mathscr{F}$ satisfying Condition~C2, and with respect to which $\{X(t)\}$ has an absolutely continuous evolutionary spectrum satisfying Condition~C4, then $\{X(t)\}$ may be represented as a one-sided moving average process of the form
\begin{equation}
X(t) = \sum_{u=0}^{\infty} h_t(u) \xi(t-u)
\label{eq:discMA}
\end{equation}
where $\{\xi(t)\}$ is a stationary uncorrelated process.
Conversely, if $\{X(t)\}$ has a one-sided moving average representation of the form~\eqref{eq:discMA}, Condition~C4 must be satisfied.
\end{theorem}

\begin{proof}
See discussion leading up to and following equation~\eqref{eq:C4}.
\end{proof}

\subsection{Continuous-parameter Processes}

As in the case of stationary processes (see~\cite{Whittle1963}), the results for discrete-parameter processes can readily be adapted to the continuous case. The measure $\mu$ is again assumed to be absolutely continuous (with respect to Lebesgue measure) and, in place of C1, we assume that
\begin{equation}
\int_{-\infty}^{\infty} \frac{\log f(\omega)}{1+\omega^2} d\omega > -\infty
\label{eq:C1star}
\end{equation}
(Condition~C1$^*$)

Then there exists a function $\psi(\omega)$ such that $|\psi(\omega)|^2 = f(\omega)$, $\psi(z)$ having no poles or zeros in the lower half-plane. The function $\psi(\omega)$ may now be written as a one-sided Fourier integral. Corresponding to~\eqref{eq:At}, we assume now that, for each $t$,
\begin{equation}
A_t(\omega) = \int_0^{\infty} e^{-i\omega u} g_t(u) du
\label{eq:Atcont}
\end{equation}
A necessary condition being
\begin{equation}
\int_{-\infty}^{\infty} \frac{\log |A_t(\omega)|^2}{1+\omega^2} d\omega > -\infty
\label{eq:C3star}
\end{equation}
for all $t$ (Condition~C3$^*$).

It then follows that we may write $X(t)$ in the form
\begin{equation}
X(t) = \int_{-\infty}^{\infty} e^{it\omega} \alpha_t(\omega) dz(\omega)
\label{eq:XtalphaCont}
\end{equation}
where $z(\omega)$ is an orthogonal process on $(-\infty, \infty)$, with $E|dz(\omega)|^2 = d\omega$, and $\alpha_t(\omega)$ has a one-sided Fourier integral representation of the form
\begin{equation}
\alpha_t(\omega) = \int_0^{\infty} e^{-i\omega u} h_t(u) du
\label{eq:alphahCont}
\end{equation}
and $|\alpha_t(\omega)|^2 = f_t(\omega)$.

A necessary condition for the validity of~\eqref{eq:alphahCont} is
\begin{equation}
\int_{-\infty}^{\infty} \frac{\log f_t(\omega)}{1+\omega^2} d\omega > -\infty
\label{eq:C4star}
\end{equation}
for all $t$ (Condition~C4$^*$).

Then, formally, we may write
\begin{equation}
X(t) = \int_0^{\infty} h_t(u) \xi(t-u) du
\label{eq:contMA}
\end{equation}
where $\xi(t) = \int_{-\infty}^{\infty} e^{i\omega t} dz(\omega)$.

\section{The Time-domain Approach}

The basic problem of linear least-squares prediction may be stated as follows: we are given the observed values of the process over the semi-infinite interval $(-\infty, t)$ and wish to predict the value of $X(t+m)$ ($m>0$). The predictor, $\tilde{X}(t+m)$, is to be chosen as that linear combination of $\{X(s), s \leq t\}$ which is such that
\begin{equation}
M(m) = E\{ \tilde{X}(t+m) - X(t+m) \}^2
\label{eq:predvar}
\end{equation}
is minimized.

Assuming that the process has a one-sided moving average form, we may easily obtain an explicit expression for $\tilde{X}(t+m)$ in terms of the process $\{\xi(t)\}$.

\subsection{Discrete-parameter Processes}

Assume $X(t)$ has a one-sided moving average representation of the form~\eqref{eq:discMA}, so that
\begin{equation}
X(t) = \sum_{u=0}^{\infty} h_t(u) \xi(t-u) = \sum_{u=-\infty}^t h_t(t-u) \xi(u)
\label{eq:XtMA}
\end{equation}
Then
\begin{equation}
X(t+m) = \sum_{u=-\infty}^t h_{t+m}(t+m-u) \xi(u) + \sum_{u=t+1}^{m} h_{t+m}(t+m-u) \xi(u)
\label{eq:Xtplusm}
\end{equation}
The predictor is to be chosen as a linear combination of $\{X(s), s \leq t\}$:
\begin{equation}
\tilde{X}(t+m) = \sum_{s=-\infty}^t b(s) X(s)
\label{eq:predX}
\end{equation}
Alternatively, as each $X(t)$ is a linear combination of the $\{\xi(t-u)\}$ ($u \geq 0$), we may write
\begin{equation}
\tilde{X}(t+m) = \sum_{u=-\infty}^t a(u) \xi(u)
\label{eq:predxi}
\end{equation}
The problem reduces to finding the coefficients $\{a(u)\}$ which minimize $M(m)$. The solution is
\begin{equation}
\tilde{X}(t+m) = \sum_{u=-\infty}^t h_{t+m}(t+m-u) \xi(u) = \sum_{u=m}^{\infty} h_{t+m}(u) \xi_{t+m-u}
\label{eq:predsol}
\end{equation}
so that
\begin{equation}
a(u) = h_{t+m}(t+m-u)
\label{eq:au}
\end{equation}
for all $u$.

The prediction variance is
\begin{equation}
M(m) = 2\pi \sum_{u=0}^{m-1} h_{t+m}^2(u)
\label{eq:predvar2}
\end{equation}

To express $\tilde{X}(t+m)$ in terms of the $X$'s, write
\begin{equation}
\xi(t) = \sum_{v=0}^{\infty} k_t(v) X(t-v)
\label{eq:xitok}
\end{equation}
Substituting~\eqref{eq:xitok} in~\eqref{eq:XtMA} and equating coefficients, we obtain the triangular system
\begin{align}
& h_t(0)k_t(0) = 1 \nonumber \\
& h_t(1)k_t(0) + h_{t-1}(0)k_t(1) = 0 \nonumber \\
& h_t(2)k_t(0) + h_{t-1}(1)k_t(1) + h_{t-2}(0)k_t(2) = 0 \label{eq:triangular}
\end{align}
and so on.

Alternatively, equating coefficients in the other order,
\begin{equation}
\sum_{v=0}^p k_{t-v}(p-v) h_t(v) = \delta_{p,0} \quad (p=0,1,2,\ldots)
\label{eq:triangular2}
\end{equation}
where $\delta_{p,0}$ is the Kronecker delta.

\subsection{Continuous-parameter Processes}

The results for continuous-parameter processes are very similar. We are given $\{X(s), -\infty < s \leq t\}$ and wish to predict $X(t+m)$ by a linear combination of past values:
\begin{equation}
\tilde{X}(t+m) = \int_{-\infty}^t X(s) b(s) ds
\label{eq:contpredX}
\end{equation}
or, equivalently,
\begin{equation}
\tilde{X}(t+m) = \int_{-\infty}^t \xi(u) a(u) du
\label{eq:contpredxi}
\end{equation}
Assuming $X(t)$ has a moving-average representation of the form~\eqref{eq:contMA}, it follows that
\begin{equation}
\tilde{X}(t+m) = \int_{-\infty}^t h_{t+m}(t+m-u) \xi(u) du = \int_m^{\infty} h_{t+m}(u) \xi(t+m-u) du
\label{eq:contpredsol}
\end{equation}

To express $\tilde{X}(t+m)$ in terms of $\{X(s)\}$, write
\begin{equation}
\xi(t) = \int_0^{\infty} k_t(v) X(t-v) dv
\label{eq:contxitok}
\end{equation}
and substitute in~\eqref{eq:contMA}, leading to the integral equations
\begin{equation}
\int_0^v h_t(u) k_{t-u}(v-u) du = \delta(v)
\label{eq:conttriangular}
\end{equation}
and
\begin{equation}
\int_0^u k_t(v) h_{t-v}(u-v) dv = \delta(u)
\label{eq:conttriangular2}
\end{equation}
where $\delta(\cdot)$ is the Dirac delta function.

\section{The Frequency-domain Approach}

We now consider an alternative approach based on a generalization of the Wiener-Hopf technique. We treat first the continuous-parameter case, and write the predictor as
\begin{equation}
\tilde{X}(t+m) = \int_0^{\infty} b_t(u) X(t-u) du
\label{eq:freqpred}
\end{equation}
or, formally,
\begin{equation}
\tilde{X}(t+m) = \int_0^{\infty} X(t-u) d\beta_t(u)
\label{eq:freqpred2}
\end{equation}
where $b_t(u)$ may depend on both $t$ and $m$.

Using the evolutionary spectral representation, the predictor may be written
\begin{equation}
\tilde{X}(t+m) = \int_{-\infty}^{\infty} e^{i\omega t} B_t(\omega) dz(\omega)
\label{eq:freqpredspec}
\end{equation}
where
\begin{equation}
B_t(\omega) = \int_0^{\infty} b_t(u) \alpha_{t-u}(\omega) e^{-iu\omega} du
\label{eq:Bt}
\end{equation}
and
\begin{equation}
X(t+m) = \int_{-\infty}^{\infty} e^{i\omega(t+m)} \alpha_{t+m}(\omega) dz(\omega)
\label{eq:Xtplusmspec}
\end{equation}

The mean squared error is
\begin{equation}
M(m) = E|\tilde{X}(t+m) - X(t+m)|^2 = \int_{-\infty}^{\infty} |e^{i\omega m} \alpha_{t+m}(\omega) - B_t(\omega)|^2 d\omega
\label{eq:freqM}
\end{equation}

Let
\begin{equation}
e^{i\omega m} \alpha_{t+m}(\omega) = C_t^{(1)}(\omega) + C_t^{(2)}(\omega)
\label{eq:Csplit}
\end{equation}
where $C_t^{(1)}(\omega)$ is a ``backward transform'' and $C_t^{(2)}(\omega)$ is a ``forward transform'' (see~\cite{Bartlett1955}, p.~201). Then
\begin{equation}
M(m) = \int_{-\infty}^{\infty} |C_t^{(1)}(\omega) - B_t(\omega)|^2 d\omega + \int_{-\infty}^{\infty} |C_t^{(2)}(\omega)|^2 d\omega
\label{eq:freqMsplit}
\end{equation}
and the minimum is attained when
\begin{equation}
B_t(\omega) = C_t^{(1)}(\omega)
\label{eq:Btopt}
\end{equation}
with
\begin{equation}
M(m) = \int_{-\infty}^{\infty} |C_t^{(2)}(\omega)|^2 d\omega
\label{eq:freqMmin}
\end{equation}

Taking Fourier transforms, for $v \geq 0$,
\begin{equation}
K_t(v) = \int_{-\infty}^{\infty} e^{i\theta(v+m)} \alpha_{t+m}(\theta) d\theta = h_{t+m}(v+m)
\label{eq:Kt}
\end{equation}
and
\begin{equation}
\int_0^v b_t(u) h_{t-u}(v-u) du = h_{t+m}(v+m)
\label{eq:freqtriangular}
\end{equation}
for all $t$ and $v \geq 0$.

If $h_{t-v}(0) \neq 0$ for all $v \in (0, \infty)$ and $\partial h_{t-u}(v-u)/\partial v = h_{t-u,v}'(v-u)$ exists and is continuous, then by differentiating both sides with respect to $v$,
\begin{equation}
h_{t-v}(0) b_t(v) + \int_0^v h_{t-u,v}'(v-u) b_t(u) du = h_{t+m,v}'(v+m)
\label{eq:freqtriangular2}
\end{equation}
or
\begin{equation}
b_t(v) + \int_0^v \frac{h_{t-u,v}'(v-u)}{h_{t-v}(0)} b_t(u) du = \frac{h_{t+m,v}'(v+m)}{h_{t-v}(0)}
\label{eq:freqtriangular3}
\end{equation}

The solution is (see~\cite{Tricomi1957}, p.~10)
\begin{equation}
b_t(v) = \frac{h_{t+m,v}'(v+m)}{h_{t-v}(0)} + \int_0^v H_t^*(v,u,-1) \frac{h_{t+m,v}'(u+m)}{h_{t-u}(0)} du
\label{eq:btfinal}
\end{equation}
where the ``resolvent kernel'' $H_t^*(v,u,-1)$ is
\begin{equation}
H_t^*(v,u,-1) = -\sum_{p=0}^{\infty} (-1)^p K_{t,p+1}^*(u,v)
\label{eq:resolvent}
\end{equation}
with the ``iterated kernels''
\begin{equation}
K_{t,p+1}^*(u,v) = \int_u^v K_{t,1}^*(u,z) K_{t,p}^*(z,v) dz \quad (p=1,2,\ldots)
\label{eq:iteratedkernels}
\end{equation}
and
\begin{equation}
K_{t,1}^*(u,v) = \frac{h_{t-u,v}'(v-u)}{h_{t-v}(0)}
\label{eq:firstkernel}
\end{equation}

For discrete-parameter processes, the predictor is
\begin{equation}
\tilde{X}(t+m) = \sum_{u=0}^{\infty} b_t(u) X(t-u)
\label{eq:discpred}
\end{equation}
and the system
\begin{equation}
\sum_{u=0}^v b_t(u) h_{t-u}(v-u) = h_{t+m}(v+m) \quad (v=0,1,2,\ldots)
\label{eq:discfreqtriangular}
\end{equation}
is triangular and easily solved by back-substitution.

Attempting to obtain $b_t(u)$ directly from the covariance function $R(s,t) = E\{X(s) X^*(t)\}$ would yield the system
\begin{equation}
\sum_{u=0}^{\infty} b_t(u) R(t-u, t-v) = R(t-v, t+m) \quad (v \geq 0)
\label{eq:covariance-integral}
\end{equation}
which is not as tractable as the spectral approach.

\section{Discussion}\label{sec:discussion}

As mentioned in Section~1, in this paper we are concerned primarily with the problem of determining the optimum predictor on the assumption that the second-order properties of the process are fully known. However, we have shown that the predictor $\tilde{X}(t+m)$ is determined uniquely by the form of the evolutionary spectra $\{f_t(\omega)\}$, so that in cases where we have no precise knowledge of the underlying model we may attack the problem by first estimating the functions $\{f_t(\omega)\}$.

It will be seen from Section~5 that $\tilde{X}(t+m)$ depends on the form of $h_{t+m}(u)$, i.e., on the form of the evolutionary spectrum at the future time point $(t+m)$. In a purely empirical analysis we would not, of course, be able to estimate $f_{t+m}(\omega)$ directly from observations extending only up to time $t$, and we would have to assume that the spectra were changing with sufficient ``smoothness'' over time to enable the form of $f_{t+m}(\omega)$ to be inferred from the spectra which have been estimated up to time $t$.

If the prediction step $m$ is much smaller than the characteristic width $B_X$ of the process (see~\cite{Priestley1965}), it would be a reasonable approximation to replace $f_{t+m}(\omega)$ by $f_t(\omega)$---the most recently available spectrum. In fact, it seems clear intuitively that the very nature of a non-stationary process precludes long-range prediction, unless we are prepared to make assumptions about the character of the non-stationarity. Consequently, if $m \gg B_X$, we would have to extrapolate the available spectra to the time point $(t+m)$. This extrapolation could be performed on the spectral ordinates themselves (by fitting suitable functions of $t$ to a range of values of $\omega$), but in general we would presumably start from a model of the process which determines the form of the evolutionary spectra in terms of a set of time-varying parameters $\{\alpha_i(t)\}$, and then extrapolate the values of these parameters. If our model specifies further the functional time-dependence of the $\{\alpha_i(t)\}$ in terms of another set of constant parameters $\{\beta_j\}$, then the extrapolation is a straightforward problem and involves merely the estimation of the $\{\beta_j\}$. If, on the other hand, the functional form of the $\{\alpha_i(t)\}$ is not specified by the model, the extrapolation would have to be performed by regression techniques.

\section*{References}
\begin{thebibliography}{99}
\bibitem{Bartlett1955}
Bartlett, M.~S. (1955). \emph{An Introduction to Stochastic Processes}. Cambridge: University Press.

\bibitem{Bendat1956}
Bendat, J.~S. (1956). A general theory of linear prediction and filtering. \emph{J. Soc. Industr. Appl. Math.}, 4, 131--151.

\bibitem{Bendat1957}
Bendat, J.~S. (1957). Exact integral equation solutions and synthesis for a large class of optimum time variable linear filters. \emph{Trans. Inst. Radio Engrs, IT-3}, 71--80.

\bibitem{Bendat1959}
Bendat, J.~S. (1959). \emph{Principles and Applications of Random Noise Theory}. New York: Wiley.

\bibitem{Booton1952}
Booton, R.~C. (1952). An optimization theory for time-varying linear systems with nonstationary statistical inputs. \emph{Proc. Inst. Radio Engrs}, 40, 977--981.

\bibitem{Cramer1961a}
Cram√©r, H. (1961a). On some classes of non-stationary processes. \emph{Proc. 4th Berkeley Symp. Math. Statist. \& Prob.}, 2, 57--78.

\bibitem{Cramer1961b}
Cram√©r, H. (1961b). On the structure of purely non-deterministic stochastic processes. \emph{Ark. Mat.}, 19, 249--266.

\bibitem{Davis1952}
Davis, R.~C. (1952). On the theory of prediction of non-stationary stochastic processes. \emph{J. Appl. Phys.}, 23, 1047--1053.

\bibitem{Doob1953}
Doob, J.~L. (1953). \emph{Stochastic Processes}. New York: Wiley.

\bibitem{GrangerHatanaka1964}
Granger, C.~W.~J. and Hatanaka, M. (1964). \emph{Spectral Analysis of Economic Time Series}. Princeton: University Press.

\bibitem{GrenanderRosenblatt1957}
Grenander, U. and Rosenblatt, M. (1957). \emph{Statistical Analysis of Stationary Time Series}. New York: Wiley.

\bibitem{Kalman1960}
Kalman, R.~E. (1960). A new approach to linear filtering and prediction problems. \emph{J. Basic Eng. (Trans. A.S.M.E., Series D)}, 82, 35--45.

\bibitem{Parzen1961}
Parzen, E. (1961). An approach to time series analysis. \emph{Ann. Math. Statist.}, 32, 951--989.

\bibitem{Priestley1965}
Priestley, M.~B. (1965). Evolutionary spectra and non-stationary processes. \emph{J. R. Statist. Soc. B}, 27, 204--237.

\bibitem{Priestley1966}
Priestley, M.~B. (1966). Design relations for non-stationary processes. \emph{J. R. Statist. Soc. B}, 28, 228--240.

\bibitem{Tricomi1957}
Tricomi, F.~G. (1957). \emph{Integral Equations}. New York: Wiley.

\bibitem{Whittle1963}
Whittle, P. (1963). \emph{Prediction and Regulation}. London: English Universities Press.

\bibitem{Whittle1965}
Whittle, P. (1965). Recursive relations for predictors of non-stationary processes. \emph{J. R. Statist. Soc. B}, 27, 523--532.

\bibitem{Yaglom1962}
Yaglom, A.~M. (1962). \emph{An Introduction to the Theory of Stationary Random Functions}. Englewood Cliffs, N.J.: Prentice-Hall.

\bibitem{Zadeh1953}
Zadeh, L.~A. (1953). Optimum non-linear filters. \emph{J. Appl. Phys.}, 24, 396--404.
\end{thebibliography}

\end{document}
